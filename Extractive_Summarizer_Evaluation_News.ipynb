{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from bert_score import BERTScorer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>Peer-to-peer (P2P) networks are here to stay, ...</td>\n",
       "      <td>But they have slowly realised that P2P is a go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tech</td>\n",
       "      <td>Dublin's hi-tech research laboratory, Media La...</td>\n",
       "      <td>In a statement, Media Labs Europe said the dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tech</td>\n",
       "      <td>A rapid alerting service that tells home compu...</td>\n",
       "      <td>A rapid alerting service that tells home compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tech</td>\n",
       "      <td>Faster, better or funkier hardware alone is no...</td>\n",
       "      <td>Dr Bjorn said that people also used their came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech</td>\n",
       "      <td>First it was the humble home video, then it wa...</td>\n",
       "      <td>But currently, putting a master feature film o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                            article  \\\n",
       "0     tech  Peer-to-peer (P2P) networks are here to stay, ...   \n",
       "1     tech  Dublin's hi-tech research laboratory, Media La...   \n",
       "2     tech  A rapid alerting service that tells home compu...   \n",
       "3     tech  Faster, better or funkier hardware alone is no...   \n",
       "4     tech  First it was the humble home video, then it wa...   \n",
       "\n",
       "                                             summary  \n",
       "0  But they have slowly realised that P2P is a go...  \n",
       "1  In a statement, Media Labs Europe said the dec...  \n",
       "2  A rapid alerting service that tells home compu...  \n",
       "3  Dr Bjorn said that people also used their came...  \n",
       "4  But currently, putting a master feature film o...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Define the categories\n",
    "categories = ['tech', 'business', 'politics', 'sport', 'entertainment']\n",
    "\n",
    "# Create an empty DataFrame\n",
    "sampled_data = pd.DataFrame()\n",
    "\n",
    "# Sample 5 outputs per category\n",
    "sampled_data = pd.concat([\n",
    "    df[df['category'] == category].sample(n=5, random_state=42)\n",
    "    for category in categories\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "sampled_data.to_csv('evaluation_sample.csv', index=False)\n",
    "\n",
    "# Display the sampled data\n",
    "sampled_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bart_summary(text, model, tokenizer):\n",
    "    \"\"\"Generate a summary using the BART model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=150, \n",
    "        min_length=30, \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_bert_score(generated_summaries, reference_summaries):\n",
    "    \"\"\"Calculate BERT Score for generated summaries against references.\"\"\"\n",
    "    bertscore = load(\"bertscore\")\n",
    "    \n",
    "    # Calculate BERT Score\n",
    "    scores = bertscore.compute(\n",
    "        predictions=generated_summaries, \n",
    "        references=reference_summaries, \n",
    "        lang=\"en\", \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    precision = np.array(scores['precision'])\n",
    "    recall = np.array(scores['recall'])\n",
    "    f1 = np.array(scores['f1'])\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"mean_precision\": precision.mean(),\n",
    "        \"mean_recall\": recall.mean(),\n",
    "        \"mean_f1\": f1.mean()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bart_summarizer(sampled_data, text_column, reference_column, model_path='fine_tuned_bart'):\n",
    "    # Load the fine-tuned BART model\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Create a new column for generated summaries\n",
    "    sampled_data['Generated Summaries'] = None\n",
    "    \n",
    "    # Generate summaries for each row\n",
    "    print(\"Generating summaries...\")\n",
    "    for i, row in tqdm(sampled_data.iterrows(), total=len(sampled_data)):\n",
    "        text = row[text_column]\n",
    "        \n",
    "        # Generate summary for this text\n",
    "        generated_summary = generate_bart_summary(text, model, tokenizer)\n",
    "        \n",
    "        # Store the generated summary\n",
    "        sampled_data.at[i, 'Generated Summaries'] = generated_summary\n",
    "    \n",
    "    # Evaluate using BERT Score\n",
    "    print(\"Calculating BERT Score...\")\n",
    "    bert_scores = evaluate_with_bert_score(\n",
    "        generated_summaries=sampled_data['Generated Summaries'].tolist(),\n",
    "        reference_summaries=sampled_data[reference_column].tolist()\n",
    "    )\n",
    "    \n",
    "    # Add scores to the dataframe\n",
    "    sampled_data['bert_precision'] = bert_scores['precision']\n",
    "    sampled_data['bert_recall'] = bert_scores['recall']\n",
    "    sampled_data['bert_f1'] = bert_scores['f1']\n",
    "    \n",
    "    # Print average scores\n",
    "    print(f\"Average BERT Score Precision: {bert_scores['mean_precision']:.4f}\")\n",
    "    print(f\"Average BERT Score Recall: {bert_scores['mean_recall']:.4f}\")\n",
    "    print(f\"Average BERT Score F1: {bert_scores['mean_f1']:.4f}\")\n",
    "    \n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:18<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BERT Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 7.95k/7.95k [00:00<00:00, 14.7MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 51.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 47053.84 seconds, 0.00 sentences/sec\n",
      "Average BERT Score Precision: 0.9222\n",
      "Average BERT Score Recall: 0.8902\n",
      "Average BERT Score F1: 0.9056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your test dataset\n",
    "    df_test = pd.read_csv('evaluation_sample.csv')\n",
    "    \n",
    "    # Evaluate BART summarizer\n",
    "    results = evaluate_bart_summarizer(\n",
    "        sampled_data=df_test,\n",
    "        text_column='article',  \n",
    "        reference_column='summary',  \n",
    "        model_path='fine_tuned_bart' \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results.to_csv('bertscore_evaluation_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
