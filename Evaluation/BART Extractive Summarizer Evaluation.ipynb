{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from bert_score import BERTScorer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bart_summary(text, model, tokenizer):\n",
    "    \"\"\"Generate a summary using the BART model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=150, \n",
    "        min_length=30, \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_bert_score(generated_summaries, reference_summaries):\n",
    "    \"\"\"Calculate BERT Score for generated summaries against references.\"\"\"\n",
    "    bertscore = load(\"bertscore\")\n",
    "    \n",
    "    # Calculate BERT Score\n",
    "    scores = bertscore.compute(\n",
    "        predictions=generated_summaries, \n",
    "        references=reference_summaries, \n",
    "        lang=\"en\", \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    precision = np.array(scores['precision'])\n",
    "    recall = np.array(scores['recall'])\n",
    "    f1 = np.array(scores['f1'])\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"mean_precision\": precision.mean(),\n",
    "        \"mean_recall\": recall.mean(),\n",
    "        \"mean_f1\": f1.mean()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bart_summarizer(sampled_data, text_column, reference_column, model_path='fine_tuned_bart'):\n",
    "    # Load the fine-tuned BART model\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Create a new column for generated summaries\n",
    "    sampled_data['Generated Summaries'] = None\n",
    "    \n",
    "    # Generate summaries for each row\n",
    "    print(\"Generating summaries...\")\n",
    "    for i, row in tqdm(sampled_data.iterrows(), total=len(sampled_data)):\n",
    "        text = row[text_column]\n",
    "        \n",
    "        # Generate summary for this text\n",
    "        generated_summary = generate_bart_summary(text, model, tokenizer)\n",
    "        \n",
    "        # Store the generated summary\n",
    "        sampled_data.at[i, 'Generated Summaries'] = generated_summary\n",
    "    \n",
    "    # Evaluate using BERT Score\n",
    "    print(\"Calculating BERT Score...\")\n",
    "    bert_scores = evaluate_with_bert_score(\n",
    "        generated_summaries=sampled_data['Generated Summaries'].tolist(),\n",
    "        reference_summaries=sampled_data[reference_column].tolist()\n",
    "    )\n",
    "    \n",
    "    # Add scores to the dataframe\n",
    "    sampled_data['bert_precision'] = bert_scores['precision']\n",
    "    sampled_data['bert_recall'] = bert_scores['recall']\n",
    "    sampled_data['bert_f1'] = bert_scores['f1']\n",
    "    \n",
    "    # Print average scores\n",
    "    print(f\"Average BERT Score Precision: {bert_scores['mean_precision']:.4f}\")\n",
    "    print(f\"Average BERT Score Recall: {bert_scores['mean_recall']:.4f}\")\n",
    "    print(f\"Average BERT Score F1: {bert_scores['mean_f1']:.4f}\")\n",
    "    \n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on BBC Dataset\n",
    "https://www.kaggle.com/datasets/pariza/bbc-news-summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...   \n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...   \n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...   \n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...   \n",
       "\n",
       "                                             summary  \n",
       "0  TimeWarner said fourth quarter sales rose 2% t...  \n",
       "1  The dollar has hit its highest level against t...  \n",
       "2  Yukos' owner Menatep Group says it will ask Ro...  \n",
       "3  Rod Eddington, BA's chief executive, said the ...  \n",
       "4  Pernod has reduced the debt it took on to fund...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('news_test.csv',index_col=False)\n",
    "news_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:20<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BERT Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 42980.15 seconds, 0.00 sentences/sec\n",
      "Average BERT Score Precision: 0.9233\n",
      "Average BERT Score Recall: 0.8914\n",
      "Average BERT Score F1: 0.9069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT score\n",
    "\n",
    "news_results = evaluate_bart_summarizer(\n",
    "    sampled_data=news_df,\n",
    "    text_column='text',  \n",
    "    reference_column='summary',  \n",
    "    model_path='fine_tuned_bart' \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_results.to_csv('bart_news_extractive_summarization.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on SciSumm Dataset\n",
    "https://www.kaggle.com/datasets/jawakar/scisummnet-corpus/data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TnT - A Statistical Part-Of-Speech Tagger Trig...</td>\n",
       "      <td>TnT - A Statistical Part-Of-Speech Tagger\\nTri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mildly Non-Projective Dependency Structures Sy...</td>\n",
       "      <td>Mildly Non-Projective Dependency Structures\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Using Corpus Statistics And WordNet Relations ...</td>\n",
       "      <td>Using Corpus Statistics And WordNet Relations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Automatic Labeling Of Semantic Roles present a...</td>\n",
       "      <td>Automatic Labeling Of Semantic Roles\\nWe prese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generative Models For Statistical Parsing With...</td>\n",
       "      <td>Generative Models For Statistical Parsing With...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  TnT - A Statistical Part-Of-Speech Tagger Trig...   \n",
       "1  Mildly Non-Projective Dependency Structures Sy...   \n",
       "2  Using Corpus Statistics And WordNet Relations ...   \n",
       "3  Automatic Labeling Of Semantic Roles present a...   \n",
       "4  Generative Models For Statistical Parsing With...   \n",
       "\n",
       "                                             summary  \n",
       "0  TnT - A Statistical Part-Of-Speech Tagger\\nTri...  \n",
       "1  Mildly Non-Projective Dependency Structures\\nS...  \n",
       "2  Using Corpus Statistics And WordNet Relations ...  \n",
       "3  Automatic Labeling Of Semantic Roles\\nWe prese...  \n",
       "4  Generative Models For Statistical Parsing With...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci_df = pd.read_csv('sci_test.csv',index_col=False)\n",
    "sci_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:28<00:00,  5.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BERT Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 43258.83 seconds, 0.00 sentences/sec\n",
      "Average BERT Score Precision: 0.8595\n",
      "Average BERT Score Recall: 0.8569\n",
      "Average BERT Score F1: 0.8578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test BART on SciTest dataset\n",
    "\n",
    "sci_results = evaluate_bart_summarizer(\n",
    "        sampled_data=sci_df,\n",
    "        text_column='text',  \n",
    "        reference_column='summary',  \n",
    "        model_path='fine_tuned_bart'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "sci_results.to_csv('bart_science_extractive_summarization.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
