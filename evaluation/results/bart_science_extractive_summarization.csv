text,summary,Generated Summaries,bert_precision,bert_recall,bert_f1
"TnT - A Statistical Part-Of-Speech Tagger Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora. A large number of current language processing systems use a part-of-speech tagger for pre-processing. The tagger assigns a (unique or ambiguous) part-ofspeech tag to each token in the input and passes its output to the next processing level, usually a parser. Furthermore, there is a large interest in part-ofspeech tagging for corpus annotation projects, who create valuable linguistic resources by a combination of automatic processing and human correction. For both applications, a tagger with the highest possible accuracy is required. The debate about which paradigm solves the part-of-speech tagging problem best is not finished. Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996). They are only surpassed by combinations of different systems, forming a &quot;voting tagger&quot;. Among the statistical approaches, the Maximum Entropy framework has a very strong position. Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words. This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging. The tagger comparison was organized as a &quot;blackbox test&quot;: set the same task to every tagger and compare the outcomes. This paper describes the models and techniques used by TnT together with the implementation. The reader will be surprised how simple the underlying model is. The result of the tagger comparison seems to support the maxime &quot;the simplest is the best&quot;. However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models. As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application. We argue that it is not only the choice of the general model that determines the result of the tagger but also the various &quot;small&quot; decisions on alternatives. The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993). The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996). For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999). TnT uses second order Markov models for part-ofspeech tagging. The states of the model represent tags, outputs represent the words. Transition probabilities depend on the states, thus pairs of tags. Output probabilities only depend on the most recent category. To be explicit, we calculate for a given sequence of words w1 of length T. t1 tr are elements of the tagset, the additional tags t_1, to, and t7-,±1 are beginning-of-sequence and end-of-sequence markers. Using these additional tags, even if they stem from rudimentary processing of punctuation marks, slightly improves tagging results. This is different from formulas presented in other publications, which just stop with a &quot;loose end&quot; at the last word. If sentence boundaries are not marked in the input, TnT adds these tags if it encounters one of [.!? ;] as a token. Transition and output probabilities are estimated from a tagged corpus. As a first step, we use the maximum likelihood probabilities P which are derived from the relative frequencies: for all t1, t2, t3 in the tagset and w3 in the lexicon. N is the total number of tokens in the training corpus. We define a maximum likelihood probability to be zero if the corresponding nominators and denominators are zero. As a second step, contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon (see below). Trigram probabilities generated from a corpus usually cannot directly be used because of the sparsedata problem. This means that there are not enough instances for each trigram to reliably estimate the probability. Furthermore, setting a probability to zero because the corresponding trigram never occured in the corpus has an undesired effect. It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability. The smoothing paradigm that delivers the best results in TnT is linear interpolation of unigrams, bigrams, and trigrams. Therefore, we estimate a trigram probability as follows: P are maximum likelihood estimates of the probabilities, and A1 + A2 ± A3 = 1, SO P again represent probability distributions. We use the context-independent variant of linear interpolation, i.e., the values of the As do not depend on the particular trigram. Contrary to intuition, this yields better results than the context-dependent variant. Due to sparse-data problems, one cannot estimate a different set of As for each trigram. Therefore, it is common practice to group trigrams by frequency and estimate tied sets of As. However, we are not aware of any publication that has investigated frequency groupings for linear interpolation in part-of-speech tagging. All groupings that we have tested yielded at most equivalent results to contextindependent linear interpolation. Some groupings even yielded worse results. The tested groupings included a) one set of As for each frequency value and b) two classes (low and high frequency) on the two ends of the scale, as well as several groupings in between and several settings for partitioning the classes. The values of A1, A2, and A3 are estimated by deleted interpolation. This technique successively removes each trigram from the training corpus and estimates best values for the As from all other ngrams in the corpus. Given the frequency counts for uni-, bi-, and trigrams, the weights can be very efficiently determined with a processing time linear in the number of different trigrams. The algorithm is given in figure 1. Note that subtracting 1 means taking unseen data into account. Without this subtraction the model would overfit the training data and would generally yield worse results. Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993). Tag probabilities are set according to the word's ending. The suffix is a strong predictor for word classes, e.g., words in the Wall Street Journal part of the Penn Treebank ending in able are adjectives (.11) in 98% of the cases (e.g. fashionable, variable) , the rest of 2% are nouns (e.g. cable, variable). The probability distribution for a particular suffix is generated from all words in the training set that share the same suffix of some predefined maximum length. The term suffix as used here means &quot;final sequence of characters of a word&quot; which is not necessarily a linguistically meaningful suffix. Probabilities are smoothed by successive abstraction. This calculates the probability of a tag t given the last m letters i of an n letter word: P(t1/7„,+1,,..ln). The sequence of increasingly more general contexts omits more and more characters of the suffix, such that P(tlin-m+2, • • • P(tlin_m+3, ,i), , P(t) are used for smoothing. The recursion formula is set A = A2 = A3 = 0 foreach trigram t1,t2,t3 with f (ti,t2,t3) >0 depending on the maximum of the following three values: for i = m ... 0, using the maximum likelihood estimates P from frequencies in the lexicon, weights Oi and the initialization For the Markov model, we need the inverse conditional probabilities P(1,2_1+1, ... /Tilt) which are obtained by Bayesian inversion. A theoretical motivated argumentation uses the standard deviation of the maximum likelihood probabilities for the weights 0, (Samuelsson, 1993). This leaves room for interpretation. We use the longest suffix that we can find in the training set (i.e., for which the frequency is greater than or equal to 1), but at most 10 characters. This is an empirically determined choice. 2) We use a context-independent approach for 0„ as we did for the contextual weights A. It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10. 3) We use different estimates for uppercase and lowercase words, i.e., we maintain two different suffix tries depending on the capitalization of the word. This information improves the tagging results. 4) Another freedom concerns the choice of the words in the lexicon that should be used for suffix handling. Should we use all words, or are some of them better suited than others? Accepting that unknown words are most probably infrequent, one can argue that using suffixes of infrequent words in the lexicon is a better approximation for unknown words than using suffixes of frequent words. Therefore, we restrict the procedure of suffix handling to words with a frequency smaller than or equal to some threshold value. Empirically, 10 turned out to be a good choice for this threshold. Additional information that turned out to be useful for the disambiguation process for several corpora and tagsets is capitalization information. Tags are usually not informative about capitalization, but probability distributions of tags around capitalized words are different from those not capitalized. The effect is larger for English, which only capitalizes proper names, and smaller for German, which capitalizes all nouns. We use flags ci that are true if wi is a capitalized word and false otherwise. These flags are added to the contextual probability distributions. Instead of and equations (3) to (5) are updated accordingly. This is equivalent to doubling the size of the tagset and using different tags depending on capitalization. The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam search. Each state that receives a 6 value smaller than the largest 6 divided by some threshold value 0 is excluded from further processing. While the Viterbi algorithm is guaranteed to find the sequence of states with the highest probability, this is no longer true when beam search is added. Nevertheless, for practical purposes and the right choice of 0, there is virtually no difference between the algorithm with and without a beam. Empirically, a value of 0 = 1000 turned out to approximately double the speed of the tagger without affecting the accuracy. The tagger currently tags between 30,000 and 60,000 tokens per second (including file I/O) on a Pentium 500 running Linux. The speed mainly depends on the percentage of unknown words and on the average ambiguity rate. We evaluate the tagger's performance under several aspects. First of all, we determine the tagging accuracy averaged over ten iterations. The overall accuracy, as well as separate accuracies for known and unknown words are measured. Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set). An important characteristic of statistical taggers is that they not only assign tags to words but also probabilities in order to rank different assignments. We distinguish reliable from unreliable assignments by the quotient of the best and second best assignmentsl . All assignments for which this quotient is larger than some threshold are regarded as reliable, the others as unreliable. As we will see below, accuracies for reliable assignments are much higher. The tests are performed on partitions of the corpora that use 90% as training set and 10% as test set, so that the test data is guaranteed to be unseen during training. Each result is obtained by repeating the experiment 10 times with different partitions and averaging the single outcomes. In all experiments, contiguous test sets are used. The alternative is a round-robin procedure that puts every 10th sentence into the test set. We argue that contiguous test sets yield more realistic results because completely unseen articles are tagged. Using the round-robin procedure, parts of an article are already seen, which significantly reduces the percentage of unknown words. Therefore, we expect even 'By definition, this quotient is oo if there is only one possible tag for a given word. higher results when testing on every 10th sentence instead of a contiguous set of 10%. In the following, accuracy denotes the number of correctly assigned tags divided by the number of tokens in the corpus processed. The tagger is allowed to assign exactly one tag to each token. We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens. The latter are interesting, since usually unknown tokens are much more difficult to process than known tokens, for which a list of valid tags can be found in the lexicon. The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frankfurter Rundschau) that are annotated with parts-ofspeech and predicate-argument structures (Skut et al., 1997). It was developed at the Saarland University in Saarbriicken2. Part of it was tagged at the IMS Stuttgart. This evaluation only uses the partof-speech annotation and ignores structural annotations. Tagging accuracies for the NEGRA corpus are shown in table 2. Figure 3 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data. Training length is the number of tokens used for training. Each training length was tested ten times, training and test sets were randomly chosen and disjoint, results were averaged. The training length is given on a logarithmic scale. It is remarkable that tagging accuracy for known words is very high even for very small training corpora. This means that we have a good chance of getting the right tag if a word is seen at least once during training. Average percentages of unknown tokens are shown in the bottom line of each diagram. We exploit the fact that the tagger not only determines tags, but also assigns probabilities. If there is an alternative that has a probability &quot;close to&quot; that of the best assignment, this alternative can be viewed as almost equally well suited. The notion of &quot;close to&quot; is expressed by the distance of probabilities, and this in turn is expressed by the quotient of probabilities. So, the distance of the probabilities of a best tag tbest and an alternative tag tau is expressed by p(tbest)/P(talt)7 which is some value greater or equal to 1 since the best tag assignment has the highest probability. Figure 4 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments). As expected, we find that accuracies for percentage known unknown • overall unknowns acc. acc. acc. a Table 5: Part-of-speech tagging accuracy for the Penn Treebank. The table shows the percentage of unknown tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall accuracy. percentage known unknown overall unknowns acc. acc. acc. reliable assignments are much higher than for unreliable assignments. This distinction is, e.g., useful for annotation projects during the cleaning process, or during pre-processing, so the tagger can emit multiple tags if the best tag is classified as unreliable. We use the Wall Street Journal as contained in the Penn Treebank for our experiments. The annotation consists of four parts: 1) a context-free structure augmented with traces to mark movement and discontinuous constituents, 2) phrasal categories that are annotated as node labels, 3) a small set of grammatical functions that are annotated as extensions to the node labels, and 4) part-of-speech tags (Marcus et al., 1993). This evaluation only uses the part-ofspeech annotation. The Wall Street Journal part of the Penn Treebank consists of approx. 50,000 sentences (1.2 million tokens). Tagging accuracies for the Penn Treebank are shown in table 5. Figure 6 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data. Training length is the number of tokens used for training. Each training length was tested ten times. Training and test sets were disjoint, results are averaged. The training length is given on a logarithmic scale. As for the NEGRA corpus, tagging accuracy is very high for known tokens even with small amounts of training data. We exploit the fact that the tagger not only determines tags, but also assigns probabilities. Figure 7 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments). Again, we find that accuracies for reliable assignments are much higher than for unreliable assignments. Average part-of-speech tagging accuracy is between 96% and 97%, depending on language and tagset, which is at least on a par with state-of-the-art results found in the literature, possibly better. For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%. This comparison needs to be re-examined, since we use a ten-fold crossvalidation and averaging of results while Ratnaparkhi only makes one test run. The accuracy for known tokens is significantly higher than for unknown tokens. For the German newspaper data, results are 8.7% better when the word was seen before and therefore is in the lexicon, than when it was not seen before (97.7% vs. 89.0%). Accuracy for known tokens is high even with very small amounts of training data. As few as 1000 tokens are sufficient to achieve 95%-96% accuracy for them. It is important for the tagger to have seen a word at least once during training. Stochastic taggers assign probabilities to tags. We exploit the probabilities to determine reliability of assignments. For a subset that is determined during processing by the tagger we achieve accuracy rates of over 99%. The accuracy of the complement set is much lower. This information can, e.g., be exploited in an annotation project to give an additional treatment to the unreliable assignments, or to pass selected ambiguities to a subsequent processing step. We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature. For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers. In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor. The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words. Note that the decisions we made yield good results for both the German and the English Corpus. They do so for several other corpora as well. The architecture remains applicable to a large variety of languages. According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both. TnT is freely available to universities and related organizations for research purposes (see http://www.coli.uni-sb.derthorstenAnt). Many thanks go to Hans Uszkoreit for his support during the development of TnT. Most of the work on TnT was carried out while the author received a grant of the Deutsche Forschungsgemeinschaft in the Graduiertenkolleg Kognitionswissenschaft Saarbriicken. Large annotated corpora are the pre-requisite for developing and testing part-ofspeech taggers, and they enable the generation of high-quality language models. Therefore, I would like to thank all the people who took the effort to annotate the Penn Treebank, the Susanne Corpus, the Stuttgarter Referenzkorpus, the NEGRA Corpus, the Verbmobil Corpora, and several others. And, last but not least, I would like to thank the users of TnT who provided me with bug reports and valuable suggestions for improvements.","TnT - A Statistical Part-Of-Speech Tagger
Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.
Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework.
A recent comparison has even shown that TnT performs significantly better for the tested corpora.
We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words.
Furthermore, we present evaluations on two corpora.
We achieve the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a tree bank of German.
","We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words.TnT uses second order Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application.The tagger comparison was organized as a &quot;blackbox test&quot",0.8724014759063721,0.842937707901001,0.857416570186615
"Mildly Non-Projective Dependency Structures Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency. In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree. While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity. In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints. The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data. Dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as Czech (Collins et al., 1999), Bulgarian (Marinov and Nivre, 2005), and Turkish (Eryi˘git and Oflazer, 2006). Many practical implementations of dependency parsing are restricted to projective structures, where the projection of a head word has to form a continuous substring of the sentence. While this constraint guarantees good parsing complexity, it is well-known that certain syntactic constructions can only be adequately represented by non-projective dependency structures, where the projection of a head can be discontinuous. This is especially relevant for languages with free or flexible word order. However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006). This raises the question of whether it is possible to characterize a class of mildly non-projective dependency structures that is rich enough to account for naturally occurring syntactic constructions, yet restricted enough to enable efficient parsing. In this paper, we review a number of proposals for classes of dependency structures that lie between strictly projective and completely unrestricted non-projective structures. These classes have in common that they can be characterized in terms of properties of the dependency structures themselves, rather than in terms of grammar formalisms that generate the structures. We compare the proposals from a theoretical point of view, and evaluate a subset of them empirically by testing their representational adequacy with respect to two dependency treebanks: the Prague Dependency Treebank (PDT) (Hajiˇc et al., 2001), and the Danish Dependency Treebank (DDT) (Kromann, 2003). The rest of the paper is structured as follows. In section 2, we provide a formal definition of dependency structures as a special kind of directed graphs, and characterize the notion of projectivity. In section 3, we define and compare five different constraints on mildly non-projective dependency structures that can be found in the literature: planarity, multiplanarity, well-nestedness, gap degree, and edge degree. In section 4, we provide an experimental evaluation of the notions of planarity, well-nestedness, gap degree, and edge degree, by investigating how large a proportion of the dependency structures found in PDT and DDT are allowed under the different constraints. In section 5, we present our conclusions and suggestions for further research. For the purposes of this paper, a dependency graph is a directed graph on the set of indices corresponding to the tokens of a sentence. We write [n] to refer to the set of positive integers up to and including n. Throughout this paper, we use standard terminology and notation from graph theory to talk about dependency graphs. In particular, we refer to the elements of the set V as nodes, and to the elements of the set E as edges. We write i --> j to mean that there is an edge from the node i to the node j (i.e., (i, j) E E), and i -->* j to mean that the node i dominates the node j, i.e., that there is a (possibly empty) path from i to j. For a given node i, the set of nodes dominated by i is the yield of i. We use the notation 3r(i) to refer to the projection of i: the yield of i, arranged in ascending order. Most of the literature on dependency grammar and dependency parsing does not allow arbitrary dependency graphs, but imposes certain structural constraints on them. In this paper, we restrict ourselves to dependency graphs that form forests. Definition 2 A dependency forest is a dependency graph with two additional properties: Figure 1 shows a dependency forest taken from PDT. It has two roots: node 2 (corresponding to the complementizer proto) and node 8 (corresponding to the final punctuation mark). Some authors extend dependency forests by a special root node with position 0, and add an edge (0, i) for every root node i of the remaining graph (McDonald et al., 2005). This ensures that the extended graph always is a tree. Although such a definition can be useful, we do not follow it here, since it obscures the distinction between projectivity and planarity to be discussed in section 3. In contrast to acyclicity and the indegree constraint, both of which impose restrictions on the dependency relation as such, the projectivity constraint concerns the interaction between the dependency relation and the positions of the nodes in the sentence: it says that the nodes in a subtree of a dependency graph must form an interval, where an interval (with endpoints i and j) is the set [i, j] := {kEV I i < k and k < j }. Definition 3 A dependency graph is projective, if the yields of its nodes are intervals. Since projectivity requires each node to dominate a continuous substring of the sentence, it corresponds to a ban on discontinuous constituents in phrase structure representations. Projectivity is an interesting constraint on dependency structures both from a theoretical and a practical perspective. Dependency grammars that only allow projective structures are closely related to context-free grammars (Gaifman, 1965; Obre¸bski and Grali´nski, 2004); among other things, they have the same (weak) expressivity. The projectivity constraint also leads to favourable parsing complexities: chart-based parsing of projective dependency grammars can be done in cubic time (Eisner, 1996); hard-wiring projectivity into a deterministic dependency parser leads to linear-time parsing in the worst case (Nivre, 2003). While the restriction to projective analyses has a number of advantages, there is clear evidence that it cannot be maintained for real-world data (Zeman, 2004; Nivre, 2006). For example, the graph in Figure 1 is non-projective: the yield of the node 1 (marked by the dashed rectangles) does not form an interval—the node 2 is ‘missing’. In this section, we present several proposals for structural constraints that relax projectivity, and relate them to each other. The notion of planarity appears in work on Link Grammar (Sleator and Temperley, 1993), where it is traced back to Mel’ˇcuk (1988). Informally, a dependency graph is planar, if its edges can be drawn above the sentence without crossing. We emphasize the word above, because planarity as it is understood here does not coincide with the standard graph-theoretic concept of the same name, where one would be allowed to also use the area below the sentence to disentangle the edges. Figure 2a shows a dependency graph that is planar but not projective: while there are no crossing edges, the yield of the node 1 (the set 11, 3}) does not form an interval. Using the notation linked(i, j) as an abbreviation for the statement ‘there is an edge from i to j, or vice versa’, we formalize planarity as follows: Definition 4 A dependency graph is planar, if it does not contain nodes a, b, c, d such that linked(a, c) A linked(b, d) A a < b < c < d . Yli-Jyrä (2003) proposes multiplanarity as a generalization of planarity suitable for modelling dependency analyses, and evaluates it experimentally using data from DDT. Definition 5 A dependency graph G = (V ; E) is m-planar, if it can be split into m planar graphs such that E = E1U- - -UEm. The planar graphs Gi are called planes. As an example of a dependency forest that is 2planar but not planar, consider the graph depicted in Figure 2b. In this graph, the edges (1, 4) and (3, 5) are crossing. Moving either edge to a separate graph partitions the original graph into two planes. Bodirsky et al. (2005) present two structural constraints on dependency graphs that characterize analyses corresponding to derivations in Tree Adjoining Grammar: the gap degree restriction and the well-nestedness constraint. A gap is a discontinuity in the projection of a node in a dependency graph (Plátek et al., 2001). More precisely, let 7ri be the projection of the node i. Then a gap is a pair (jk, jk+1) of nodes adjacent in 7ri such that Definition 6 The gap degree of a node i in a dependency graph, gd(i), is the number of gaps in 7ri. As an example, consider the node labelled i in the dependency graphs in Figure 3. In Graph 3a, the projection of i is an interval ((2, 3, 4)), so i has gap degree 0. In Graph 3b, 7ri = (2, 3, 6) contains a single gap ((3, 6)), so the gap degree of i is 1. In the rightmost graph, the gap degree of i is 2, since 7ri = (2, 4, 6) contains two gaps ((2, 4) and (4, 6)). Definition 7 The gap degree of a dependency graph G, gd(G), is the maximum among the gap degrees of its nodes. Thus, the gap degree of the graphs in Figure 3 is 0, 1 and 2, respectively, since the node i has the maximum gap degree in all three cases. The well-nestedness constraint restricts the positioning of disjoint subtrees in a dependency forest. Two subtrees are called disjoint, if neither of their roots dominates the other. Definition 8 Two subtrees T1, T2 interleave, if there are nodes l1, r1 E T1 and l2, r2 E T2 such that l1 < l2 < r1 < r2. A dependency graph is well-nested, if no two of its disjoint subtrees interleave. Both Graph 3a and Graph 3b are well-nested. Graph 3c is not well-nested. To see this, let T1 be the subtree rooted at the node labelled i, and let T2 be the subtree rooted at j. These subtrees interleave, as T1 contains the nodes 2 and 4, and T2 contains the nodes 3 and 5. The notion of edge degree was introduced by Nivre (2006) in order to allow mildly non-projective structures while maintaining good parsing efficiency in data-driven dependency parsing.2 Define the span of an edge (i, j) as the interval S((i, j)) W= [min(i, j),max(i, j)]. Definition 9 Let G = (V I E) be a dependency forest, let e = (i, j) be an edge in E, and let Ge be the subgraph of G that is induced by the nodes contained in the span of e. • The degree of an edge e 2 E, ed(e), is the number of connected components c in Ge such that the root of c is not dominated by the head of e. • The edge degree of G, ed(G), is the maximum among the degrees of the edges in G. To illustrate the notion of edge degree, we return to Figure 3. Graph 3a has edge degree 0: the only edge that spans more nodes than its head and its dependent is (1, 5), but the root of the connected component f2, 3, 4g is dominated by 1. Both Graph 3b and 3c have edge degree 1: the edge (3, 6) in Graph 3b and the edges (2, 4), (3, 5) and (4, 6) in Graph 3c each span a single connected component that is not dominated by the respective head. Apart from proposals for structural constraints relaxing projectivity, there are dependency frameworks that in principle allow unrestricted graphs, but provide mechanisms to control the actually permitted forms of non-projectivity in the grammar. The non-projective dependency grammar of Kahane et al. (1998) is based on an operation on dependency trees called lifting: a ‘lift’ of a tree T is the new tree that is obtained when one replaces one 2We use the term edge degree instead of the original simple term degree from Nivre (2006) to mark the distinction from the notion of gap degree. or more edges (i, k) in T by edges (j, k), where j ! * i. The exact conditions under which a certain lifting may take place are specified in the rules of the grammar. A dependency tree is acceptable, if it can be lifted to form a projective graph.3 A similar design is pursued in Topological Dependency Grammar (Duchier and Debusmann, 2001), where a dependency analysis consists of two, mutually constraining graphs: the ID graph represents information about immediate dominance, the LP graph models the topological structure of a sentence. As a principle of the grammar, the LP graph is required to be a lift of the ID graph; this lifting can be constrained in the lexicon. The structural conditions we have presented here naturally fall into two groups: multiplanarity, gap degree and edge degree are parametric constraints with an infinite scale of possible values; planarity and well-nestedness come as binary constraints. We discuss these two groups in turn. Parametric constraints With respect to the graded constraints, we find that multiplanarity is different from both gap degree and edge degree in that it involves a notion of optimization: since every dependency graph is m-planar for some sufficiently large m (put each edge onto a separate plane), the interesting question in the context of multiplanarity is about the minimal values for m that occur in real-world data. But then, one not only needs to show that a dependency graph can be decomposed into m planar graphs, but also that this decomposition is the one with the smallest number of planes among all possible decompositions. Up to now, no tractable algorithm to find the minimal decomposition has been given, so itis not clear how to evaluate the significance of the concept as such. The evaluation presented by Yli-Jyrä (2003) makes use of additional constraints that are sufficient to make the decomposition unique. The fundamental difference between gap degree and edge degree is that the gap degree measures the number of discontinuities within a subtree, while the edge degree measures the number of intervening constituents spanned by a single edge. This difference is illustrated by the graphs displayed in Figure 4. Graph 4a has gap degree 2 but edge degree 1: the subtree rooted at node 2 (marked by the solid edges) has two gaps, but each of its edges only spans one connected component not dominated by 2 (marked by the squares). In contrast, Graph 4b has gap degree 1 but edge degree 2: the subtree rooted at node 2 has one gap, but this gap contains two components not dominated by 2. Nivre (2006) shows experimentally that limiting the permissible edge degree to 1 or 2 can reduce the average parsing time for a deterministic algorithm from quadratic to linear, while omitting less than 1% of the structures found in DDT and PDT. It can be expected that constraints on the gap degree would have very similar effects. Binary constraints For the two binary constraints, we find that well-nestedness subsumes planarity: a graph that contains interleaving subtrees cannot be drawn without crossing edges, so every planar graph must also be well-nested. To see that the converse does not hold, consider Graph 3b, which is well-nested, but not planar. Since both planarity and well-nestedness are proper extensions of projectivity, we get the following hierarchy for sets of dependency graphs: projective C planar C well-nested C unrestricted The planarity constraint appears like a very natural one at first sight, as it expresses the intuition that ‘crossing edges are bad’, but still allows a limited form of non-projectivity. However, many authors use planarity in conjunction with a special representation of the root node: either as an artificial node at the sentence boundary, as we mentioned in section 2, or as the target of an infinitely long perpendicular edge coming ‘from the outside’, as in earlier versions of Word Grammar (Hudson, 2003). In these situations, planarity reduces to projectivity, so nothing is gained. Even in cases where planarity is used without a special representation of the root node, it remains a peculiar concept. When we compare it with the notion of gaps, for example, we find that, in a planar dependency tree, every gap .i; j/ must contain the root node r, in the sense that i < r < j: if the gap would only contain non-root nodes k, then the two paths from r to k and from i to j would cross. This particular property does not seem to be mirrored in any linguistic prediction. In contrast to planarity, well-nestedness is independent from both gap degree and edge degree in the sense that for every d > 0, there are both wellnested and non-well-nested dependency graphs with gap degree or edge degree d. All projective dependency graphs (d = 0) are trivially well-nested. Well-nestedness also brings computational benefits. In particular, chart-based parsers for grammar formalisms in which derivations obey the well-nestedness constraint (such as Tree Adjoining Grammar) are not hampered by the ‘crossing configurations’ to which Satta (1992) attributes the fact that the universal recognition problem of Linear Context-Free Rewriting Systems is X30-complete. In this section, we present an experimental evaluation of planarity, well-nestedness, gap degree, and edge degree, by examining how large a proportion of the structures found in two dependency treebanks are allowed under different constraints. Assuming that the treebank structures are sampled from naturally occurring structures in natural language, this provides an indirect evaluation of the linguistic adequacy of the different proposals. The experiments are based on data from the Prague Dependency Treebank (PDT) (Hajiˇc et al., 2001) and the Danish Dependency Treebank (DDT) (Kromann, 2003). PDT contains 1.5M words of newspaper text, annotated in three layers according to the theoretical framework of Functional Generative Description (Böhmová et al., 2003). Our experiments concern only the analytical layer, and are based on the dedicated training section of the treebank. DDT comprises 100k words of text selected from the Danish PAROLE corpus, with annotation property all structures gap degree 0 gap degree 1 gap degree 2 gap degree 3 gap degree 4 edge degree 0 edge degree 1 edge degree 2 edge degree 3 edge degree 4 edge degree 5 edge degree 6 projective planar well-nested of primary and secondary dependencies based on Discontinuous Grammar (Kromann, 2003). Only primary dependencies are considered in the experiments, which are based on the entire treebank.4 The results of our experiments are given in Table 1. For the binary constraints (planarity, well-nestedness), we simply report the number and percentage of structures in each data set that satisfy the constraint. For the parametric constraints (gap degree, edge degree), we report the number and percentage of structures having degree d (d > 0), where degree 0 is equivalent (for both gap degree and edge degree) to projectivity. For DDT, we see that about 15% of all analyses are non-projective. The minimal degree of non-projectivity required to cover all of the data is 2 in the case of gap degree and 4 in the case of edge degree. For both measures, the number of structures drops quickly as the degree increases. (As an example, only 7 or 0.17% of the analyses in DDT have gap 4A total number of 17 analyses in DDT were excluded because they either had more than one root node, or violated the indegree constraint. (Both cases are annotation errors.) degree 2.) Regarding the binary constraints, we find that planarity accounts for slightly more than the projective structures (86.41% of the data is planar), while almost all structures in DDT (99.89%) meet the well-nestedness constraint. The difference between the two constraints becomes clearer when we base the figures on the set of non-projective structures only: out of these, less than 10% are planar, while more than 99% are well-nested. For PDT, both the number of non-projective structures (around 23%) and the minimal degrees of non-projectivity required to cover the full data (gap degree 4 and edge degree 6) are higher than in DDT. The proportion of planar analyses is smaller than in DDT if we base it on the set of all structures (82.16%), but significantly larger when based on the set of non-projective structures only (22.93%). However, this is still very far from the well-nestedness constraint, which has almost perfect coverage on both data sets. As a general result, our experiments confirm previous studies on non-projective dependency parsing (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006): The phenomenon of non-projectivity cannot be ignored without also ignoring a significant portion of real-world data (around 15% for DDT, and 23% for PDT). At the same time, already a small step beyond projectivity accounts for almost all of the structures occurring in these treebanks. More specifically, we find that already an edge degree restriction of d < 1 covers 98.24% of DDT and 99.54% of PDT, while the same restriction on the gap degree scale achieves a coverage of 99.84% (DDT) and 99.57% (PDT). Together with the previous evidence that both measures also have computational advantages, this provides a strong indication for the usefulness of these constraints in the context of non-projective dependency parsing. When we compare the two graded constraints to each other, we find that the gap degree measure partitions the data into less and larger clusters than the edge degree, which may be an advantage in the context of using the degree constraints as features in a data-driven approach towards parsing. However, our purely quantitative experiments cannot answer the question, which of the two measures yields the more informative clusters. The planarity constraint appears to be of little use as a generalization of projectivity: enforcing it excludes more than 75% of the non-projective data in PDT, and 90% of the data in DDT. The relatively large difference in coverage between the two treebanks may at least partially be explained with their different annotation schemes for sentence-final punctuation. In DDT, sentence-final punctuation marks are annotated as dependents of the main verb of a dependency nexus. This, as we have discussed above, places severe restrictions on permitted forms of non-projectivity in the remaining sentence, as every discontinuity that includes the main verb must also include the dependent punctuation marks. On the other hand, in PDT, a sentencefinal punctuation mark is annotated as a separate root node with no dependents. This scheme does not restrict the remaining discontinuities at all. In contrast to planarity, the well-nestedness constraint appears to constitute a very attractive extension of projectivity. For one thing, the almost perfect coverage of well-nestedness on DDT and PDT (99.89%) could by no means be expected on purely combinatorial grounds—only 7% of all possible dependency structures for sentences of length 17 (the average sentence length in PDT), and only slightly more than 5% of all possible dependency structures for sentences of length 18 (the average sentence length in DDT) are well-nested.5 Moreover, a cursory inspection of the few problematic cases in DDT indicates that violations of the wellnestedness constraint may, at least in part, be due to properties of the annotation scheme, such as the analysis of punctuation in quotations. However, a more detailed analysis of the data from both treebanks is needed before any stronger conclusions can be drawn concerning well-nestedness. In this paper, we have reviewed a number of proposals for the characterization of mildly non-projective dependency structures, motivated by the need to find a better balance between expressivity and complexity than that offered by either strictly projective or unrestricted non-projective structures. Experimental evaluation based on data from two treebanks shows, that a combination of the wellnestedness constraint and parametric constraints on discontinuity (formalized either as gap degree or edge degree) gives a very good fit with the empirical linguistic data. Important goals for future work are to widen the empirical basis by investigating more languages, and to perform a more detailed analysis of linguistic phenomena that violate certain constraints. Another important line of research is the integration of these constraints into parsing algorithms for non-projective dependency structures, potentially leading to a better trade-off between accuracy and efficiency than that obtained with existing methods. Acknowledgements We thank three anonymous reviewers of this paper for their comments. The work of Marco Kuhlmann is funded by the Collaborative Research Centre 378 ‘Resource-Adaptive Cognitive Processes’ of the Deutsche Forschungsgemeinschaft. The work of Joakim Nivre is partially supported by the Swedish Research Council.","Mildly Non-Projective Dependency Structures
Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency.
In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree.
While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity.
In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints.
The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data.
","In section 3, we define and compare five different constraints on mildly non-projective dependency structures that is rich enough to account for naturally occurring syntactic constructions, yet restricted enough to enable efficient parsing.In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints.The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data.",0.9305551052093506,0.891218900680542,0.9104623198509216
"Using Corpus Statistics And WordNet Relations For Sense Identification Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identify a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples. Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identify a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples. An impressive array of statistical methods have been developed for word sense identification. They range from dictionary-based approaches that rely on definitions (Vdronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Schtitze 1995; Dagan and Itai 1994). We have drawn on these two traditions, using corpus-based co-occurrence and the lexical knowledge base that is embodied in the WordNet lexicon. The two traditions complement each other. Corpus-based approaches have the advantage of being generally applicable to new texts, domains, and corpora without needing costly and perhaps error-prone parsing or semantic analysis. They require only training corpora in which the sense distinctions have been marked, but therein lies their weakness. Obtaining training materials for statistical methods is costly and timeconsuming—it is a &quot;knowledge acquisition bottleneck&quot; (Gale, Church, and Yarowsky 1992a). To open this bottleneck, we use WordNet's lexical relations to locate unsupervised training examples. Section 2 describes a statistical classifier, TLC (Topical/Local Classifier), that uses topical context (the open-class words that co-occur with a particular sense), local context (the open- and closed-class items that occur within a small window around a word), or a combination of the two. The results of combining the two types of context to disambiguate a noun (line), a verb (serve), and an adjective (hard) are presented. The following questions are discussed: When is topical context superior to local context (and vice versa)? Is their combination superior to either type alone? Do the answers to these questions depend on the size of the training? Do they depend on the syntactic category of the target? Manually tagged training materials were used in the development of TLC and the experiments in Section 2. The Cognitive Science Laboratory at Princeton University, with support from NSF-ARPA, is producing textual corpora that can be used in developing and evaluating automatic methods for disambiguation. Examples of the different meanings of one thousand common, polysemous, open-class English words are being manually tagged. The results of this effort will be a useful resource for training statistical classifiers, but what about the next thousand polysemous words, and the next? In order to identify senses of these words, it will be necessary to learn how to harvest training examples automatically. Section 3 describes WordNet's lexical relations and the role that monosemous &quot;relatives&quot; of polysemous words can play in creating unsupervised training materials. TLC is trained with automatically extracted examples, its performance is compared with that obtained from manually tagged training materials. Work on automatic sense identification from the 1950s onward has been well summarized by Hirst (1987) and Dagan and Itai (1994). The discussion below is limited to work that is closely related to our research. Hearst (1991) represents local context with a shallow syntactic parse in which the context is segmented into prepositional phrases, noun phrases, and verb groups. The target noun is coded for the word it modifies, the word that modifies it, and the prepositions that precede and follow it. Open-class items within ±3 phrase segments of the target are coded in terms of their relation to the target (modifier or head) or their role in a construct that is adjacent to the target. Evidence is combined in a manner similar to that used by the local classifier component of TLC. With supervised training of up to 70 sentences per sense, performance on three homographs was quite good (88-100% correct); with fewer training examples and semantically related senses, performance on two additional words was less satisfactory (73-77% correct). Gale, Church, and Yarowsky (1992a) developed a topical classifier based on Bayesian decision theory. The only information the classifier uses is an unordered list of words that co-occur with the target in training examples. No other cues, such as part-of-speech tags or word order, are used. Leacock, Towel!, and Voorhees (1993) compared this Bayesian classifier with a content vector classifier as used in information retrieval and a neural network with backpropagation. The classifiers were compared using different numbers of senses (two, three, or six manually tagged senses of line) and different amounts of training material (50, 100, and 200 examples). On the sixsense task, the classifiers averaged 74% correct answers. Leacock, Towel!, and Voorhees (1993) found that the response patterns of the three classifiers converged, suggesting that each of the classifiers was extracting as much data as is available in purely topical approaches that look only at word counts from training examples. If this is the case, any technique that uses only topical information will not be significantly more accurate than the three classifiers tested. Leacock, Towell, and Voorhees (1996) showed that performance of the content vector topical classifier could be improved with the addition of local templates— specific word patterns that were recognized as being indicative of a particular sense— in an extension of an idea initially suggested by Weiss (1973). Although the templates proved to be highly reliable when they occurred, all too often, none were found. Yarowsky (1993) also found that template-like structures are very powerful indicators of sense. He located collocations by looking at adjacent words or at the first word to the left or right in a given part of speech and found that, with binary ambiguity, a word has only one sense in a given collocation with a probability of 90-99%.1 However, he had an average of only 29% recall (i.e., the collocations were found in only 29% of the cases). When local information occurred it was highly reliable, but all too often, it did not occur. Bruce and Wiebe (1994a, 1994b) have developed a classifier that represents local context by morphology (the inflection on the target word), the syntactic category of words within a window of ±2 words from the target, and collocation-specific items found in the sentence. The collocation-specific items are those determined to be the most informative, where an item is considered informative if the model for independence between it and a sense tag provided a poor fit to the training data. The relative probabilities of senses, available from the training corpus, are used in the decision process as prior probabilities. For each test example, the evidence in its local context is combined in a Bayesian-type model of the probability of each sense, and the most probable sense is selected. Performance ranges from 77-84% correct on the test words, where a lower bound for performance based on always selecting the most frequent sense for the same words (i.e., the sense with the greatest prior probability) would yield 53-80% correct. Yarowsky (1994), building on his earlier work, designed a classifier that looks at words within ±k positions from the target; lemma forms are obtained through morphological analysis; and a coarse part-of-speech assignment is performed by dictionary lookup. Context is represented by collocations based on words or parts of speech at specific positions within the window or, less specifically, in any position. Also coded are some special classes of words, such as WEEKDAY, that might serve to distinguish among word senses. For each type of local-context evidence found in the corpus, a log-likelihood ratio is constructed, indicating the strength of the evidence for one form of the homograph versus the other. These ratios are then arranged in a sorted decision list with the largest values (strongest evidence) first. A decision is made for a test sentence by scanning down the decision list until a match is found. Thus, only the single best piece of evidence is used. The classifier was tested on disambiguating the homographs that result from accent removal in Spanish and French (e.g., seria, seria). In tests with the number of training examples ranging from a few hundred to several thousand, overall accuracy was high, above 90%. Clearly, sense identification is an active area of research, and considerable ingenuity is apparent. But despite the promising results reported in this literature, the reality is that there still are no large-scale, operational systems for tagging the senses of words in text. The statistical classifier, TLC, uses topical context, local context, or a combination of the two, for word sense identification. TLC's flexibility in using both forms is an important asset for our investigations. A noun, a verb, and an adjective were tested in this study. Table 1 provides a synonym or brief gloss for each of the senses used. Training corpora and testing corpora were collected as follows: Wall Street Journal corpus and from the American Printing House for the Blind corpus.' Examples for hard were taken from the LDC San Jose Mercury News (SJM) corpus. Each consisted of the sentence containing the target and one sentence preceding it. The resulting strings had an average length of 49 items. 2. Examples where the target was the head of an unambiguous collocation were removed from the files. Being unambiguous, they do not need to be disambiguated. These collocations, for example, product line and hard candy were found using WordNet. In Section 3, we consider how they can be used for unsupervised training. Examples where the target was part of a proper noun were also removed; for example, Japan Air Lines was not taken as an example of line. first 25, 50, 100, and 200 examples of the least frequent sense, and examples from the other senses in numbers that reflected their relative frequencies in the corpus. As an illustration, in the smallest training set for hard, there were 25 examples of the least frequent sense, 37 examples of the second most frequent sense, and 256 examples of the most frequent sense. The test sets were of fixed size: each contained 150 of the least frequent sense and examples of the other senses in numbers that reflected their relative frequencies. The operation of TLC consists of preprocessing, training, and testing. During preprocessing, examples are tagged with a part-of-speech tagger (Brill 1994); special tags are inserted at sentence breaks; and each open-class word found in WordNet is replaced with its base form. This step normalizes across morphological variants without resorting to the more drastic measure of stemming. Morphological information is not lost, since the part-of-speech tag remains unchanged. Training consists of counting the frequencies of various contextual cues for each sense. Testing consists of taking a new example of the polysemous word and computing the most probable sense, based on the cues present in the context of the new item. A comparison is made to the sense assigned by a human judge, and the classifier's decision is scored as correct or incorrect. TLC uses a Bayesian approach to find the sense s, that is the most probable given the cues ci contained in a context window of ±k positions around the polysemous target word. For each s„ the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_k,. • • s,) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: In TLC, we have made this assumption and have estimated p(ci I si) from the training. Of course, the sparse data problem affects these probabilities too, and so TLC uses the Good-Turing formula (Good 1953; Chiang, Lin, and Su 1995), to smooth the values of p(cj s,), including providing probabilities for cues that did not occur in the training. TLC actually uses the mean of the Good-Turing value and the training-derived value for p(cj s,). When cues do not appear in training, it uses the mean of the GoodTuring value and the global probability of the cue p(ci), obtained from a large text corpus. This approach to smoothing has yielded consistently better performance than relying on the Good-Turing values alone. tuation. For this cue type, p(cj I s,) is the probability that item cl appears precisely at location j for sense Si. Positions j = —2, —1, 1,2 are used. The global probabilities, for example p(the_i), are based on counts of closed-class items found at these positions relative to the nouns in a large text corpus. The local window width of ±2 was selected after pilot testing on the semantically tagged Brown corpus. As in (2) above, the local window does not extend beyond a sentence boundary. 4. Part-of-speech tags in the positions j = —2, —1,0, 1,2 are also used as cues. The probabilities for these tags are computed for specific positions (e.g., p(DT_i I p(DTA) in a manner similar to that described in (3) above. When TLC is configured to use only topical information, cue type (1) is employed. When it is configured for local information, cue types (2), (3), and (4) are used. Finally, in combined mode, the set of cues contains all four types. 2.3 Results Figures 1 to 3 show the accuracy of the classifier as a function of the size of the training set when using local context, topical context, and a combination of the two, averaged across three runs for each training set. To the extent that the words used are representative, some clear differences appear as a function of syntactic category. With the verb serve, local context was more reliable than topical context at all levels of training (78% versus 68% with 200 training examples for the least frequent sense). The combination of local and topical context showed improvement (83%) over either form alone (see Figure 1). With the adjective hard, local context was much more reliable as an indicator of sense than topical context for all training sizes (83% versus 60% with 200 training examples) and the combined classifier's performance (at 83%) was the same as for local (see Figure 2). In the case of the noun line, topical was slightly better than local at all set sizes, but with 200 training examples, their combination yielded 84% accuracy, greater than either topical (78%) or local (67%) alone (see Figure 3). To summarize, local context was more reliable than topical context as an indicator of sense for this verb and this adjective, but slightly less reliable for this noun. The combination of local and topical context showed improved or equal performance for all three words. Performance for all of the classifiers improved with increased training size. All classifiers performed best with at least 200 training examples per sense, but the learning curve tended to level off beyond a minimum 100 training examples. These results are consistent with those of Yarowsky (1993), based on his experiments with pseudowords, homophones, and homonyms (discussed below). He observed that performance for verbs and adjectives dropped sharply as the window increased, while distant context remained useful for nouns. Thus one is tempted to conclude that nouns depend more on topic than do verbs and adjectives. But such a conclusion is probably an overgeneralization, inasmuch as some noun senses are clearly nontopical. Thus, Leacock, Towell, and Voorhees (1993) found that some senses of the noun line are not susceptible to disambiguation with topical context. For example, the 'textual' sense of line can appear with any topic, whereas the 'product' sense of line cannot. When it happens that a nontopical sense accounts for a large proportion of occurrences (in our study, all senses of hard are nontopical), then adding topical context to local will have little benefit and may even reduce accuracy. One should not conclude from these results that the topical classifiers and TLC are inferior to the classifiers reviewed in Section 2. In our experiments, monosemous collocations in WordNet that contain the target word were systematically removed from the training and testing materials. This was done on the assumption that these words are not ambiguous. Removing them undoubtedly made the task more difficult than it would normally be. How much more difficult? An estimate is possible. We Classifier performance on four senses of the verb serve. Percentage accounted for by most frequent sense = 41%. searched through 7,000 sentences containing line and found 1,470 sentences contained line as the head of a monosemous collocation in WordNet, i.e., line could be correctly disambiguated in some 21% of those 7,000 sentences simply on the basis of the WordNet entries in which it occurred. In other words, if these sentences had been included in the experiment—and had been identified by automatic lookup—overall accuracy would have increased from 83% to 87%. Using topical context alone, TLC performs no worse than other topical classifiers. Leacock, Towell, and Voorhees (1993) report that the three topical classifiers tested averaged 74% accuracy on six senses of the noun line. With these same training and testing data, TLC performed at 73% accuracy. Similarly, when the content vector and neural network classifiers were run on manually tagged training and testing examples of the verb serve, they averaged 74% accuracy—as did TLC using only topical context. When local context is combined with topical, TLC is superior to the topical classifiers compared in the Leacock, Towel!, and Voorhees (1993) study. Just how useful is a sense classifier whose accuracy is 85% or less? Probably not very useful if it is part of a fully automated NLP application, but its performance might be adequate in an interactive application (e.g., machine-assisted translation, on-line thesaurus functions in word processing, interactive information retrieval). In fact, when recall does not have to be 100% (as when a human is in the loop) the precision of the classifier can be improved considerably. The classifier described above always selects the sense that has the highest probability. We have observed that when Classifier performance on three senses of the adjective hard. Percentage accounted for by most frequent sense = 80%. the difference between the probability of this sense and that of the second highest is relatively small, the classifier's choice is often incorrect. One way to improve the precision of the classifier, though at the price of reduced recall, is to identify these situations and allow it to respond do not know rather than forcing a decision. What is needed is a measure of the difference in the probabilities of the two senses. Following the approach of Dagan and Itai (1994), we use the log of the ratio of the probabilities In(pi/p2) for this purpose. Based on this value, a threshold e can be set to control when the classifier selects the most probable sense. For example, if e = 2, then ln(pi/p2) must be 2 or greater for a decision to be made. Dagan and Itai (1994) also describe a way to make the threshold dynamic so that it adjusts for the amount of evidence used to estimate pi and p2. The basic idea is to create a one-tailed confidence interval so that we can state with probability 1 — a that the true value of the difference measure is greater than O. When the amount of evidence is small, the value of the measure must be larger in order to insure that e is indeed exceeded. Table 2 shows precision and recall values for serve, hard, and line at eight different settings of 0 using a 60% confidence interval. TLC was first trained on 100 examples of each sense, and it was then tested on separate 100-example sets. In all cases, precision was positively correlated with the square root of 0 (all r values > .97), and recall was negatively correlated with the square root of 0 (r values < —.96). As cross-validation, the equations of the lines that fit the precision and recall results on the test sample were used to predict the precision and recall at the various values of 0 on a second test sample. They provided a good fit to the new data, accounting for an average of 93% of the variance. The standard errors of estimate for hard, serve, and line were .028, .030, and .029 for precision, and .053, .068, and .041 for recall. This demonstrates that it is possible to produce accurate predictions of precision and recall as a function of for new test sets. When the threshold is set to a large value, precision approaches 100%. The criterion thus provides a way to locate those cases that can be identified automatically with very high accuracy. When TLC uses a high criterion for assigning senses, it can be used to augment the training examples by automatically collecting new examples from the test corpus. In summary, the results obtained with TLC support the following preliminary conclusions: (a) improvement with training levels off after about 100 training examples for the least frequent sense; (b) the high predictive power of local context for the verb and adjective indicate that the local parameters effectively capture syntactically mediated relations, e.g., the subject and object or complement of verbs, or the noun that an adjective modifies; (c) nouns may be more &quot;topical&quot; than verbs and adjectives, and therefore benefit more from the combination of topical and local context; (d) the precision of TLC can be considerably improved at the price of recall, a trade-off that may be desirable in some interactive NLP applications. A final observation we can make is that when topical and local information is combined, what we have called &quot;nontopical senses&quot; can reduce overall accuracy. For example, the 'textual' sense of line is relatively topic-independent. The results of the line experiment were not affected too adversely because the nontopical sense of line accounted for only 10% of the training examples. The effects of nontopical senses will be more serious when most senses are nontopical, as in the case of many adjectives and verbs. The generality of these conclusions must, of course, be tested with additional words, which brings us to the problem of obtaining training and testing corpora. On one hand, it is surprising that a purely statistical classifier can &quot;learn&quot; how to identify a sense of a polysemous word with as few as 100 example contexts. On the other hand, anyone who has manually built such sets knows that even collecting 100 examples of each sense is a long and tedious process. The next section presents one way in which the lexical knowledge in WordNet can be used to extract training examples automatically. Corpus-based word sense identifiers are data hungry—it takes them mere seconds to digest all of the information contained in training materials that take months to prepare manually. So, although statistical classifiers are undeniably effective, they are not feasible until we can obtain reliable unsupervised training data. In the Gale, Church, and Yarowsky (1992a) study, training and testing materials were automatically acquired using an aligned French-English bilingual corpus by searching for English words that have two different French translations. For example, English tokens of sentence were translated as either peine or phrase. They collected contexts of sentence translated as peine to build a corpus for the judicial sense, and collected contexts of sentence translated as phrase to build a corpus for the grammatical sense. One problem with relying on bilingual corpora for data collection is that bilingual corpora are rare, and aligned bilingual corpora are even rarer. Another is that since French and English are so closely related, different senses of polysemous English words often translate to the same French word. For example, line is equally polysemous in French and English—and most senses of line translate into French as ligne. Several artificial techniques have been used so that classifiers can be developed and tested without having to invest in manually tagging the data: Yarowsky (1993) and Schtitze (1995) have acquired training and testing materials by creating pseudowords from existing nonhomographic forms. For example, a pseudoword was created by combining abused/escorted. Examples containing the string escorted were collected to train on one sense of the pseudoword and examples containing the string abused were collected to train on the other sense. In addition, Yarowsky (1993) used homophones (e.g., cellar/seller) and Yarowsky (1994) created homographs by stripping accents from French and Spanish words. Although these latter techniques are useful in their own right (e.g., spoken language systems or corrupted transmissions), the resulting materials do not generalize to the acquisition of tagged training for real polysemous or even homographic words. The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993). Yarowsky (1992) used a thesaurus to collect training materials. He tested the unsupervised training materials on 12 nouns with almost perfect results on homonyms (95-99%), 72% accuracy for four senses of interest, and 77% on three senses of cone. The training was collected in the following manner. Take a Roget's category—his examples were TOOL and ANIMAL—and collect sentences from a corpus (in this case, Grolier's Encyclopedia) using the words in each category. Consider the noun crane, which appears in both the Roget's categories TOOL and ANIMAL. To represent the TOOL category, Yarowsky extracted contexts from Gro/ier's Encyclopedia. For example, contexts with the words adz, shovel, crane, sickle, and so on. Similarly he collected sentences with names of animals from the ANIMAL category. In these samples, crane and drill appeared under both categories. Yarowsky points out that the resulting noise will be a problem only when one of the spurious senses is salient, dominating the training set, and he uses frequency-based weights to minimize these effects. We propose to minimize spurious training by using monosemous words and collocations—on the assumption that, if a word has only one sense in WordNet, it is monosemous. Schtitze (1995) developed a statistical topical approach to word sense identification that provides its own automatically extracted training examples. For each occurrence t of a polysemous word in a corpus, a context vector is constructed by summing all the vectors that represent the co-occurrence patterns of the open-class words in t's context (i.e., topical information is expressed as a kind of second-order co-occurrence). These context vectors are clustered, and the centroid of each cluster is used to represent a &quot;sense.&quot; When given a new occurrence of the word, a vector of the words in its context is constructed, and this vector is compared to the sense representations to find the closest match. Schulze has used the method to disambiguate pseudowords, homographs, and polysemous words. Performance varies depending, in part, on the number of clusters that are created to represent senses, and on the degree to which the distinctions correspond to different topics. This approach performs very well, especially with pseudowords and homographs. However, there is no automatic means to map the sense representations derived from the system onto the more conventional word senses found in dictionaries. Consequently, it does not provide disambiguated examples that can be used by other systems. Yarowsky (1995) has proposed automatically augmenting a small set of experimenter-supplied seed collocations (e.g., manufacturing plant and plant life for two different senses of the noun plant) into a much larger set of training materials. He resolved the problem of the sparseness of his collocations by iteratively bootstrapping acquisition of training materials from a few seed collocations for each sense of a homograph. He locates examples containing the seeds in the corpus and analyzes these to find new predictive patterns in these sentences and retrieves examples containing these patterns. He repeats this step iteratively. Results for the 12 pairs of homographs reported are almost perfect. In his paper, Yarowsky suggests WordNet as a source for the seed collocations—a suggestion that we pursue in the next section. WordNet is particularly well suited to the task of locating sense-relevant context because each word sense is represented as a node in a rich semantic lexical network with synonymy, hyponymy, and meronymy links to other words, some of them polysemous and others monosemous. These lexical &quot;relatives&quot; provide a key to finding relevant training sentences in a corpus. For example, the noun suit is polysemous, but one sense of it has business suit as a monosemous daughter and another has legal proceeding as a hypernym. By collecting sentences containing the unambiguous nouns business suit and legal proceeding we can build two corpora of contexts for the respective senses of the polysemous word. All the systems described in Section 2.1 could benefit from the additional training materials that monosemous relatives can provide. The WordNet on-line lexical database (Miller 1990, 1995) has been developed at Princeton University over the past 10 years.' Like a standard dictionary, WordNet contains the definitions of words. It differs from a standard dictionary in that, instead of being organized alphabetically, WordNet is organized conceptually. The basic unit in WordNet is a synonym set, or synset, which represents a lexicalized concept. For example, WordNet Version 1.5 distinguishes between two senses of the noun shot with the synsets {shot, snapshot} and {shot, injection}. In the context, &quot;The photographer took a shot of Mary,&quot; the word snapshot can be substituted for one sense of shot. In the context, &quot;The nurse gave Mary a flu shot,&quot; the word injection can be substituted for another sense of shot. Nouns, verbs, adjectives, and adverbs are each organized differently in WordNet. All are organized in synsets, but the semantic relations among the synsets differ depending on the grammatical category, as can be seen in Table 3. Nouns are organized in a hierarchical tree structure based on hypernymy/hyponymy. The hyponym of a noun is its subordinate, and the relation between a hyponym and its hypernym is an is a kind of relation. For example, maple is a hyponym of tree, which is to say that a maple is a kind of tree. Hypernymy (supername) and its inverse, hyponymy (subname), are transitive semantic relations between synsets. Meronymy (part-name), and its inverse holortymy (whole-name), are complex semantic relations that distinguish component parts, substantive parts, and member parts. The verbal hierarchy is based on troponymy, the is a manner of relation. For example, stroll is a troponym of walk, which is to say that strolling is a manner of walking. Entailment relations between verbs are also coded in WordNet. The organization of attributive adjectives is based on the antonymy relation. Where direct antonyms exist, adjective synsets point to antonym synsets. A head adjective is one that has a direct antonym (e.g., hot versus cold or long versus short). Many adjectives, like sultry, have no direct antonyms. When an adjective has no direct antonym, its synset points to a head that is semantically similar to it. Thus sultry and torrid are similar in meaning to hot, which has the direct antonym of cold. So, although sultry has no direct antonym, it has cold as its indirect antonym. Relational adjectives do not have antonyms; instead they point to nouns. Consider the difference between a nervous disorder and a nervous student. In the former, nervous pertains to a noun, as in nervous system, whereas the latter is defined by its relation to other adjectives—its synonyms (e.g., edgy) and antonyms (e.g., relaxed). Adverbs have synonymy and antonymy relations. When the adverb is morphologically related to an adjective (when an -ly suffix is added to an adjective) and semantically related to the adjective as well, the adverb points to the adjective. We have had some success in exploiting WordNet's semantic relations for word sense identification. Since the main problem with classifiers that use local context is the sparseness of the training data, Leacock and Chodorow (1998) used a proximity measure on the hypernym relation to replace the subject and complement of the verb serve in the testing examples with the subject and complement from training examples that were &quot;closest&quot; to them in the noun hierarchy. For example, one of the test sentences was &quot;Sauerbraten is usually served with dumplings,&quot; where neither sauerbraten nor dumpling appeared in any training sentence. The similarity measures on WordNet found that sauerbraten was most similar to dinner in the training, and dumpling to bacon. These nouns were substituted for the novel ones in the test sets. Thus the sentence &quot;Dinner is usually served with bacon&quot; was substituted for the original sentence. Augmentation of the local .context classifier with WordNet similarity measures showed a small but consistent improvement in the classifier's performance. The improvement was greater with the smaller training sets. Resnik (1992) uses an information-based measure, the most informative class, on the WordNet taxonomy. A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms). Based on verb/object pairs collected from a corpus, Resnik found, for example, that the objects for the verb open fall into two classes: receptacle and oral communication. Conversely, the class of a verb's object could be used to determine the appropriate sense of that verb. The experiments in the next section depend on a subset of the WordNet lexical relations, those involving monosemous relatives, so we were interested in determining just what proportion of word senses have such relatives. We examined 8,500 polysemous nouns that appeared in a moderate-size, 25-million-word corpus. In all, these 8,500 nouns have more than 24,000 WordNet senses. Restricting the relations to synonyms, immediate hyponyms (i.e., daughters), and immediate hypernyms (parents), we found that about 64% (15,400) have monosemous relatives attested in the corpus. With larger corpora (e.g., with text obtained by Web crawling) and more lexical relations (e.g., meronymy), this percentage can be expected to increase. The approach we have used is related to that of Yarowsky (1992) in that training materials are collected using a knowledge base, but it differs in other respects, notably in the selection of training and testing materials, the choice of a knowledge base, and use of both topical and local classifiers. Yarowsky collects his training and testing materials from a specialized corpus, Grolier's Encyclopedia. It remains to be seen whether a statistical classifier trained on a topically organized corpus such as an encyclopedia will perform in the same way when tested on general unrestricted text, such as newspapers, periodicals, and books. One of our goals is to determine whether automatic extraction of training examples is feasible using general corpora. In his experiment, Yarowsky uses an updated on-line version of Roget's Thesaurus that is not generally available to the research community. The only generally available version of Roget's is the 1912 edition, which contains many lexical gaps. We are using WordNet, which can be obtained via anonymous ftp. Yarowsky's classifier is purely topical, but we also examine local context. Finally, we hope to avoid inclusion of spurious senses by using monosemous relatives. In this experiment we collected monosemous relatives of senses of 14 nouns. Training sets are created in the following manner. A program called AutoTrain retrieves from WordNet all of the monosemous relatives of a polysemous word sense, samples and retrieves example sentences containing these monosemous relatives from a 30-million-word corpus of the San Jose Mercury News, and formats them for TLC. The sampling process retrieves the &quot;closest&quot; relatives first. For example, suppose that the system is asked to retrieve 100 examples for each sense of the noun court. The system first looks for the strongest or top-level relatives: for monosemous synonyms of the sense (e.g., tribunal) and for daughter collocations that contain the target word as the head (e.g., superior court) and tallies the number of examples in the corpus for each. If the corpus has 100 or more examples for these top-level relatives, it retrieves a sampling of them and formats them for TLC. If there are not enough top-level examples, the remainder of the target's monosemous relatives are inspected in the order: all other daughters; hyponym collocations that contain the target; all other hyponyms; hypernyms; and, finally, sisters. AutoTrain takes as broad a sampling as possible across the corpus and never takes more than one example from an article. The number of examples for each relative is based on the relative proportion of its occurrences in the corpus. Table 4 shows the monosemous relatives that were used to train five senses of the noun line—the monosemous relatives of the sixth sense in the original study, line as an abstract division, are not attested in the SIM corpus. The purpose of the experiment was to see how well TLC performed using unsupervised training and, when possible, to compare this with its performance when training on the manually tagged materials being produced at Princeton's Cognitive Science Laboratory.' When a sufficient number of examples for two or more senses were available, 100 examples of each sense were set aside to use in training. The remainder were used for testing. Only the topical and local open-class cues were used, since preliminary tests showed that performance declined when using local closed-class and part-of-speech cues obtained from the monosemous relatives. This is not surprising, as many of the relatives are collocations whose local syntax is quite different from that of the polysemous word in its typical usage. For example, the 'formation' sense of line is often followed by an of-phrase as in a line of children, but its relative, picket line, is not. Prior probabilities for the sense were taken from the manually tagged materials. Table 5 shows the results when TLC was trained on monosemous relatives and on manually tagged training materials. Baseline performance is when the classifier always chooses the most frequent sense. Eight additional words had a sufficient number of manually tagged examples for testing but not for training TLC. These are shown in Table 6. For four of the examples in Table 5, training with relatives produced results within 1% or 2% of manually tagged training. Line and work, however, showed a substantial decrease in performance. In the case of line, this might be due to overly specific training contexts. Almost half of the training examples for the 'formation' sense of line come from one relative, picket line. In fact, all of the monosemous relatives, except for rivet line and trap line, are human formations. This may have skewed training so that the classifier performs poorly on other uses of line as formation. In order to compare our results with those reported in Yarowsky (1992), we trained and tested on the same two senses of the noun duty that Yarowsky had tested ('obligation' and 'tax'). He reported that his thesaurus-based approach yielded 96% precision with 100% recall. TLC used training examples based on monosemous WordNet relatives and correctly identified the senses with 93.5% precision at 100% recall. Table 6 shows TLC's performance on the other eight words after training with monosemous relatives and testing on manually tagged examples. Performance is about the same as, or only slightly better than, the highest prior probability. In part, this is due to the rather high probability of the most frequent sense for this set. The values in the table are based on decisions made on all test examples. If a threshold is set for TLC (see Section 2.4), precision of the classifier can be increased substantially, at the expense of recall. Table 7 shows recall levels when TLC is trained on monosemous relatives and the value of e is set for 95% precision. Operating in this mode, the classifier can gather new training materials, automatically, and with high precision. This is a particularly good way to find clear cases of the most frequent sense. The results also show that not all words are well suited to this kind of operation. Little can be gained for a word like work, where the two senses, 'activity' and 'product,' are closely related and therefore difficult for the classifier to distinguish, due to a high degree of overlap in the training contexts. Problems of this sort can be detected even before testing, by computing correlations between the vectors of open-class words for the different senses. The cosine correlation between the 'activity' and 'product' senses of work is r = .49, indicating a high degree of overlap. The mean correlation between pairs of senses for the other words in Table 7 is r = .31. Our evidence indicates that local context is superior to topical context as an indicator of word sense when using a statistical classifier. The benefits of adding topical to local context alone depend on syntactic category as well as on the characteristics of the individual word. The three words studied yielded three different patterns; a substantial benefit for the noun line, slightly less for the verb serve, and none for the adjective hard. Some word senses are simply not limited to specific topics, and appear freely in many different domains of discourse. The existence of nontopical senses also limits the applicability of the &quot;one sense per discourse&quot; generalization of Gale, Church, and Yarowsky (1992b), who observed that, within a document, a repeated word is almost always used in the same sense. Future work should be directed toward developing methods for determining when a word has a nontopical sense. One approach to this problem is to look for a word that appears in many more topical domains than its total number of senses. Because the supply of manually tagged training data will always be limited, we propose a method to obtain training data automatically using commonly available materials: exploiting WordNet's lexical relations to harvest training examples from LDC corpora or even the World Wide Web. We found this method to be effective, although not as effective as using manually tagged training. We have presented the components of a system for acquiring unsupervised training materials that can be used with any statistical classifier. The components can be fit together in the following manner. For a polysemous word, locate the monosemous relatives for each of its senses in WordNet and extract examples containing these relatives from a large corpus. Senses whose contexts greatly overlap can be identified with a simple cosine correlation. Often, correlations are high between senses of a word that are systematically related, as we saw for the 'activity' and 'product' senses of work. In some cases, the contexts for the two closely related senses may be combined. Since the frequencies of the monosemous relatives do not correlate with the frequencies of the senses, prior probabilities must be estimated for classifiers that use them. In the experiments of Section 3.2, these were estimated from the testing materials. They can also be estimated from a small manually tagged sample, such as the parts of the Brown corpus that have been tagged with senses in WordNet. When the threshold is set to maximize precision, the results are highly reliable and can be used to support an interactive application, such as machine-assisted translation, with the goal of reducing the amount of interaction. Although we have looked at only a few examples, it is clear that, given WordNet and a large enough corpus, the methods outlined for training on monosemous relatives can be generalized to build training materials for thousands of polysemous words. We are indebted to the other members of the WordNet group who have provided advice and technical support: Christiane Fellbaum, Shari Landes, and Randee Tengi. We are also grateful to Paul Bagyenda, Ben Johnson-Laird and Joshua Schecter. We thank Scott Wayland, Tim Allison and Jill Hollifield for tagging the serve and hard corpora. Finally we are grateful to the three anonymous CL reviewers for their comments and advice. This material is based upon work supported in part by the National Science Foundation under NSF Award No. 1R19528983 and by the Defense Advanced Research Projects Agency, Grant No. N00014-91-1634.","Using Corpus Statistics And WordNet Relations For Sense Identification
Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck.
We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora.
We describe a statistical classifier that combines topical context with local cues to identity a word sense.
The classifier is used to disambiguate a noun, a verb, and an adjective.
A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus.
Test results are compared with those from manually tagged training examples.
We present a method to obtain sense-tagged examples using monosemous relatives.
","The results of combining the two types of context to disambiguate a noun (line), a verb (serve), and an adjective (hard) are presented.We describe a statistical classifier that combines topical context with local cues to identify a word sense.A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus.TLC is trained with automatically extracted examples, its performance is compared with that obtained from manually tagged training materials.",0.9101662039756775,0.8932077884674072,0.9016072154045105
"Automatic Labeling Of Semantic Roles present a system for identifying the semantic relationships, or filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word frame, the system labels constituents with either abstract semantic roles, such as or more domain-specific semantic roles, such as and The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data. We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles, such as SPEAKER, MESSAGE, and TOPIC. The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data. Recent years have been exhilarating ones for natural language understanding. The excitement and rapid advances that had characterized other language-processing tasks such as speech recognition, part-of-speech tagging, and parsing have finally begun to appear in tasks in which understanding and semantics play a greater role. For example, there has been widespread commercial deployment of simple speech-based natural language understanding systems that answer questions about flight arrival times, give directions, report on bank balances, or perform simple financial transactions. More sophisticated research systems generate concise summaries of news articles, answer fact-based questions, and recognize complex semantic and dialogue structure. But the challenges that lie ahead are still similar to the challenge that the field has faced since Winograd (1972): moving away from carefully hand-crafted, domaindependent systems toward robustness and domain independence. This goal is not as far away as it once was, thanks to the development of large semantic databases such as WordNet (Fellbaum 1998) and progress in domain-independent machine learning algorithms. Current information extraction and dialogue understanding systems, however, are still based on domain-specific frame-and-slot templates. Systems for booking airplane information use domain-specific frames with slots like ORIG CITY, DEST CITY, or DEPART TIME (Stallard 2000). Systems for studying mergers and acquisitions use slots like PRODUCTS, RELATIONSHIP, JOINT VENTURE COMPANY, and AMOUNT (Hobbs et al. 1997). For natural language understanding tasks to proceed beyond these specific domains, we need semantic frames and semantic understanding systems that do not require a new set of slots for each new application domain. In this article we describe a shallow semantic interpreter based on semantic roles that are less domain specific than TO AIRPORT or JOINT VENTURE COMPANY. These roles are defined at the level of semantic frames of the type introduced by Fillmore (1976), which describe abstract actions or relationships, along with their participants. For example, the JUDGEMENT frame contains roles like JUDGE, EVALUEE, and REASON, and the STATEMENT frame contains roles like SPEAKER, ADDRESSEE, and MESSAGE, as the following examples show: These shallow semantic roles could play an important role in information extraction. For example, a semantic role parse would allow a system to realize that the ruling that is the direct object of change in (3) plays the same THEME role as the ruling that is the subject of change in (4): The fact that semantic roles are defined at the frame level means, for example, that the verbs send and receive would share the semantic roles (SENDER, RECIPIENT, GOODS, etc.) defined with respect to a common TRANSFER frame. Such common frames might allow a question-answering system to take a question like (5) and discover that (6) is relevant in constructing an answer to the question: This shallow semantic level of interpretation has additional uses outside of generalizing information extraction, question answering, and semantic dialogue systems. One such application is in word sense disambiguation, where the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcategorization frames of a verb such as serve can be used to help disambiguate a particular instance of the word. Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of text data mining (TDM) (Hearst 1999). Finally, incorporating semantic roles into probabilistic models of language may eventually yield more accurate parsers and better language models for speech recognition. This article describes an algorithm for identifying the semantic roles filled by constituents in a sentence. We apply statistical techniques that have been successful for the related problems of syntactic parsing, part-of-speech tagging, and word sense disambiguation, including probabilistic parsing and statistical classification. Our statistical algorithms are trained on a hand-labeled data set: the FrameNet database (Baker, Fillmore, and Lowe 1998; Johnson et al. 2001). The FrameNet database defines a tag set of semantic roles called frame elements and included, at the time of our experiments, roughly 50,000 sentences from the British National Corpus hand-labeled with these frame elements. This article presents our system in stages, beginning in Section 2 with a more detailed description of the data and the set of frame elements or semantic roles used. We then introduce (in Section 3) the statistical classification technique used and examine in turn the knowledge sources of which our system makes use. Section 4 describes the basic syntactic and lexical features used by our system, which are derived from a Penn Treebank–style parse of individual sentences to be analyzed. We break our task into two subproblems: finding the relevant sentence constituents (deferred until Section 5), and giving them the correct semantic labels (Sections 4.2 and 4.3). Section 6 adds higher-level semantic knowledge to the system, attempting to model the selectional restrictions on role fillers not directly captured by lexical statistics. We compare hand-built and automatically derived resources for providing this information. Section 7 examines techniques for adding knowledge about systematic alternations in verb argument structure with sentence-level features. We combine syntactic parsing and semantic role identification into a single probability model in Section 8. Section 9 addresses the question of generalizing statistics from one target predicate to another, beginning with a look at domain-independent thematic roles in Section 9.1. Finally we draw conclusions and discuss future directions in Section 10. Semantic roles are one of the oldest classes of constructs in linguistic theory, dating back thousands of years to Panini’s k¯araka theory (Misra 1966; Rocher 1964; Dahiya 1995). Longevity, in this case, begets variety, and the literature records scores of proposals for sets of semantic roles. These sets of roles range from the very specific to the very general, and many have been used in computational implementations of one type or another. At the specific end of the spectrum are domain-specific roles such as the FRoM AIRPoRT, To AIRPoRT, or DEPART TIME discussed above, or verb-specific roles such as EATER and EATEN for the verb eat. The opposite end of the spectrum consists of theories with only two “proto-roles” or “macroroles”: PRoTo-AGENT and PRoTo-PATIENT (Van Valin 1993; Dowty 1991). In between lie many theories with approximately 10 roles, such as Fillmore’s (1971) list of nine: AGENT, EXPERIENCER, INSTRUMENT, OBJECT, SoURCE, GoAL, LoCATIoN, TIME, and PATH.1 Sample domains and frames from the FrameNet lexicon. Many of these sets of roles have been proposed by linguists as part of theories of linking, the part of grammatical theory that describes the relationship between semantic roles and their syntactic realization. Other sets have been used by computer scientists in implementing natural language understanding systems. As a rule, the more abstract roles have been proposed by linguists, who are more concerned with explaining generalizations across verbs in the syntactic realization of their arguments, whereas the more specific roles have more often been proposed by computer scientists, who are more concerned with the details of the realization of the arguments of specific verbs. The FrameNet project (Baker, Fillmore, and Lowe 1998) proposes roles that are neither as general as the 10 abstract thematic roles, nor as specific as the thousands of potential verb-specific roles. FrameNet roles are defined for each semantic frame. A frame is a schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore 1976). For example, the frame CONVERSATION, shown in Figure 1, is invoked by the semantically related verbs argue, banter, debate, converse, and gossip, as well as the nouns dispute, discussion, and tiff, and is defined as follows: The roles defined for this frame, and shared by all its lexical entries, include PROTAGONIST-1 and PROTAGONIST-2 or simply PROTAGONISTS for the participants in the conversation, as well as MEDIUM and TOPIC. Similarly, the JUDGMENT frame mentioned above has the roles JUDGE, EVALUEE, and REASON and is invoked by verbs such as blame, admire, and praise and nouns such as fault and admiration. We refer to the roles for a given frame as frame elements. A number of hand-annotated examples from the JUDGMENT frame are included below to give a flavor of the FrameNet database: Defining semantic roles at this intermediate frame level helps avoid some of the well-known difficulties of defining a unique small set of universal, abstract thematic roles while also allowing some generalization across the roles of different verbs, nouns, and adjectives, each of which adds semantics to the general frame or highlights a particular aspect of the frame. One way of thinking about traditional abstract thematic roles, such as AGENT and PATIENT, in the context of FrameNet is to conceive them as frame elements defined by abstract frames, such as action and motion, at the top of an inheritance hierarchy of semantic frames (Fillmore and Baker 2000). The examples above illustrate another difference between frame elements and thematic roles as commonly described in the literature. Whereas thematic roles tend to be arguments mainly of verbs, frame elements can be arguments of any predicate, and the FrameNet database thus includes nouns and adjectives as well as verbs. The examples above also illustrate a few of the phenomena that make it hard to identify frame elements automatically. Many of these are caused by the fact that there is not always a direct correspondence between syntax and semantics. Whereas the subject of blame is often the JUDGE, the direct object of blame can be an EVALUEE (e.g., the poor in “blaming the poor”) or a REASON (e.g., everything in “blame everything on coyotes”). The identity of the JUDGE can also be expressed in a genitive pronoun, (e.g., his in “his praise”) or even an adjective (e.g., critical in “critical praise”). The corpus used in this project is perhaps best described in terms of the methodology used by the FrameNet team. We outline the process here; for more detail see Johnson et al. (2001). As the first step, semantic frames were defined for the general domains chosen; the frame elements, or semantic roles for participants in a frame, were defined; and a list of target words, or lexical predicates whose meaning includes aspects of the frame, was compiled for each frame. Example sentences were chosen by searching the British National Corpus for instances of each target word. Separate searches were performed for various patterns over lexical items and part-of-speech sequences in the target words’ context, producing a set of subcorpora for each target word, designed to capture different argument structures and ensure that some examples of each possible syntactic usage of the target word would be included in the final database. Thus, the focus of the project was on completeness of examples for lexicographic needs, rather than on statistically representative data. Sentences from each subcorpus were then annotated by hand, marking boundaries of each frame element expressed in the sentence and assigning tags for the annotated constituent’s frame semantic role, syntactic category (e.g., noun phrase or prepositional phrase), and grammatical function in relation to the target word (e.g., object or complement of a verb). In the final phase of the process, the annotated sentences for each target word were checked for consistency. In addition to the tags just mentioned, the annotations include certain other information, which we do not make use of in this work, such as word sense tags for some target words and tags indicating metaphoric usages. Tests of interannotator agreement were performed for data from a small number of predicates before the final consistency check. Interannotator agreement at the sentence level, including all frame element judgments and boundaries for one predicate, varied from .66 to .82 depending on the predicate. The kappa statistic (Siegel and Castellan 1988) varied from .67 to .82. Because of the large number of possible categories when boundary judgments are considered, kappa is nearly identical to the interannotator agreement. The system described in this article (which gets .65/.61 precision/recall on individual frame elements; see Table 15) correctly identifies all frame elements in 38% of test sentences. Although this .38 is not directly comparable to the .66–.82 interannotator agreements, it’s clear that the performance of our system still falls significantly short of human performance on the task. The British National Corpus was chosen as the basis of the FrameNet project despite differences between British and American usage because, at 100 million words, it provides the largest corpus of English with a balanced mixture of text genres. The British National Corpus includes automatically assigned syntactic part-of-speech tags for each word but does not include full syntactic parses. The FrameNet annotators did not make use of, or produce, a complete syntactic parse of the annotated sentences, although some syntactic information is provided by the grammatical function and phrase type tags of the annotated frame elements. The preliminary version of the FrameNet corpus used for our experiments contained 67 frame types from 12 general semantic domains chosen for annotation. A complete list of the semantic domains represented in our data is shown in Table 1, along with representative frames and predicates. Within these frames, examples of a total of 1,462 distinct lexical predicates, or target words, were annotated: 927 verbs, 339 nouns, and 175 adjectives. There are a total of 49,013 annotated sentences and 99,232 annotated frame elements (which do not include the target words themselves). How important is the particular set of semantic roles that underlies our system? For example, could the optimal choice of semantic roles be very dependent on the application that needs to exploit their information? Although there may well be application-specific constraints on semantic roles, our semantic role classifiers seem in practice to be relatively independent of the exact set of semantic roles under consideration. Section 9.1 describes an experiment in which we collapsed the FrameNet roles into a set of 18 abstract thematic roles. We then retrained our classifier and achieved roughly comparable results; overall performance was 82.1% for abstract thematic roles, compared to 80.4% for frame-specific roles. Although this doesn’t show that the detailed set of semantic roles is irrelevant, it does suggest that our statistical classification algorithm, at least, is relatively robust to even quite large changes in role identities. Assignment of semantic roles is an important part of language understanding, and the problem of how to assign such roles has been attacked by many computational systems. Traditional parsing and understanding systems, including implementations of unification-based grammars such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994), rely on hand-developed grammars that must anticipate each way in which semantic roles may be realized syntactically. Writing such grammars is time consuming, and typically such systems have limited coverage. Data-driven techniques have recently been applied to template-based semantic interpretation in limited domains by “shallow” systems that avoid complex feature structures and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as Atlanta filled a semantic slot such as DESTINATION in a semantic frame for air travel. In a data-driven approach to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998) extend this technique to derive automatically entire “case frames” for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalizing beyond the relatively small number of frames considered in the tasks. More recently, a domain-independent system has been trained by Blaheta and Charniak (2000) on the function tags, such as MANNER and TEMPORAL, included in the Penn Treebank corpus. Some of these tags correspond to FrameNet semantic roles, but the Treebank tags do not include all the arguments of most predicates. In this article, we aim to develop a statistical system for automatically learning to identify all semantic roles for a wide variety of predicates in unrestricted text. In this section we describe the first, basic version of our statistically trained system for automatically identifying frame elements in text. The system will be extended in later sections. We first describe in detail the sentence- and constituent-level features on which our system is based and then use these features to calculate probabilities for predicting frame element labels in Section 4.2. In this section we give results for a system that labels roles using the human-annotated boundaries for the frame elements within the sentence; we return to the question of automatically identifying the boundaries in Section 5. Our system is a statistical one, based on training a classifier on a labeled training set and testing on a held-out portion of the data. The system is trained by first using an automatic syntactic parser to analyze the 36,995 training sentences, matching annotated frame elements to parse constituents and extracting various features from the string of words and the parse tree. During testing, the parser is run on the test sentences and the same features are extracted. Probabilities for each possible semantic role r are then computed from the features. The probability computation is described in the next section; here we discuss the features used. The features used represent various aspects of the syntactic structure of the sentence as well as lexical information. The relationship between such surface manifestations and semantic roles is the subject of linking theory (see Levin and Rappaport Hovav [1996] for a synthesis of work in this area). In general, linking theory argues that the syntactic realization of arguments of a predicate is predictable from semantics; exactly how this relationship works, however, is the subject of much debate. Regardless of the underlying mechanisms used to generate syntax from semantics, the relationship between the two suggests that it may be possible to learn to recognize semantic relationships from syntactic cues, given examples with both types of information. 4.1.1 Phrase Type. Different semantic roles tend to be realized by different syntactic categories. For example, in communication frames, the SpEAKER is likely to appear as a noun phrase, Topic as a prepositional phrase or noun phrase, and MEDiUM as a prepositional phrase, as in: “[Speaker We ] talked [Topic about the proposal ] [Medium over the phone ] .” The phrase type feature we used indicates the syntactic category of the phrase expressing the semantic roles, using the set of syntactic categories of the Penn Treebank project, as described in Marcus, Santorini, and Marcinkiewicz (1993). In our data, frame elements are most commonly expressed as noun phrases (NPs, 47% of frame elements in the training set), and prepositional phrases (PPs, 22%). The next most common categories are adverbial phrases (ADVPs, 4%), particles (e.g. “make something up”; PRTs, 2%) and clauses (SBARs, 2%, and Ss, 2%). (Tables 22 and 23 in the Appendix provides a listing of Penn Treebank’s part-of-speech tags and constituent labels.) We used Collins’ (1997) statistical parser trained on examples from the Penn Treebank to generate parses of the same format for the sentences in our data. Phrase types were derived automatically from parse trees generated by the parser, as shown in Figure 2. Given the automatically generated parse tree, the constituent spanning the same set of words as each annotated frame element was found, and the constituent’s nonterminal label was taken as the phrase type. In cases in which more than one constituent matches because of a unary production in the parse tree, the higher constituent was chosen. A sample sentence with parser output (above) and FrameNet annotation (below). Parse constituents corresponding to frame elements are highlighted. The matching was performed by calculating the starting and ending word positions for each constituent in the parse tree, as well as for each annotated frame element, and matching each frame element with the parse constituent with the same beginning and ending points. Punctuation was ignored in this computation. Because of parsing errors, or, less frequently, mismatches between the parse tree formalism and the FrameNet annotation standards, for 13% of the frame elements in the training set, there was no parse constituent matching an annotated frame element. The one case of systematic mismatch between the parse tree formalism and the FrameNet annotation standards is the FrameNet convention of including both a relative pronoun and its antecedent in frame elements, as in the first frame element in the following sentence: Mismatch caused by the treatment of relative pronouns accounts for 1% of the frame elements in the training set. During testing, the largest constituent beginning at the frame element’s left boundary and lying entirely within the element was used to calculate the frame element’s features. We did not use this technique on the training set, as we expected that it would add noise to the data, but instead discarded examples with no matching parse constituent. Our technique for finding a near match handles common parse errors such as a prepositional phrase being incorrectly attached to a noun phrase at the right-hand edge, and it guarantees that some syntactic category will be returned: the part-of-speech tag of the frame element’s first word in the limiting case. alization as subject or direct object is one of the primary facts that linking theory attempts to explain. It was a motivation for the case hierarchy of Fillmore (1968), which allowed such rules as “If there is an underlying AGENT, it becomes the syntactic subject.” Similarly, in his theory of macroroles, Van Valin (1993) describes the ACTOR as being preferred in English for the subject. Functional grammarians consider syntactic subjects historically to have been grammaticalized agent markers. As an example of how such a feature can be useful, in the sentence “He drove the car over the cliff,” the subject NP is more likely to fill the AGENT role than the other two NPs. We will discuss various grammatical-function features that attempt to indicate a constituent’s syntactic relation to the rest of the sentence, for example, as a subject or object of a verb. The first such feature, which we call “governing category,” or gov, has only two values, S and VP, corresponding to subjects and objects of verbs, respectively. This feature is restricted to apply only to NPs, as it was found to have little effect on other phrase types. As with phrase type, the feature was read from parse trees returned by the parser. We follow links from child to parent up the parse tree from the constituent corresponding to a frame element until either an S or VP node is found and assign the value of the feature according to whether this node is an S or a VP. NP nodes found under S nodes are generally grammatical subjects, and NP nodes under VP nodes are generally objects. In most cases the S or VP node determining the value of this feature immediately dominates the NP node, but attachment errors by the parser or constructions such as conjunction of two NPs can cause intermediate nodes to be introduced. Searching for higher ancestor nodes makes the feature robust to such cases. Even given good parses, this feature is not perfect in discriminating grammatical functions, and in particular it confuses direct objects with adjunct NPs such as temporal phrases. For example, town in the sentence “He left town” and yesterday in the sentence “He left yesterday” will both be assigned a governing category of VP. Direct and indirect objects both appear directly under the VP node. For example, in the sentence “He gave me a new hose,” me and a new hose are both assigned a governing category of VP. More sophisticated handling of such cases could improve our system. 4.1.3 Parse Tree Path. Like the governing-category feature described above, the parse tree path feature (path) is designed to capture the syntactic relation of a constituent to the rest of the sentence. The path feature, however, describes the syntactic relation between the target word (that is, the predicate invoking the semantic frame) and the constituent in question, whereas the gov feature is independent of where the target word appears in the sentence; that is, it identifies all subjects whether they are the subject of the target word or not. The path feature is defined as the path from the target word through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 3. Although the path is composed as a string of symbols, our system treats the string as an atomic value. The path includes, as the first element of the string, the part of speech of the target word and, as the last element, the phrase type or syntactic category of the sentence constituent marked as a frame element. After some experimentation, we settled on a version of the path feature that collapses the various part-of-speech tags for verbs, including past-tense verb (VBD), third-person singular present-tense verb (VBZ), other present-tense verb (VBP), and past participle (VBN), into a single verb tag denoted “VB.” Our path feature is dependent on the syntactic representation used, which in our case is the Treebank-2 annotation style (Marcus et al. 1994), as our parser is trained on this later version of the Treebank data. Figure 4 shows the annotation for the sentence “They expect him to cut costs throughout the organization,” which exhibits In this example, the path from the target word ate to the frame element He can be represented as VBTVPTS↓NP, with T indicating upward movement in the parse tree and ↓ downward movement. The NP corresponding to He is found as described in Section 4.1.1. Treebank annotation of raising constructions. the syntactic phenomenon known as subject-to-object raising, in which the main verb’s object is interpreted as the embedded verb’s subject. The Treebank-2 style tends to be generous in its usage of S nodes to indicate clauses, a decision intended to make possible a relatively straightforward mapping from S nodes to predications. In this example, the path from cut to the frame element him would be VBTVPTVPTStNP, which typically indicates a verb’s subject, despite the accusative case of the pronoun him. For the target word of expect in the sentence of Figure 4, the path to him would be VBTVPtStNP, rather than the typical direct-object path of VBTVPtNP. An example of Treebank-2 annotation of an “equi” construction, in which a noun phrase serves as an argument of both the main and subordinate verbs, is shown in Figure 5. Here, an empty category is used in the subject position of the subordinate clause and is co-indexed with the NP Congress in the direct-object position of the main clause. The empty category, however, is not used in the statistical model of the parser or shown in its output and is also not used by the FrameNet annotation, which would mark the NP Congress as a frame element of raise in this example. Thus, the value of our path feature from the target word raise to the frame element Congress would be VBTVPTVPTSTVPtNP, and from the target word of persuaded the path to Congress would be the standard direct-object path VBTVPtNP. Other changes in annotation style from the original Treebank style were specifically intended to make predicate argument structure easy to read from the parse trees and include new empty (or null) constituents, co-indexing relations between nodes, and secondary functional tags such as subject and temporal. Our parser output, however, does not include this additional information, but rather simply gives trees of phrase type categories. The sentence in Figure 4 is one example of how the change in annotation style of Treebank-2 can affect this level of representation; the earlier style assigned the word him an NP node directly under the VP of expect. The most common values of the path feature, along with interpretations, are shown in Table 2. For the purposes of choosing a frame element label for a constituent, the path feature is similar to the gov feature defined above. Because the path captures more information than the governing category, it may be more susceptible to parser errors and data sparseness. As an indication of this, our path feature takes on a total of 2,978 possible values in the training data when frame elements with no matching Example of target word renting in a small clause. parse constituent are not counted and 4,086 possible values when paths are found to the best-matching constituent in these cases. The governing-category feature, on the other hand, which is defined only for NPs, has only two values (S, corresponding to subjects, and VP, corresponding to objects). In cases in which the path feature includes an S or VP ancestor of an NP node as part of the path to the target word, the gov feature is a function of the path feature. This is the case most of the time, including for our prototypical subject (VBTVPTStNP) and object (VBTVPtNP) paths. Of the 35,138 frame elements identified as NPs by the parser, only 4% have a path feature that does not include a VP or S ancestor. One such example is shown in Figure 6, where the small clause “the remainder renting ...” has no S node, giving a path feature from renting to the remainder of VBTVPTNPtNP. The value of the gov feature here is VP, as the algorithm finds the VP of the sentence’s main clause as it follows parent links up the tree. The feature is spurious in this case, because the main VP is not headed by, or relevant to, the target word renting. Systems based on the path and gov features are compared in Section 4.3. The differences between the two are relatively small for the purpose of identifying semantic roles when frame element boundaries are known. The path feature will, however, be important in identifying which constituents are frame elements for a given target word, as it gives us a way of navigating through the parse tree to find the frame elements in the sentence. 4.1.4 Position. To overcome errors due to incorrect parses, as well as to see how much can be done without parse trees, we introduced position as a feature. The position feature simply indicates whether the constituent to be labeled occurs before or after the predicate defining the semantic frame. We expected this feature to be highly correlated with grammatical function, since subjects will generally appear before a verb and objects after. Although we do not have hand-checked parses against which to measure the performance of the automatic parser on our corpus, the result that 13% of frame elements have no matching parse constituent gives a rough idea of the parser’s accuracy. Almost all of these cases in which no matching parse constituent was found are due to parser error. Other parser errors include cases in which a constituent is found, but with the incorrect label or internal structure. This result also considers only the individual constituent representing the frame element: the parse for the rest of the sentence may be incorrect, resulting in an incorrect value for the grammatical function features described in the previous two sections. Collins (1997) reports 88% labeled precision and recall on individual parse constituents on data from the Penn Treebank, roughly consistent with our finding of at least 13% error. 4.1.5 Voice. The distinction between active and passive verbs plays an important role in the connection between semantic role and grammatical function, since direct objects of active verbs often correspond in semantic role to subjects of passive verbs. From the parser output, verbs were classified as active or passive by building a set of 10 passive-identifying patterns. Each of the patterns requires both a passive auxiliary (some form of to be or to get) and a past participle. Roughly 5% of the examples were identified as passive uses. 4.1.6 Head Word. As previously noted, we expected lexical dependencies to be extremely important in labeling semantic roles, as indicated by their importance in related tasks such as parsing. Head words of noun phrases can be used to express selectional restrictions on the semantic types of role fillers. For example, in a communication frame, noun phrases headed by Bill, brother, or he are more likely to be the SpEAKER, whereas those headed by proposal, story, or question are more likely to be the Topic. (We did not attempt to resolve pronoun references.) Since the parser we used assigns each constituent a head word as an integral part of the parsing model, we were able to read the head words of the constituents from the parser output, employing the same set of rules for identifying the head child of each constituent in the parse tree. The rules for assigning a head word are listed in Collins (1999). Prepositions are considered to be the head words of prepositional phrases. The rules for assigning head words do not attempt to distinguish between cases in which the preposition expresses the semantic content of a role filler, such as PATH frame elements expressed by prepositional phrases headed by along, through, or in, and cases in which the preposition might be considered to be purely a case marker, as in most uses of of, where the semantic content of the role filler is expressed by the preposition’s object. Complementizers are considered to be heads, meaning that infinitive verb phrases are always headed by to and subordinate clauses such as in the sentence “I’m sure that he came” are headed by that. For our experiments, we divided the FrameNet corpus as follows: one-tenth of the annotated sentences for each target word were reserved as a test set, and another onetenth were set aside as a tuning set for developing our system. A few target words where fewer than 10 examples had been chosen for annotation were removed from the corpus. (Section 9 will discuss generalization to unseen predicates.) In our corpus, the average number of sentences per target word is only 34, and the number of sentences per frame is 732, both relatively small amounts of data on which to train frame element classifiers. To label the semantic role of a constituent automatically, we wish to estimate a probability distribution indicating how likely the constituent is to fill each possible Distributions calculated for semantic role identification: r indicates semantic role, pt phrase type, gov grammatical function, h head word, and t target word, or predicate. Distribution role, given the features described above and the predicate, or target word, t: P(r I h, pt, gov, position, voice, t) where r indicates semantic role, h head word, and pt phrase type. It would be possible to calculate this distribution directly from the training data by counting the number of times each role appears with a combination of features and dividing by the total number of times the combination of features appears: #(r, h, pt,gov, position, voice, t) P(r  |h, pt, gov, position, voice, t) = #(h, pt,gov, position, voice, t) In many cases, however, we will never have seen a particular combination of features in the training data, and in others we will have seen the combination only a small number of times, providing a poor estimate of the probability. The small number of training sentences for each target word and the large number of values that the head word feature in particular can take (any word in the language) contribute to the sparsity of the data. Although we expect our features to interact in various ways, we cannot train directly on the full feature set. For this reason, we built our classifier by combining probabilities from distributions conditioned on a variety of subsets of the features. Table 3 shows the probability distributions used in the final version of the system. Coverage indicates the percentage of the test data for which the conditioning event had been seen in training data. Accuracy is the proportion of covered test data for which the correct role is given the highest probability, and Performance, which is the product of coverage and accuracy, is the overall percentage of test data for which the correct role is predicted.3 Accuracy is somewhat similar to the familiar metric of precision in that it is calculated over cases for which a decision is made, and performance is similar to recall in that it is calculated over all true frame elements. Unlike in a traditional precision/recall trade-off, however, these results have no threshold to adjust, and the task is a multiway classification rather than a binary decision. The distributions calculated were simply the empirical distributions from the training data. That is, occurrences of each role and each set of conditioning events were counted in a table, and probabilities calculated by dividing the counts for each role by the total number Sample probabilities for P(r  |pt, gov, t) calculated from training data for the verb abduct. The variable gov is defined only for noun phrases. The roles defined for the removing frame in the motion domain are AGENT (AGT), THEME (THM), COTHEME (COTHM) (“... had been abducted with him”), and MANNER (MANR). of observations for each conditioning event. For example, the distribution P(r  |pt, t) was calculated as follows: Some sample probabilities calculated from the training are shown in Table 4. As can be seen from Table 3, there is a trade-off between more-specific distributions, which have high accuracy but low coverage, and less-specific distributions, which have low accuracy but high coverage. The lexical head word statistics, in particular, are valuable when data are available but are particularly sparse because of the large number of possible head words. To combine the strengths of the various distributions, we merged them in various ways to obtain an estimate of the full distribution P(r  |h, pt,gov, position, voice, t). The first combination method is linear interpolation, which simply averages the probabilities given by each of the distributions: where Ei λi = 1. The geometric mean, when expressed in the log domain, is similar: where Z is a normalizing constant ensuring that Er P(r  |constituent) = 1. Results for systems based on linear interpolation are shown in the first row of Table 5. These results were obtained using equal values of λ for each distribution defined for the relevant conditioning event (but excluding distributions for which the conditioning event was not seen in the training data). As a more sophisticated method of choosing interpolation weights, the expectation maximization (EM) algorithm was used to estimate the likelihood of the observed role’s being produced by each of the distributions in the general techniques of Jelinek and Mercer (1980). Because a number of the distributions used may have no training data for a given set of variables, the data were divided according to the set of distributions available, and a separate set of interpolation weights was trained for each set of distributions. This technique (line 2 of Table 5) did not outperform equal weights even on the data used to determine the weights. Although the EM algorithm is guaranteed to increase the likelihood of the training data, that likelihood does not always correspond to our scoring, which is based only on whether the correct outcome is assigned the highest probability. Results of the EM interpolation on held-out test data are shown in Table 6. Experimentation has shown that the weights used have relatively little impact in our interpolation scheme, no doubt because the evaluation metric depends only on the ranking of the probabilities and not on their exact values. Changing the interpolation weights rarely changes the probabilities of the roles enough to change their ranking. What matters most is whether a combination of variables has been seen in the training data or not. Results for the geometric mean are shown in row 3 of Table 5. As with linear interpolation, the exact weights were found to have little effect, and the results shown reflect equal weights. An area we have not explored is the use of the maximum-entropy techniques of, for example, Pietra, Pietra, and Lafferty (1997), to set weights for the log-linear model, either at the level of combining our probability distributions or at the level of calculating weights for individual values of the features. In the “backoff” combination method, a lattice was constructed over the distributions in Table 3 from more-specific conditioning events to less-specific, as shown in Figure 7. The lattice is used to select a subset of the available distributions to combine. The less-specific distributions were used only when no data were present for any more-specific distribution. Thus, the distributions selected are arranged in a cut across the lattice representing the most-specific distributions for which data are available. The selected probabilities were combined with both linear interpolation and a geometric mean, with results shown in Table 5. The final row of the table represents the baseline Lattice organization of the distributions from Table 3, with more-specific distributions toward the top. of always selecting the most common role of the target word for all its constituents, that is, using only P(r I t). Although this lattice is reminiscent of techniques of backing off to less specific distributions commonly used in n-gram language modeling, it differs in that we use the lattice only to select distributions for which the conditioning event has been seen in the training data. Discounting and deleted interpolation methods in language modeling typically are used to assign small, nonzero probability to a predicted variable unseen in the training data even when a specific conditioning event has been seen. In our case, we are perfectly willing to assign zero probability to a specific role (the predicted variable). We are interested only in finding the role with the highest probability, and a role given a small, nonzero probability by smoothing techniques will still not be chosen as the classifier’s output. The lattice presented in Figure 7 represents just one way of choosing subsets of features for our system. Designing a feature lattice can be thought of as choosing a set of feature subsets: once the probability distributions of the lattice have been chosen, the graph structure of the lattice is determined by the subsumption relations among the sets of conditioning variables. Given a set of N conditioning variables, there are 2N possible subsets, and 22N possible sets of subsets, giving us a doubly exponential number of possible lattices. The particular lattice of Figure 7 was chosen to represent some expected interaction between features. For example, we expect position and voice to interact, and they are always used together. We expect the head word h and the phrase type pt to be relatively independent predictors of the semantic role and therefore include them separately as roots of the backoff structure. Although we will not explore all the possibilities for our lattice, some of the feature interactions are examined more closely in Section 4.3. The final system performed at 80.4% accuracy, which can be compared to the 40.9% achieved by always choosing the most probable role for each target word, essentially chance performance on this task. Results for this system on test data, held out during development of the system, are shown in Table 6. Surprisingly, the EM-based interpolation performed better than the lattice-based system on the held-out test set, but not on the data used to set the weights in the EM-based system. We return to an analysis of which roles are hardest to classify in Section 9.1. Three of our features, position, gov, and path, attempt to capture the syntactic relation between the target word and the constituent to be labeled, and in particular to differentiate the subjects from objects of verbs. To compare these three features directly, experiments were performed using each feature alone in an otherwise identical sysLattice structures for comparing grammatical-function features. tem. Results are shown in Table 7. For the first set of experiments, corresponding to the first column of Table 7, no voice information was used, with the result that the remaining distributions formed the lattice of Figure 8a. (“GF” (grammatical function) in the figure represents one of the features position, gov, and path.) Adding voice information back into the system independently of the grammatical-function feature results Minimal lattice. in the lattice of Figure 8b, corresponding to the second column of Table 7. Choosing distributions such that the grammatical function and voice features are always used together results in Figure 8c, corresponding to the third column of Table 7. In each case, as in previous results, the grammatical function feature was used only when the candidate constituent was an NP. The last row of Table 7 shows results using no grammatical-function feature: the distributions making use of GF are removed from the lattices of Figure 8. As a guideline for interpreting these results, with 8,167 observations, the threshold for statistical significance with p < .05 is a 1.0% absolute difference in performance. It is interesting to note that looking at a constituent’s position relative to the target word performed as well as either of our features that read grammatical function off the parse tree, both with and without passive information. The gov and path features seem roughly equivalent in performance. Using head word, phrase type, and target word without either position or grammatical function yielded only 76.3% accuracy, indicating that although the two features accomplish a similar goal, it is important to include some measure of the constituent’s relationship to the target word, whether relative position or either of the syntactic features. Use of the active/passive voice feature seems to be beneficial only when the feature is tied to grammatical function: the second column in Table 7 shows no improvement over the first, while the right-hand column, where grammatical function and voice are tied, shows gains (although only trends) of at least 0.5% in all cases. As before, our three indicators of grammatical function seem roughly equivalent, with the best result in this case being the gov feature. The lattice of Figure 8c performs as well as our system of Figure 7, indicating that including both position and either of the syntactic relations is redundant. As an experiment to see how much can be accomplished with as simple a system as possible, we constructed the minimal lattice of Figure 9, which includes just two distributions, along with a prior for the target word to be used as a last resort when no data are available. This structure assumes that head word and grammatical function are independent. It further makes no use of the voice feature. We chose the path feature as the representation of grammatical function in this case. This system classified 76.3% of frame elements correctly, indicating that one can obtain roughly nine-tenths the performance of the full system with a simple approach. (We will return to a similar system for the purposes of cross-domain experiments in Section 9.) In this section we examine the system’s performance on the task of locating the frame elements in a sentence. Although our probability model considers the question of finding the boundaries of frame elements separately from the question of finding the correct label for a particular frame element, similar features are used to calculate both probabilities. In the experiments below, the system is no longer given frame element boundaries but is still given as inputs the human-annotated target word and the frame to which it belongs. We do not address the task of identifying which frames come into play in a sentence but envision that existing word sense disambiguation techniques could be applied to the task. As before, features are extracted from the sentence and its parse and are used to calculate probability tables, with the predicted variable in this case being fe, a binary indicator of whether a given constituent in the parse tree is or is not a frame element. The features used were the path feature of Section 4.1.3, the identity of the target word, and the identity of the constituent’s head word. The probability distributions calculated from the training data were P(fe  |path), P(fe  |path, t), and P(fe  |h, t), where fe indicates an event where the parse constituent in question is a frame element, path the path through the parse tree from the target word to the parse constituent, t the identity of the target word, and h the head word of the parse constituent. Some sample values from these distributions are shown in Table 8. For example, the path VBTVPtNP, which corresponds to the direct object of a verbal target word, had a high probability of being a frame element. The table also illustrates cases of sparse data for various feature combinations. By varying the probability threshold at which a decision is made, one can plot a precision/recall curve as shown in Figure 10. P(fe  |path, t) performs relatively poorly because of fragmentation of the training data (recall that only about 30 sentences are available for each target word). Although the lexical statistic P(fe  |h, t) alone is not useful as a classifier, using it in linear interpolation with the path statistics improves results. The curve labeled “interpolation” in Figure 10 reflects a linear interpolation of the form Note that this method can identify only those frame elements that have a corresponding constituent in the automatically generated parse tree. For this reason, it is interesting to calculate how many true frame elements overlap with the results of the system, relaxing the criterion that the boundaries must match exactly. Results for partial matching are shown in Table 9. Three types of overlap are possible: the identified constituent entirely within the true frame element, the true frame element entirely within the identified constituent, and each sequence partially contained by the other. An example of the first case is shown in Figure 11, where the true MESSAGE frame element is Mandarin by a head, but because of an error in the parser output, no constituent exactly matches the frame element’s boundaries. In this case, the system identifies Plot of precision/recall curve for various methods of identifying frame elements. Recall is calculated over only frame elements with matching parse constituents. Exactly matching boundaries 66% 5,421 Identified constituent entirely within true frame element 8 663 True frame element entirely within identified constituent 7 599 Both partially within the other 0 26 No overlap with any true frame element 13 972 two frame elements, indicated by shading, which together span the true frame element. When the automatically identified constituents were fed through the role-labeling system described above, 79.6% of the constituents that had been correctly identified in the first stage were assigned the correct role in the second, roughly equivalent to the performance when roles were assigned to constituents identified by hand. A more sophisticated integrated system for identifying and labeling frame elements is described in Section 7.1. As can be seen from Table 3, information about the head word of a constituent is valuable in predicting the constituent’s role. Of all the distributions presented, An example of overlap between identified frame elements and the true boundaries, caused by parser error. In this case two frame elements identified by the classifier (shaded subtrees) are entirely within the human annotation (indicated below the sentence), contributing two instances to row 2 of Table 9. P(r I h, pt, t) predicts the correct role most often (87.4% of the time) when training data for a particular head word have been seen. Because of the large vocabulary of possible head words, however, it also has the smallest coverage, meaning that it is likely that, for a given case in the test data, no frame element with the same head word will have been seen in the set of training sentences for the target word in question. To capitalize on the information provided by the head word, we wish to find a way to generalize from head words seen in the training data to other head words. In this section we compare three different approaches to the task of generalizing over head words: automatic clustering of a large vocabulary of head words to identify words with similar semantics; use of a hand-built ontological resource, WordNet, to organize head words in a semantic hierarchy; and bootstrapping to make use of unlabeled data in training the system. We will focus on frame elements filled by noun phrases, which constitute roughly half the total. To find groups of head words that are likely to fill the same semantic roles, an automatic clustering of nouns was performed using word co-occurrence data from a large corpus. This technique is based on the expectation that words with similar semantics will tend to co-occur with the same other sets of words. For example, nouns describing foods will tend to occur as direct objects of verbs such as eat devour, and savor. The clustering algorithm attempts to find such patterns of co-occurrence from the counts of grammatical relations between pairs of specific words in the corpus, without the use of any external knowledge or semantic representation. We extracted verb–direct object relations from an automatically parsed version of the British National Corpus, using the parser of Carroll and Rooth (1998).4 Clustering was performed using the probabilistic model of co-occurrence described in detail by Hofmann and Puzicha (1998). (For other natural language processing [NLP] applications of the probabilistic clustering algorithm, see, e.g., Rooth [1995], Rooth et al. [1999]; for application to language modeling, see Gildea and Hofmann [1999]. According to this model, the two observed variables, in this case the verb and the head noun of its object, can be considered independent given the value of a hidden cluster variable, c: One begins by setting a priori the number of values that c can take and using the EM algorithm to estimate the distributions P(c), P(n  |c), and P(v  |c). Deterministic annealing was used to prevent overfitting of the training data. We are interested only in the clusters of nouns given by the distribution P(n  |c): the verbs and the distribution P(v  |c) are thrown away once training is complete. Other grammatical relations besides direct object could be used, as could a set of relations. We used the direct object (following other clustering work such as Pereira, Tishby, and Lee [1993]) because it is particularly likely to exhibit semantically significant selectional restrictions. A total of 2,610,946 verb-object pairs were used as training data for the clustering, with a further 290,105 pairs used as a cross-validation set to control the parameters of the clustering algorithm. Direct objects were identified as noun phrases directly under a verb phrase node—not a perfect technique, since it also finds nominal adjuncts such as “I start today.” Forms of the verb to be were excluded from the data, as its cooccurrence patterns are not semantically informative. The number of values possible for the latent cluster variable was set to 256. (Comparable results were found with 64 clusters; the use of deterministic annealing prevents large numbers of clusters from resulting in overfitting.) The soft clustering of nouns thus generated is used as follows: for each example in the frame element–annotated training data, probabilities for values of the hidden cluster variable were calculated using Bayes’ rule: The clustering was applied only to noun phrase constituents; the distribution P(n  |c) from the clustering is used as a distribution P(h  |c) over noun head words. Using the cluster probabilities, a new estimate of P(r  |c, pt, t) is calculated for cases where pt, the phrase type or syntactic category of the constituent, is NP: � where j is an index ranging over the frame elements in the training set and their associated features pt, t, h and their semantic roles r. During testing, a smoothed estimate of the head word–based role probability is calculated by marginalizing over cluster values: As with the other methods of generalization described in this section, automatic clustering was applied only to noun phrases, which represent 50% of the constituents in the test data. We would not expect head word to be as valuable for other phrase types. The second most common category is prepositional phrases. The head of a prepositional phrase (PP) is considered to be the preposition, according to the rules we use, and because the set of prepositions is small, coverage is not as great a problem. Furthermore, the preposition is often a direct indicator of the semantic role. (A more complete model might distinguish between cases in which the preposition serves as a case or role marker and others in which it is semantically informative, with clustering performed on the preposition’s object in the former case. We did not attempt to make this distinction.) Phrase types other than NP and PP make up only a small proportion of the data. Table 10 shows results for the use of automatic clustering on constituents identified by the parser as noun phrases. As can be seen in the table, the vocabulary used for clustering includes almost all (97.9%) of the test data, and the decrease in accuracy from direct lexical statistics to clustered statistics is relatively small (from 87.0% to 79.7%). When combined with the full system described above, clustered statistics increase performance on NP constituents from 83.4% to 85.0% (statistically significant at p < .05). Over the entire test set, this translates into an improvement from 80.4% to 81.2%. The automatic clustering described above can be seen as an imperfect method of deriving semantic classes from the vocabulary, and we might expect a hand-developed set of classes to do better. We tested this hypothesis using WordNet (Fellbaum 1998), a freely available semantic hierarchy. The basic technique, when presented with a head word for which no training examples had been seen, was to ascend the type hierarchy until reaching a level for which training data are available. To do this, counts of training data were percolated up the semantic hierarchy in a technique similar to that of, for example, McCarthy (2000). For each training example, the count #(r, s, pt, t) was incremented in a table indexed by the semantic role r, WordNet sense s, phrase type pt, and target word t, for each WordNet sense s above the head word h in the hypernym hierarchy. In fact, the WordNet hierarchy is not a tree, but rather includes multiple inheritance. For example, person has as hypernyms both life form and causal agent. In such cases, we simply took the first hypernym listed, effectively converting the structure into a tree. A further complication is that several WordNet senses are possible for a given head word. We simply used the first sense listed for each word; a word sense disambiguation module capable of distinguishing WordNet senses might improve our results. As with the clustering experiments reported above, the WordNet hierarchy was used only for noun phrases. The WordNet hierarchy does not include pronouns; to increase coverage, the personal pronouns I, me, you, he, she, him, her, we, and us were added as hyponyms of person. Pronouns that refer to inanimate, or both animate and inanimate, objects were not included. In addition, the CELEX English lexical database (Baayen, Piepenbrock, and Gulikers 1995) was used to convert plural nouns to their singular forms. As shown in Table 11, accuracy for the WordNet technique is roughly the same as that in the automatic clustering results in Table 10: 84.3% on NPs, as opposed to 85.0% with automatic clustering. This indicates that the error introduced by the unsupervised clustering is roughly equivalent to the error caused by our arbitrary choice of the first WordNet sense for each word and the first hypernym for each WordNet sense. Coverage for the WordNet technique is lower, however, largely because of the absence of proper nouns from WordNet, as well as the absence of nonanimate pronouns (both personal pronouns such as it and they and indefinite pronouns such as something and anyone). A dictionary of proper nouns would likely help improve coverage, and a module for anaphora resolution might help cases with pronouns, with or without the use of WordNet. The conversion of plural forms to singular base forms was an important part of the success of the WordNet system, increasing coverage from 71.0% to 80.8%. Of the remaining 19.2% of all noun phrases not covered by the combination of lexical and WordNet sense statistics, 22% consisted of head words defined in WordNet, but for which no training data were available for any hypernym, and 78% consisted of head words not defined in WordNet. A third way of attempting to improve coverage of the lexical statistics is to “bootstrap,” or label unannotated data with the automatic system described in Sections 4 and 5 and use the (imperfect) result as further training data. This can be considered a variant of the EM algorithm, although we use the single most likely hypothesis for the unannotated data, rather than calculating the expectation over all hypotheses. Only one iteration of training on the unannotated data was performed. The unannotated data used consisted of 156,590 sentences containing the target words of our corpus, increasing the total amount of data available to roughly six times the 36,995 annotated training sentences. Table 12 shows results on noun phrases for the bootstrapping method. The accuracy of a system trained only on data from the automatic labeling (Panto) is 81.0%, reasonably close to the 87.0% for the system trained only on annotated data (Ptrai,,,). Combining the annotated and automatically labeled data increases coverage from 41.6% to 54.7% and performance to 44.5%. Because the automatically labeled data are not as accurate as the annotated data, we can do slightly better by using the automatic data only in cases where no training data are available, backing off to the distribution Panto from Ptrai,,,. The fourth row of Table 12 shows results with Panto incorporated into the backoff lattice of all the features of Figure 7, which actually resulted in a slight decrease in performance from the system without the bootstrapped data, shown in the third row. This is presumably because, although the system trained on automatically labeled data performed with reasonable accuracy, many of the cases it classifies correctly overlap with the training data. In fact our backing-off estimate of P(r h, pt, t) classifies correctly only 66% of the additional cases that it covers over Ptrain(r h, pt, t). The three methods of generalizing lexical statistics each had roughly equivalent accuracy on cases for which they were able to derive an estimate of the role probabilities for unseen head words. The differences between the three were primarily due to how much they could improve the coverage of the estimator, that is, how many new noun heads they were able to handle. The automatic-clustering method performed by far the best on this metric; only 2.1% of test cases were unseen in the data used for the automatic clustering. This indicates how much can be achieved with unsupervised methods given very large training corpora. The bootstrapping technique described here, although it has a similar unsupervised flavor, made use of much less data than the corpus used for noun clustering. Unlike probabilistic clustering, the bootstrapping technique can make use of only those sentences containing the target words in question. The WordNet experiment, on the other hand, indicates both the usefulness of hand-built resources when they apply and the difficulty of attaining broad coverage with such resources. Combining the three systems described would indicate whether their gains are complementary or overlapping. One of the primary difficulties in labeling semantic roles is that one predicate may be used with different argument structures: for example, in the sentences “He opened the door” and “The door opened,” the verb open assigns different semantic roles to its syntactic subject. In this section we compare two strategies for handling this type of alternation in our system: a sentence-level feature for frame element groups and a subcategorization feature for the syntactic uses of verbs. Then a simple system using the predicate’s argument structure, or syntactic signature, as the primary feature will be contrasted with previous systems based on local, independent features. The system described in previous sections for classifying frame elements makes an important simplifying assumption: it classifies each frame element independent of the decisions made for the other frame elements in the sentence. In this section we remove this assumption and present a system that can make use of the information that, for example, a given target word requires that one role always be present or that having two instances of the same role is extremely unlikely. To capture this information, we introduce the notion of a frame element group, which is the set of frame element roles present in a particular sentence (technically a multiset, as duplicates are possible, though quite rare). Frame element groups (FEGs) are unordered: examples are shown in Table 13. Sample probabilities from the training data for the frame element groups of the target word blame are shown in Table 14. The FrameNet corpus recognizes three types of “null-instantiated” frame elements (Fillmore 1986), which are implied but do not appear in the sentence. An example of null instantiation is the sentence “Have you eaten?” where food is understood. We did not attempt to identify such null elements, and any null-instantiated roles are not included in the sentence’s FEG. This increases the variability of observed FEGs, as a predicate may require a certain role but allow it to be null instantiated. Our system for choosing the most likely overall assignment of roles for all the frame elements of a sentence uses an approximation that we derive beginning with the true probability of the optimal role assignment r*: where P(r1...n I t,f1...n) represents the probability of an overall assignment of roles ri to each of the n constituents of a sentence, given the target word t and the various features fi of each of the constituents. In the first step we apply Bayes’ rule to this and in the second we make the assumption that the features of the various constituents of a sentence are independent given the target word and each constituent’s role and discard the term P(f1...n  |t), which is constant with respect to r: We estimate the prior over frame element assignments as the probability of the frame element groups, represented with the set operator {}: and finally discard the feature prior P(fi  |t) as being constant over the argmax expression: This leaves us with an expression in terms of the prior for frame element groups of a particular target word P({r1...n}  |t), the local probability of a frame element given a constituent’s features P(ri  |fi, t) on which our previous system was based, and the individual priors for the frame elements chosen P(ri  |t). This formulation can be used to assign roles either when the frame element boundaries are known or when they are not, as we will discuss later in this section. Calculating empirical FEG priors from the training data is relatively straightforward, but the sparseness of the data presents a problem. In fact, 15% of the test sentences had an FEG not seen in the training data for the target word in question. Using the empirical value for the FEG prior, these sentences could never be correctly classified. For this reason, we introduce a smoothed estimate of the FEG prior consisting of a linear interpolation of the empirical FEG prior and the product, for each possible frame element, of the probability of being present or not present in a sentence given the target word: The value of A was empirically set to maximize performance on the development set; a value of 0.6 yielded performance of 81.6%, a significant improvement over the 80.4% of the baseline system. Results were relatively insensitive to the exact value of A. Up to this point, we have considered separately the problems of labeling roles given that we know where the boundaries of the frame elements lie (Section 4, as well as Section 6) and finding the constituents to label in the sentence (Section 5). We now turn to combining the two systems described above into a complete role labeling system. We use equation (16), repeated below, to estimate the probability that a constituent is a frame element: where p is the path through the parse tree from the target word to the constituent, t is the target word, and h is the constituent’s head word. The first two rows of Table 15 show the results when constituents are determined to be frame elements by setting the threshold on the probability P(fe  |p, h, t) to 0.5 and then running the labeling system of Section 4 on the resulting set of constituents. The first two columns of results show precision and recall for the task of identifying frame element boundaries correctly. The second pair of columns gives precision and recall for the combined task of boundary identification and role labeling; to be counted as correct, the frame element must both have the correct boundary and be labeled with the correct role. Contrary to our results using human-annotated boundaries, incorporating FEG priors into the system based on automatically identified boundaries had a negative effect on labeled precision and recall. No doubt this is due to introducing a dependency on other frame element decisions that may be incorrect: the use of FEG priors causes errors in boundary identification to be compounded. One way around this problem is to integrate boundary identification with role labeling, allowing the FEG priors and the role-labeling decisions to affect which constituents are frame elements. This was accomplished by extending the formulation where fei is a binary variable indicating that a constituent is a frame element and P(fei  |fi) is calculated as above. When fei is true, role probabilities are calculated as before; when fei is false, ri assumes an empty role with probability one and is not included in the FEG represented by fr1...nj. One caveat in using this integrated approach is its exponential complexity: each combination of role assignments to constituents is considered, and the number of combinations is exponential in the number of constituents. Although this did not pose a problem when only the annotated frame elements were under consideration, now we Two subcategorizations for the target word open. The relevant production in the parse tree is highlighted. On the left, the value of the feature is “VP → VB NP”; on the right it is “VP → VB.” must include every parse constituent with a nonzero probability for P(fei  |fi). To make the computation tractable, we implement a pruning scheme: hypotheses are extended by choosing assignments for one constituent at a time, and only the top m hypotheses are retained for extension by assignments to the next constituent. Here we set m = 10 after experimentation showed that increasing m yielded no significant improvement. Results for the integrated approach are shown in the last row of Table 15. Allowing role assignments to influence boundary identification improves results both on the unlabeled boundary identification task and on the combined identification and labeling task. The integrated approach puts us in a different portion of the precision/recall curve from the results in the first two rows, as it returns a higher number of frame elements (7,736 vs. 5,719). A more direct comparison can be made by lowering the probability threshold for frame element identification from 0.5 to 0.35 to force the nonintegrated system to return the same number of frame elements as the integrated system. This yields a frame element identification precision of 71.3% and recall of 67.6% and a labeled precision of 60.8% and recall of 57.6%, which is dominated by the result for the integrated system. The integrated system does not have a probability threshold to set; nonetheless it comes closer to identifying the correct number of frame elements (8,167) than does the independent boundary identifier when the theoretically optimal threshold of 0.5 is used with the latter. Recall that use of the FEG prior was motivated by the tendency of verbs to assign differing roles to the same syntactic position. For example, the verb open assigns different roles to the syntactic subject in He opened the door and The door opened. In this section we consider a different feature motivated by these problems: the syntactic subcategorization of the verb. For example, the verb open seems to be more likely to assign the role PATIENT to its subject in an intransitive context and AGENT to its subject in a transitive context. Our use of a subcategorization feature was intended to differentiate between transitive and intransitive uses of a verb. The feature used was the identity of the phrase structure rule expanding the target word’s parent node in the parse tree, as shown in Figure 12. For example, for He closed the door, with close as the target word, the subcategorization feature would be “VP → VB NP.” The subcategorization feature was used only when the target word was a verb. The various part-of-speech tags for verb forms (VBD for past-tense verb forms, VBZ for third-person singular present tense, VBP for other present tense, VBG for present participles, and VBN for past participles) were collapsed into a single tag VB. It is important to note that we are not able to distinguish complements from adjuncts, and our subcategorization feature could be sabotaged by cases such as The door closed yesterday. In the Penn Treebank style, yesterday is considered an NP with tree structure equivalent to that of a direct object. Our subcategorization feature is fairly specific: for example, the addition of an ADVP to a verb phrase will result in a different value. We tested variations of the feature that counted the number of NPs in a VP or the total number of children of the VP, with no significant change in results. The subcategorization feature was used in conjunction with the path feature, which represents the sequence of nonterminals along the path through the parse tree from the target word to the constituent representing a frame element. Making use of the new subcategorization (subcat) feature by adding the distribution P(r  |subcat, path, t) to the lattice of distributions in the baseline system resulted in a slight improvement to 80.8% performance from 80.4%. As with the gov feature in the baseline system, it was found beneficial to use the subcat feature only for NP constituents. Combining the FEG priors and subcategorization feature into a single system resulted in performance of 81.6%, no improvement over using FEG priors without subcategorization. We suspect that the two seemingly different approaches in fact provide similar information. For example, in our hypothetical example of the sentence He opened the door vs. the sentence The door opened, the verb open would have high priors for the FEGs {AGENT, THEME} and {THEME}, but a low prior for {AGENT}. In sentences with only one candidate frame element (the subject in The door closed), the use of the FEG prior will cause it to be labeled THEME, even when the feature probabilities prefer labeling a subject as AGENT. Thus the FEG prior, by representing the set of arguments the predicate is likely to take, essentially already performs the function of the subcategorization feature. The FEG prior allows us to introduce a dependency between the classifications of the sentence’s various constituents with a single parameter. Thus, it can handle the alternation of our example without, for example, introducing the role chosen for one constituent as an additional feature in the probability distribution for the next constituent’s role. It appears that because introducing additional features can further fragment our already sparse data, it is preferable to have a single parameter for the FEG prior. An interesting result reinforcing this conclusion is that some of the argumentstructure features that aided the system when individual frame elements were considered independently are unnecessary when using FEG priors. Removing the features passive and position from the system and using a smaller lattice of only the distributions not employing these features yields an improved performance of 82.8% on the role-labeling task using hand-annotated boundaries. We believe that, because these features pertain to syntactic alternations in how arguments are realized, they overlap with the function of the FEG prior. Adding unnecessary features to the system can reduce performance by fragmenting the training data. In the experiments reported in previous sections, we have used the parse tree returned by a statistical parser as input to the role-labeling system. In this section, we explore the interaction between semantic roles and syntactic parsing by integrating the parser with the semantic-role probability model. This allows the semantic-role assignment to affect the syntactic attachment decisions made by the parser, with the hope of improving the accuracy of the complete system. Although most statistical parsing work measures performance in terms of syntactic trees without semantic information, an assignment of role fillers has been incorporated into a statistical parsing model by Miller et al. (2000) for the domain-specific templates of the Message Understanding Conference (Defense Advanced Research Projects Agency 1998) task. A key finding of Miller et al.’s work was that a system developed by annotating role fillers in text and training a statistical system performed at the same level as one based on writing a large system of rules, which requires much more highly skilled labor to design. We use as the baseline of all our parsing experiments the model described in Collins (1999). The algorithm is a form of chart parsing, which uses dynamic programming to search through the exponential number of possible parses by considering subtrees for each subsequence of the sentence independently. To apply chart parsing to a probabilistic grammar, independence relations must be assumed to hold between the probabilities of a parse tree and the internal structure of its subtrees. In the case of stochastic context-free grammar, the probability of a tree is independent of the internal structure of its subtrees, given the topmost nonterminal of the subtree. The chart-parsing algorithm can simply find the highest-probability parse for each nonterminal for each substring of the input sentence. No lower-probability subtrees will ever be used in a complete parse, and they can be thrown away. Recent lexicalized stochastic parsers such as Collins (1999), Charniak (1997), and others add additional features to each constituent, the most important being the head word of the parse constituent. The statistical system for assigning semantic roles described in the previous sections does not fit easily into the chart-parsing framework, as it relies on long-distance dependencies between the target word and its frame elements. In particular, the path feature, which is used to “navigate” through the sentence from the target word to its likely frame elements, may be an arbitrarily long sequence of syntactic constituents. A path feature looking for frame elements for a target word in another part of the sentence may examine the internal structure of a constituent, violating the independence assumptions of the chart parser. The use of priors over FEGs further complicates matters by introducing sentence-level features dependent on the entire parse. For these reasons, we use the syntactic parsing model without frame element probabilities to generate a number of candidate parses, compute the best frame element assignment for each, and then choose the analysis with the highest overall probability. The frame element assignments are computed as in Section 7.1, with frame element probabilities being applied to every constituent in the parse. To return a large number of candidate parses, the parser was modified to include constituents in the chart even when they were equivalent, according to the parsing model, to a higher-probability constituent. Rather than choosing a fixed n and keeping the n best constituents for each entry in the chart, we chose a probability threshold and kept all constituents within a margin of the highest-probability constituent. Thus the mechanism is similar to the beam search used to prune nonequivalent edges, but a lower threshold was used for equivalent edges (1e vs. 1 100). Using these pruning parameters, an average of 14.9 parses per sentence were obtained. After rescoring with frame element probabilities, 18% of the sentences were assigned a parse different from the original best parse. Nevertheless, the impact on identification of frame elements was small; results are shown in Table 16. The results show a slight, but not statistically significant, increase in recall of frame elements. One possible reason that the improvement is not greater is the relatively small number of parses per sentence available for rescoring. Unfortunately, the parsing algorithm used to generate n-best parses is inefficient, and generating large numbers of parses seems to be computationally intractable. In theory, the complexity of n-best variations of the Viterbi chart-parsing algorithm is quadratic in n. One can simply expand the dynamic programming chart to have n slots for the best solutions to each subproblem, rather than one. As our grammar forms new constituents from pairs of smaller constituents (that is, it internally uses a binarized grammar), for each pair of constituents considered in a single-best parser, up to n2 pairs would be present in the n-best variant. The beam search used by modern parsers, however, makes the analysis more complex. Lexicalization of parse constituents dramatically increases the number of categories that must be stored in the chart, and efficient parsing requires that constituents below a particular probability threshold be dropped from further consideration. In practice, returning a larger number of parses with our algorithm seems to require increasing the pruning beam size to a degree that makes run times prohibitive. In addition to the robustness of even relatively simple parsing models, one explanation for the modest improvement may be the fact that even our integrated system includes semantic information for only one word in the sentence. As the coverage of our frame descriptions increases, it may be possible to do better and to model the interactions between the frames invoked by a text. Most of the statistics used in the system as described above are conditioned on the target word, or predicate, for which semantic roles are being identified. This limits the applicability of the system to words for which training data are available. In Section 6, we attempted to generalize across fillers for the roles of a single predicate. In this section, we turn to the related but somewhat more difficult question of generalizing from seen to unseen predicates. Many ways of attempting this generalization are possible, but the simplest is provided by the frame-semantic information of the FrameNet database. We can use data from target words in the same frame to predict behavior for an unseen word, or, if no data are available for the frame in question, we can use data from the same broad semantic domain into which the frames are grouped. To investigate the degree to which our system is dependent on the set of semantic roles used, we performed experiments using abstract, general semantic roles such as AGENT, PATIENT, and GOAL. Such roles were proposed in theories of linking such as Fillmore (1968) and Jackendoff (1972) to explain the syntactic realization of semantic arguments. This level of roles, often called thematic roles, was seen as useful for expressing generalizations such as “If a sentence has an AGENT, the AGENT will occupy the subject position.” Such correlations might enable a statistical system to generalize from one semantic domain to another. Recent work on linguistic theories of linking has attempted to explain syntactic realization in terms of the fundamentals of verbs’ meaning (see Levin and Rappaport Hovav [1996] for a survey of a number of theories). Although such an explanation is desirable, our goal is more modest: an automatic procedure for identifying semantic roles in text. We aim to use abstract roles as a means of generalizing from limited training data in various semantic domains. We see this effort as consistent with various theoretical accounts of the underlying mechanisms of argument linking, since the various theories all postulate some sort of generalization between the roles of specific predicates. To this end, we developed a correspondence from frame-specific roles to a set of abstract thematic roles. For each frame, an abstract thematic role was assigned to each frame element in the frame’s definition. Since there is no canonical set of abstract semantic roles, we decided upon the list shown in Table 17. We are interested in adjuncts as well as arguments, leading to roles such as DEGREE not found in many theories of verb-argument linking. The difficulty of fitting many relations into standard categories such as AGENT and PATIENT led us to include other roles such as TOPIC. In all, we used 18 roles, a somewhat richer set than is often used, but still much more restricted than the frame-specific roles. Even with this enriched set, not all framespecific roles fit neatly into one category. An experiment was performed replacing each role tag in the training and test data with the corresponding thematic role and training the system as described above on the new dataset. Results were roughly comparable for the two types of semantic roles: overall performance was 82.1% for thematic roles, compared to 80.4% for framespecific roles. This reflects the fact that most frames had a one-to-one mapping from frame-specific to abstract roles, so the tasks were largely equivalent. We expect abstract roles to be most useful when one is generalizing to predicates and frames not found in the training data, the topic of the following sections. One interesting consequence of using abstract roles is that they allow us to compare more easily the system’s performance on different roles because of the smaller number of categories. This breakdown is shown in Table 18. Results are given for two systems: the first assumes that the frame element boundaries are known and the second finds them automatically. The second system, which is described in Section 7.1, corresponds to the rightmost two columns in Table 18. The “Labeled Recall” column shows how often the frame element is correctly identified, whereas the “Unlabeled Recall” column shows how often a constituent with the given role is correctly identified as being a frame element, even if it is labeled with the wrong role. EXPERIENCER and AGENT, two similar roles generally found as the subject for complementary sets of verbs, are the roles that are correctly identified the most often. The “Unlabeled Recall” column shows that these roles are easy to find in the sentence, as a predicate’s subject is almost always a frame element, and the “Known Boundaries” column shows that they are also not often confused with other roles when it is known that they are frame elements. The two most difficult roles in terms of unlabeled recall, MANNER and DEGREE, are typically realized by adverbs or prepositional phrases and considered adjuncts. It is interesting to note that these are considered in FrameNet to be general frame elements that can be used in any frame. STATE Rex spied out Sam Maggott hollering at all and sundry and making good use of his over-sized red gingham handkerchief. Topic He said, “We would urge people to be aware and be alert with fireworks because your fun might be someone else’s tragedy.” This section has shown that our system can use roles defined at a more abstract level than the corpus’s frame-level roles and in fact that when we are looking at a single predicate, the choice has little effect. In the following sections, we attempt to use the abstract roles to generalize the behavior of semantically related predicates. We will present results at different, successively broader levels of generalization, making use of the categorization of FrameNet predicates into frames and more general semantic domains. We first turn to using data from the appropriate frame when no data for the target word are available. Table 19 shows results for various probability distributions using a division of training and test data constructed such that no target words are in common. Every tenth target word was included in the test set. The amount of training data available for each frame varied, from just one target word in some cases to 167 target words in the “perception/noise” frame. The training set contained a total of 75,919 frame elements and the test set 7,801 frame elements. Performance broken down by abstract role. The third column represents accuracy when frame element boundaries are given to the system, and the fourth and fifth columns reflect finding the boundaries automatically. Unlabeled recall includes cases that were identified as a frame element but given the wrong role. The results show a familiar trade-off between coverage and accuracy. Conditioning both the head word and path features on the frame reduces coverage but improves accuracy. A linear interpolation, λ1P(r I path, f ) + λ2P(r I h,f) + λ3P(r I pt, position, voice,f) achieved 79.4% performance on the test set, significantly better than any of the individual distributions and approaching the result of 82.1% for the original system, using target-specific statistics and thematic roles. This result indicates that predicates in the same frame behave similarly in terms of their argument structure, a finding generally consistent with theories of linking that claim that the syntactic realization of verb arguments can be predicted from their semantics. We would expect verbs in the same frame to be semantically similar and to have the same patterns of argument structure. The relatively high performance of frame-level statistics indicates that the Minimal lattice for cross-frame generalization. frames defined by FrameNet are fine-grained enough to capture the relevant semantic similarities. This result is encouraging in that it indicates that a relatively small amount of data can be annotated for a few words in a semantic frame and used to train a system that can then bootstrap to a larger number of predicates. More difficult than the question of unseen predicates in a known frame are frames for which no training data are present. The 67 frames in the current data set cover only a fraction of the English language, and the high cost of annotation makes it difficult to expand the data set to cover all semantic domains. The FrameNet project is defining additional frames and annotating data to expand the scope of the database. The question of how many frames exist, however, remains unanswered for the time being; a full account of frame semantics is expected to include multiple frames being invoked by many words, as well as an inheritance hierarchy of frames and a more detailed representation of each frame’s meaning. In this section, we examine the FrameNet data by holding out an entire frame for testing and using other frames from the same general semantic domain for training. Recall from Figure 1 that domains like COMMUNICATION include frames like CONVERSATION, QUESTIONING, and STATEMENT. Because of the variation in difficulty between different frames and the dependence of the results on which frames are held out for testing, we used a jackknifing methodology. Each frame was used in turn as test data, with all other frames used as training data. The results in Table 20 show average results over the entire data set. Combining the distributions gives a system based on the (very restricted) backoff lattice of Figure 13. This system achieves performance of 51.0%, compared to 82.1% for the original system and 79.4% for the within-frame generalization task. The results show that generalizing across frames, even within a domain, is more difficult than generalizing across target words within a frame. There are several factors that may account for this: the FrameNet domains were intended primarily as a way of organizing the project, and their semantics have not been formalized. Thus, it may not be surprising that they do not correspond to significant generalizations about argument structure. The domains are fairly broad, as indicated by the fact that always choosing the most common role for a given domain (the baseline for cross-frame, within-domain generalization, given as P(r  |d) in Table 20, classifies 28.4% of frame elements correctly) does not do better than the cross-domain baseline of always choosing the most common role from the entire database regardless of domain (P(r) in Table 20, which yields 28.7% correct). This contrasts with a 40.9% baseline for P(r  |t), that is, always choosing the most common role for a particular target word (Table 5, last line). Domain information does not seem to help a great deal, given no information about the frame. Furthermore, the cross-frame experiments here are dependent on the mapping of frame-level roles to abstract thematic roles. This mapping was done at the frame level; that is, FrameNet roles with the same label in two different frames may be translated into two different thematic roles, but all target words in the same frame make use of the same mapping. The mapping of roles within a frame is generally one to one, and therefore the choice of mapping has little effect when using statistics conditioned on the target word and on the frame, as in the previous section. When we are attempting to generalize between frames, the mapping determines which roles from the training frame are used to calculate probabilities for the roles in the test frames, and the choice of mapping is much more significant. The mapping used is necessarily somewhat arbitrary. It is interesting to note that the path feature performs better when not conditioned on the domain. The head word, however, seems to be more domain-specific: although coverage declines when the context is restricted to the semantic domain, accuracy improves. This seems to indicate that the identity of certain role fillers is domainspecific, but that the syntax/semantics correspondence captured by the path feature is more general, as predicted by theories of syntactic linking. As general as they are, the semantic domains of the current FrameNet database cover only a small portion of the language. The domains are defined at the level of, for example, COMMUNICATION and EMOTION; a list of the 12 domains in our corpus is given in Table 1. Whether generalization is possible across domains is an important question for a general language-understanding system. For these experiments, a jackknifing protocol similar to that of the previous section was used, this time holding out one entire domain at a time and using all the others as training material. Results for the path and head word feature are shown in Table 21. The distributions P(r  |path), P(r  |h), and P(r) of Table 21 also appeared in Table 20; the difference between the experiments is only in the division of training and test sets. A linear interpolation, λ1P(r  |path) + λ2P(r  |h), classifies 39.8% of frame elements correctly. This is no better than our result of 40.9% (Table 3) for always choosing a predicate’s most frequent role; however, the cross-domain system does not have role frequencies for the test predicates. As one might expect, as we make successively broader generalizations to semantically more distant predicates, performance degrades. Our results indicate that frame semantics give us a level at which generalizations relevant to argument linking can be made. Our results for unseen predicates within the same frame are encouraging, indicating that the predicates are semantically similar in ways that result in similar argument structure, as the semantically based theories of linking advocated by Levin (1993) and Levin and Rappaport Hovav (1996) would predict. We hope that corpus-based systems such as ours can provide a way of testing and elaborating such theories in the future. We believe that some level of skeletal representation of the relevant aspects of a word’s meaning, along the lines of Kipper et al. (2000) and of the frame hierarchy being developed by the FrameNet project, could be used in the future to help a statistical system generalize from similar words for which training data are available. Our system is able to label semantic roles automatically with fairly high accuracy, indicating promise for applications in various natural language tasks. Semantic roles do not seem to be simple functions of a sentence’s syntactic tree structure, and lexical statistics were found to be extremely valuable, as has been the case in other natural language processing applications. Although lexical statistics are quite accurate on the data covered by observations in the training set, the sparsity of their coverage led us to introduce semantically motivated knowledge sources, which in turn allowed us to compare automatically derived and hand-built semantic resources. Various methods of extending the coverage of lexical statistics indicated that the broader coverage of automatic clustering outweighed its imprecision. Carefully choosing sentence-level features for representing alternations in verb argument structure allowed us to introduce dependencies between frame element decisions within a sentence without adding too much complexity to the system. Integrating semantic interpretation and syntactic parsing yielded only the slightest gain, showing that although probabilistic models allow easy integration of modules, the gain over an unintegrated system may not be large because of the robustness of even simple probabilistic systems. Many aspects of our system are still quite preliminary. For example, our system currently assumes knowledge of the correct frame type for the target word to determine the semantic roles of its arguments. A more complete semantic analysis system would thus require a module for frame disambiguation. It is not clear how difficult this problem is and how much it overlaps with the general problem of word-sense disambiguation. Much else remains to be done to apply the system described here to the interpretation of general text. One technique for dealing with the sparseness of lexical statistics would be the combination of FrameNet data with named-entity systems for recognizing times, dates, and locations, the effort that has gone into recognizing these items, typically used as adjuncts, should complement the FrameNet data, which is more focused on arguments. Generalization to predicates for which no annotated data are available may be possible using other lexical resources or automatic clustering of predicates. Automatically learning generalizations about the semantics and syntactic behavior of predicates is an exciting problem for the years to come. Penn Treebank constituent (or nonterminal) labels. We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus. This work was primarily funded by National Science Foundation grant ITR/HCI #0086132 to the FrameNet project.","Automatic Labeling Of Semantic Roles
We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame.
Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as Agent or Patient, or more domain-specific semantic roles, such as Speaker, Message, and Topic.
The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.
We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence.
These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.
We used various lexical clustering algorithms to generalize across possible fillers of roles.
Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.
Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents.
At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.
Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task.
We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.
We propose the first SRL model on FrameNet.
","The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence.Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles.We also explore the usefulness of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.We present a system",0.9460533261299133,0.8855969309806824,0.9148274064064026
"Generative Models For Statistical Parsing With Combinatory Categorial Grammar This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar. Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies. The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank. However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parsevalmeasures without committing itself on certain se mantically significant decisions, such as predicting null elements arising from deletion or movement. The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations. CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames. As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition. This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)). Since CCG produces unary and binary branching trees with a very fine-grained category set, CCG Parseval scores cannot be compared with scores of standard Treebank parsers. Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels. According to this metric, a local tree with parent node P, head daughter H and non-head daughter S (and position of S relative to P, ie. leftor right, which is implicit in CCG categories) de fines a hP;H;Si dependency between the head word of S, wS, and the head word of H , wH. This measureis neutral with respect to the branching factor. Fur thermore, as noted by Hockenmaier (2001), it doesnot penalize equivalent analyses of multiple modi Computational Linguistics (ACL), Philadelphia, July 2002, pp. 335-342. Proceedings of the 40th Annual Meeting of the Association for Pierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29 N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N > > > > > N N N N (SnNP)n(SnNP) > NP NP NP NP < > > NP S[adj]nNP (S[b]nNP)=PP PP > NPnNP S[b]nNP < < NP S[b]nNP > NP S[dcl]nNP < S[dcl] Figure 1: A CCG derivation in our corpus fiers. In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not whatthe label of the local tree is which defines this depen dency), scores can be compared across grammars with different sets of labels and different kinds of trees. In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser. For further discussion we refer the reader to Clark and Hockenmaier (2002) . CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002). Almost alltypes of construction?with the exception of gap ping and UCP (?Unlike Coordinate Phrases?) arecovered by the translation procedure, which pro cesses 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% of the sentences in the test corpus (WSJ section 23). The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992). Figure1 shows a derivation taken from CCGbank. Categories, such as ((S[b]nNP)=PP)=NP, encode unsaturated subcat frames. The complement-adjunct distinction is made explicit; for instance as a nonexecutive director is marked up as PP-CLR in the Tree bank, and hence treated as a PP-complement of join, whereas Nov. 29 is marked up as an NP-TMP and therefore analyzed as VP modifier. The -CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement, but the translation to CCG is automatic and must do the best it can with the information in the Treebank.The verbal categories in CCGbank carry features distinguishing declarative verbs (and auxiliaries) from past participles in past tense, past par ticiples for passive, bare infinitives and ing-forms. There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement. The derivations in CCGbank are ?normal-form? in the sense that analyses involving the combinatory rules of type-raising and composition are only used when syntactically necessary. Expansion HeadCat NonHeadCat P(exp j : : : ) P(H j : : : ) P(S j : : : ) Baseline P P;exp P;exp;H + Conj P;con jP P;exp;con jP P;exp;H ;con jP + Grandparent P;GP P;GP;exp P;GP;exp;H + ? P#?L;RP P;exp#?L;RP P;exp;H#?L;RP Table 1: The unlexicalized models The models described here are all extensions of a very simple model which models derivations by a top-down tree-generating process. This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1. Given a (parent) node with category P, choose the expansion exp of P, where exp can beleaf (for lexical categories), unary (for unary ex pansions such as type-raising), left (for binary trees where the head daughter is left) or right (binary trees, head right). If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability). All the experiments reported in this section were conducted using sections 02-21 of CCGbank as training corpus, and section 23 as test corpus. We replace all rare words in the training data with their POS-tag. For all experiments reported here and in section 5, the frequency threshold was set to 5. LikeCollins (1999), we assume that the test data is POS tagged, and can therefore replace unknown words inthe test data with their POS-tag, which is more ap propriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words. The performance of the baseline model is shown in the top row of table 3. For six out of the 2379 sentences in our test corpus we do not get a parse.1The reason is that a lexicon consisting of the word category pairs observed in the training corpus does not contain all the entries required to parse the test corpus. We discuss a simple, but imperfect, solution to this problem in section 7. State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes. We too can extend the baseline model described in the previous section by including more features. Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly. In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i","Generative Models For Statistical Parsing With Combinatory Categorial Grammar
This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser.
These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar.
In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies.
The CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing.
The dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features.
","We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.According to this metric, a local tree with parent node P, head daughter H and non-head daughter S (and position of S relative to P, ie. leftor right, which is implicit in CCG categories) de fines a hP;H;Si dependency between the head word of S, w",0.8449679017066956,0.8503581881523132,0.8476544618606567
"Corpus Statistics Meet The Noun Compound: Some Empirical Results Tagged Dependency -.— Tagged Adjacency -e— L. Pattern 3 5 Training scheme (integers denote window widths) Figure 5: Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus. Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus). This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results. Three training schemes have been used and the tuned analysis procedures applied to the test set. Figure 5 shows the resulting accuracy, with accuracy values from figure 3 displayed with dotted lines. If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy. However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%. 4 Conclusion The experiments above demonstrate a number of important points. The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns. At the very least, this information can be applied in broad coverage parsing to assist in the control of search. I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern. While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method. The significance of the use of conceptual association deserves some mention. I have argued that without it a broad coverage system would be impossible. This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed. In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information. Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast. In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature. This result is in accordance with the informal reasoning given in section 1.3. The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets. In all, the most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching. Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing. 5 Acknowledgements This work has received valuable input from people too numerous to mention. The most significant contributions have been made by Richard Buckland, Robert Dale and Mark Dras. I am also indebted to Vance Gledhill, Mike Johnson, Philip Resnik, Richard Sproat, Wilco ter Stal, Lucy Vanderwende and Wobcke. Financial support is gratefully ack- 53 nowledged from the Microsoft Institute and the Australian Government. If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparck Jones, 1983). Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984). While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984), techniques suitable for broad coverage parsing remain unavailable. This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens et al, 1987; Vanderwende, 1993 and Sproat, 1994). The task is illustrated in example 1: The parses assigned to these two compounds differ, even though the sequence of parts of speech are identical. The problem is analogous to the prepositional phrase attachment task explored in Hindle and Rooth (1993). The approach they propose involves computing lexical associations from a corpus and using these to select the correct parse. A similar architecture may be applied to noun compounds. In the experiments below the accuracy of such a system is measured. Comparisons are made across five dimensions: While Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds. Brent (1993) has proposed the use of simple word patterns for the acquisition of verb subcategorisation information. An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below. While such patterns produce false training examples, the resulting noise often only introduces minor distortions. A more I.beral alternative is the use of a cooccurrence window. Yarowsky (1992) uses a fixed 100 word window to collect information used for sense disambiguation. Similarly, Smadja (1993) uses a six content word window to extract significant collocations. A range of windowed training schemes are employed below. Importantly, the use of a window provides a natural means of trading off the amount of data against its quality. When data sparseness undermines the system accuracy, a wider window may admit a sufficient volume of extra accurate data to outweigh the additional noise. There are at least four existing corpus-based algorithms proposed for syntactically analysing noun compounds. Only two of these have been subjected to evaluation, and in each case, no comparison to any of the other three was performed. In fact all authors appear unaware of the other three proposals. I will therefore briefly describe these algorithms. Three of the algorithms use what I will call the ADJACENCY MODEL, an analysis procedure that goes back to Marcus (1980, p253). Therein, the procedure is stated in terms of calls to an oracle which can determine if a noun compound is acceptable. It is reproduced here for reference: Given three nouns n1, n2 and n3: Only more recently has it been suggested that corpus statistics might provide the oracle, and this idea is the basis of the three algorithms which use the adjacency model. The simplest of these is reported in Pustejovsky et al (1993). Given a three word compound, a search is conducted elsewhere in the corpus for each of the two possible subcomponents. Whichever is found is then chosen as the more closely bracketed pair. For example, when backup compiler disk is encountered, the analysis will be: The proposal of Liberman and Sproat (1992) is more sophisticated and allows for the frequency of the words in the compound. Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest. Again, there is no evaluation of the method other than a demonstration that four examples work correctly. The third proposal based on the adjacency model appears in Resnik (1993) and is rather more complex again. The SELECTIONAL ASSOCIATION between a predicate and a word is defined based on the contribution of the word to the conditional entropy of the predicate. The association between each pair of words in the compound is then computed by taking the maximum selectional association from all possible ways of regarding the pair as predicate and argument. Whilst this association metric is complicated, the decision procedure still follows the outline devised by Marcus (1980) above. Resnik (1993) used unambiguous noun compounds from the parsed Wall Street Journal (WSJ) corpus to estimate the association N alues and analysed a test set of around 160 compounds. After some tuning, the accuracy was about 73%, as compared with a baseline of 64% achieved by always bracketing the first two nouns together. The fourth algorithm, first described in Lauer (1994), differs in one striking manner from the other three. It uses what I will call the DEPENDENCY MODEL. This model utilises the following procedure when given three nouns ni, n2 and n3: Figure 1 shows a graphical comparison of the two analysis models. In Lauer (1994), the degree of acceptability is again provided by statistical measures over a corpus. The metric used is a mutual information-like measure based on probabilities of modification relationships. This is derived from the idea that parse trees capture the structure of semantic relationships within a noun compound.' The dependency model attempts to choose a parse which makes the resulting relationships as acceptable as possible. For example, when backup compiler disk is encountered, the analysis will be: I claim that the dependency model makes more intuitive sense for the following reason. Consider the compound calcium ion exchange, which is typically left-branching (that is, the first two words are bracketed together). There does not seem to be any reason why calcium ion should be any more frequent than ion exchange. Both are plausible compounds and regardless of the bracketing, ions are the object of an exchange. Instead, the correct parse depends on whether calcium characterises the ions or mediates the exchange. Another significant difference between the models is the predictions they make about the proportion 'Lauer and Dras (1994) give a formal construction motivating the algorithm given in Lauer (1994). of left and right-branching compounds. Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is twothirds of the time). In the test set used here and in that of Resnik (1993), the proportion of leftbranching compounds is 67% and 64% respectively. In contrast, the adjacency model appears to predict a proportion of 50%. The dependency model has also been proposed by Kobayasi et al (1994) for analysing Japanese noun compounds, apparently independently. Using a corpus to acquire associations, they bracket sequences of Kanji with lengths four to six (roughly equivalent to two or three words). A simple calculation shows that using their own preprocessing hueristics to guess a bracketing provides a higher accuracy on their test set than their statistical model does. This renders their experiment inconclusive. A test set of syntactically ambiguous noun compounds was extracted from our 8 million word Grolier's encyclopedia corpus in the following way.2 Because the corpus is not tagged or parsed, a somewhat conservative strategy of looking for unambiguous sequences of nouns was used. To distinguish nouns from other words, the University of Pennsylvania morphological analyser (described in Karp et al, 1992) was used to generate the set of words that can only be used as nouns (I shall henceforth call this set A). All consecutive sequences of these words were extracted, and the three word sequences used to form the test set. For reasons made clear below, only sequences consisting entirely of words from Roget's thesaurus were retained, giving a total of 308 test triples.3 These triples were manually analysed using as context the entire article in which they appeared. In some cases, the sequence was not a noun compound (nouns can appear adjacent to one another across various constituent boundaries) and was marked as an error. Other compounds exhibited what Hindle and Rooth (1993) have termed SEMANTIC INDETERMINACY where the two possible bracketings cannot be distinguished in the context. The remaining compounds were assigned either a left-branching or right-branching analysis. Table 1 shows the number of each kind and an example of each. Accuracy figures in all the results reported below were computed using only those 244 compounds which received a parse. One problem with applying lexical association to noun compounds is the enormous number of parameters required, one for every possible pair of nouns. Not only does this require a vast amount of memory space, it creates a severe data sparseness problem since we require at least some data about each parameter. Resnik and Hearst (1993) coined the term CONCEPTUAL ASSOCIATION to refer to association values computed between groups of words. By assuming that all words within a group behave similarly, the parameter space can be built in terms of the groups rather than in terms of the words. In this study, conceptual association is used with groups consisting of all categories from the 1911 version of Roget's thesaurus.4 Given two thesaurus categories t1 and t2, there is a parameter which represents the degree of acceptability of the structure [n1n2j where ni is a noun appearing in t1 and n2 appears in t2. By the assumption that words within a group behave similarly, this is constant given the two categories. Following Lauer and Dras (1994) we can formally write this parameter as Pr(ti —.12) where the event ti t2 denotes the modification of a noun in t2 by a noun in t1. To ensure that the test set is disjoint from the training data, all occurrences of the test noun compounds have been removed from the training corpus. Two types of training scheme are explored in this study, both unsupervised. The first employs a pattern that follows Pustejovsky (1993) in counting the occurrences of subcomponents. A training instance is any sequence of four words w1w2w3w4 where tv1,1v4 Ar and w2, w3 E Ar. Let countp(ni, n2) be the number of times a sequence w1n1n2w4 occurs in the training corpus with wi, tv4 H. The second type uses a window to collect training instances by observing how often a pair of nouns cooccur within some fixed number of words. In this study, a variety of window sizes are used. For n > 2, let countn(ni , n2) be the number of times a sequence niwi win2 occurs in the training corpus where i < n — 2. Note that windowed counts are asymmetric. In the case of a window two words wide, this yields the mutual information metric proposed by Liberman and Sproat (1992). Using each of these different training schemes to arrive at appropriate counts it is then possible to estimate the parameters. Since these are expressed in terms of categories rather than words, it is necessary to combine the counts of words to arrive at estimates. In all cases the estimates used are: Here ambig(w) is the number of categories in which w appears. It has the effect of dividing the evidence from a training instance across all possible categories for the words. The normaliser ensures that all parameters for a head noun sum to unity. Given the high level descriptions in section 1.3 it remains only to formalise the decision process used to analyse a noun compound. Each test compound presents a set of possible analyses and the goal is to choose which analysis is most likely. For three word compounds it suffices to compute the ratio of two probabilities, that of a left-branching analysis and that of a right-branching one. If this ratio is greater than unity, then the left-branching analysis is chosen. When it is less than unity, a right-branching analysis is chosen.5 If the ratio is exactly unity, the analyser guesses left-branching, although this is fairly rare for conceptual association as shown by the experimental results below. For the adjacency model, when the given compound is w1w2w3, we can estimate this ratio as: In both cases, we sum over all possible categories for the words in the compound. Because the dependency model equations have two factors, they are affected more severely by data sparseness. If the probability estimate for Pr(t2 t9 is zero for all possible categories 12 and 13 then both the numerator and the denominator will be zero. This will conceal any preference given by the parameters involving 11. In such cases, we observe that the test instance itself provides the information that the event 12 t3 can occur and we recalculate the ratio using Pr(12 19 = Jr for all possible categories 12,13 where k is any non-zero constant. However, no correction is made to the probability estimates for Pr(ti t2) and Pr(ti 13) for unseen cases, thus putting the dependency model on an equal footing with the adjacency model above. The equations presented above for the dependency model differ from those developed in Lauer and Dras (1994) in one way. There, an additional weighting factor (of 2.0) is used to favour a left-branching analysis. This arises because their construction is based on the dependency model which predicts that left-branching analyses should occur twice as often. Also, the work reported in Lauer and Dras (1994) uses simplistic estimates of the probability of a word given its thesaurus category. The equations above assume these probabilities are uniformly constant. Section 3.2 below shows the result of making these two additions to the method. Eight different training schemes have been used to estimate the parameters and each set of estimates used to analyse the test set under both the adjacency and the dependency model. The schemes used are: The accuracy on the test set for all these experiments is shown in figure 2. As can be seen, the dependency model is more accurate than the adjacency model. This is true across the whole spectrum of training schemes. The proportion of cases in which the procedure was forced to guess, either because no data supported either analysis or because both were equally supported, is quite low. For the pattern and two-word window training schemes, the guess rate is less than 4% for both models. In the three-word window training scheme, the guess rates are less than 1%. For all larger windows, neither model is ever forced to guess. In the case of the pattern training scheme, the difference between 68.9% for adjacency and 77.5% for dependency is statistically significant at the 5% level (p = 0.0316), demonstrating the superiority of the dependency model, at least for the compounds within Grolier's encyclopedia. In no case do any of the windowed training schemes outperform the pattern scheme. It seems that additional instances admitted by the windowed schemes are too noisy to make an improvement. Initial results from applying these methods to the EMA corpus have been obtained by Wilco ter Stal (1995), and support the conclusion that the dependency model is superior to the adjacency model. Lauer and Dras (1994) suggest two improvements to the method used above. These are: tion in the probability of categories. While these changes are motivated by the dependency model, I have also applied them to the adjacency model for comparison. To implement them, equations 1 and 2 must be modified to incorporate a factor of in each term of the sum and the l 1 td1t2iissi entire ratio must be multiplied by two. Five training schemes have been applied with these extensions. The accuracy results are shown in figure 3. For comparison, the untuned accuracy figures are shown with dotted lines. A marked improvement is observed for the adjacency model, while the dependency model is only slightly improved. To determine the difference made by conceptual association, the pattern training scheme has been retrained using lexical counts for both the dependency and adjacency model, but only for the words in the test set. If the same system were to be applied across all of Ar (a total of 90,000 nouns), then around 8.1 billion parameters would be required. Left-branching is favoured by a factor of two as described in the previous section, but no estimates for the category probabilities are used (these being meaningless for the lexical association method). Accuracy and guess rates are shown in figure 4. Conceptual association outperforms lexical association, presumably because of its ability to generalise. One problem with the training methods given in section 2.3 is the restriction of training data to nouns in H. Many nouns, especially common ones, have verbal or adjectival usages that preclude them from being in N. Yet when they occur as nouns, they still provide useful training information that the current system ignores. To test whether using tagged data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus. Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus). This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results. Three training schemes have been used and the tuned analysis procedures applied to the test set. Figure 5 shows the resulting accuracy, with accuracy values from figure 3 displayed with dotted lines. If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy. However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%. The experiments above demonstrate a number of important points. The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns. At the very least, this information can be applied in broad coverage parsing to assist in the control of search. I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern. While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method. The significance of the use of conceptual association deserves some mention. I have argued that without it a broad coverage system would be impossible. This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed. In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information. Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast. In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature. This result is in accordance with the informal reasoning given in section 1.3. The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets. In all, the most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching. Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing. This work has received valuable input from people too numerous to mention. The most significant contributions have been made by Richard Buckland, nowledged from the Microsoft Institute and the Australian Government.","Corpus Statistics Meet The Noun Compound: Some Empirical Results
A variety of statistical methods for noun compound analysis are implemented and compared.
The results support two main conclusions.
First, the use of conceptual association not only enables a broad coverage, but also improves the accuracy.
Second, an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents, even though the latter is more prevalent in the literature.
We propose an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus.
We test both adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words.
","Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus).In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature.The most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching.While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguists (Finin, 1980; McDonald, 1982; Isabelle, 1984), techniques suitable for broad coverage processing.",0.8376703858375549,0.8488077521324158,0.8432022929191589
"Building a Large Annotated Corpus of English: The Penn Treebank Mitchell P. Marcus* University of Pennsylvania Mary Ann Marcinkiewicz~ University of Pennsylvania Beatrice Santorini t Northwestern University 1. Introduction There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora re beginning to serve as important research tools for investigators in natural anguage processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic onstruction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models. In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1. The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and pre- senting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of ? Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA 19104. f Department of Linguistics, Northwestern University, Evanston, IL 60208. :~ Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA 19104. 1 A distinction is sometimes made between a corpus as a carefully structured set of materials gathered together to jointly meet some design principles, and a collection, which may be much more opportunistic n construction. We acknowledge that from this point of view, the raw materials of the Penn Treebank form a collection. (~) 1993 Association for Computational Linguistics Computational Linguistics Volume 19, Number 2 the POS tagging phase is automatically parsed and simplified to yield a skeletal syn- tactic representation, which is then corrected by human annotators. After presenting the set of syntactic tags that we use, we illustrate and discuss the bracketing process. In particular, we will outline various factors that affect the speed with which annotators are able to correct bracketed structures, a task that--not surprisingly--is considerably more difficult than correcting POS-tagged text. Finally, Section 5 describes the com- position and size of the current Treebank corpus, briefly reviews some of the research projects that have relied on it to date, and indicates the directions that the project is likely to take in the future. Part-of-Speech Tagging 2.1 A Simplified POS Tagset for English The POS tagsets used to annotate large corpora in the past have traditionally been fairly extensive. The pioneering Brown Corpus distinguishes 87 simple tags (Francis 1964; Francis and Ku~era 1982) and allows the formation of compound tags; thus, the contraction I m is tagged as PPSS+BEM (PPSS for ""non-third person nominative per- sonal pronoun"" and BEM for ""am, m"". 2 Subsequent projects have tended to elaborate the Brown Corpus tagset. For instance, the Lancaster-Oslo/Bergen (LOB) Corpus uses about 135 tags, the Lancaster UCREL group about 165 tags, and the London-Lund Cor- pus of Spoken English 197 tags. 3 The rationale behind developing such large, richly articulated tagsets is to approach ""the ideal of providing distinct codings for all classes of words having distinct grammatical behaviour"" (Garside, Leech, and Sampson 1987, p. 167). 2.1.1 Recoverability. Like the tagsets just mentioned, the Penn Treebank tagset is based on that of the Brown Corpus. However, the stochastic orientation of the Penn Tree- bank and the resulting concern with sparse data led us to modify the Brown Corpus tagset by paring it down considerably. A key strategy in reducing the tagset was to eliminate redundancy by taking into account both lexical and syntactic information. Thus, whereas many POS tags in the Brown Corpus tagset are unique to a particular lexical item, the Penn Treebank tagset strives to eliminate such instances of lexical re- dundancy. For instance, the Brown Corpus distinguishes five different forms for main verbs: the base form is tagged VB, and forms with overt endings are indicated by appending D for past tense, G for present participle/gerund, N for past participle, and Z for third person singular present. Exactly the same paradigm is recognized for have, but have (regardless of whether it is used as an auxiliary or a main verb) is as- signed its own base tag HV. The Brown Corpus further distinguishes three forms of do--the base form (DO), the past tense (DOD), and the third person singular present (DOZ), 4 and eight forms of be--the five forms distinguished for regular verbs as well as the irregular forms am (BEM), are (BER), and was (BEDZ). By contrast, since the distinctions between the forms of VB on the one hand and the forms of BE, DO, and HV on the other are lexically recoverable, they are eliminated in the Penn Treebank, as shown in Table 1. 5 2 Counting both simple and compound tags, the Brown Corpus tagset contains 187 tags. 3 A useful overview of the relation of these and other tagsets to each other and to the Brown Corpus tagset is given in Appendix B of Garside, Leech, and Sampson (1987). 4 The gerund and the participle of do are tagged VBG and VBN in the Brown Corpus, respectively--presumably because they are never used as auxiliary verbs in American English. 5 The irregular present ense forms am and are are tagged as VBP in the Penn Treebank (see Section 2.1.3), just like any other non-third person singular present ense form. 314 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English Table 1 Elimination of lexically recoverable distinctions. sing/VB be/VB do/VB have/VB sings/VBZ is/VBZ does/VBZ has/VBZ sang/VBD was/VBD did/VBD had/VBD singing/VBG being/VBG doing/VBG having/VBG sung/VBN been/VBN done/VBN had/VBN A second example of lexical recoverability concerns those words that can precede articles in noun phrases. The Brown Corpus assigns a separate tag to pre-qualifiers (quite, rather, such), pre-quantifiers (all, half, many, nary) and both. The Penn Treebank, on the other hand, assigns all of these words to a single category PDT (predeterminer). Further examples of lexically recoverable categories are the Brown Corpus categories PPL (singular eflexive pronoun) and PPLS (plural reflexive pronoun), which we col- lapse with PRP (personal pronoun), and the Brown Corpus category RN (nominal adverb), which we collapse with RB (adverb). Beyond reducing lexically recoverable distinctions, we also eliminated certain POS distinctions that are recoverable with reference to syntactic structure. For instance, the Penn Treebank tagset does not distinguish subject pronouns from object pronouns even in cases where the distinction is not recoverable from the pronouns form, as with you, since the distinction is recoverable on the basis of the pronouns position in the parse tree in the parsed version of the corpus. Similarly, the Penn Treebank tagset conflates ubordinating conjunctions with prepositions, tagging both categories as IN. The distinction between the two categories i not lost, however, since subor- dinating conjunctions can be recovered as those instances of IN that precede clauses, whereas prepositions are those instances of IN that precede noun phrases or preposi- tional phrases. We would like to emphasize that the lexical and syntactic recoverability inherent in the POS-tagged version of the Penn Treebank corpus allows end users to employ a much richer tagset han the small one described in Section 2.2 if the need arises. 2.1.2 Consistency. As noted above, one reason for eliminating a POS tag such as RN (nominal adverb) is its lexical recoverability. Another important reason for doing so is consistency. For instance, in the Brown Corpus, the deictic adverbs there and now are always tagged RB (adverb), whereas their counterparts here and then are inconsistently tagged as RB (adverb) or RN (nominal adverb) even in identical syntactic ontexts, such as after a preposition. It is clear that reducing the size of the tagset reduces the chances of such tagging inconsistencies. 2.1.3 Syntactic Function. A further difference between the Penn Treebank and the Brown Corpus concerns the significance accorded to syntactic ontext. In the Brown Corpus, words tend to be tagged independently of their syntactic function. 6 For in- stance, in the phrase the one, one is always tagged as CD (cardinal number), whereas 6 An important exception is there, which the Brown Corpus tags as EX (existential there) when it is used as a formal subject and as RB (adverb) when it is used as a locative adverb. In the case of there, we did not pursue our strategy oftagset reduction toits logical conclusion, which would have implied tagging existential there as NN (common noun). 315 Computational Linguistics Volume 19, Number 2 in the corresponding plural phrase the ones, ones is always tagged as NNS (plural com- mon noun), despite the parallel function of one and ones as heads of the noun phrase. By contrast, since one of the main roles of the tagged version of the Penn Treebank corpus is to serve as the basis for a bracketed version of the corpus, we encode a words syntactic function in its POS tag whenever possible. Thus, one is tagged as NN (singular common noun) rather than as CD (cardinal number) when it is the head of a noun phrase. Similarly, while the Brown Corpus tags both as ABX (pre-quantifier, double conjunction), regardless of whether it functions as a prenominal modifier (both the boys), a postnominal modifier (the boys both), the head of a noun phrase (both of the boys) or part of a complex coordinating conjunction (both boys and girls), the Penn Treebank tags both differently in each of these syntactic ontexts--as PDT (predeter- miner), RB (adverb), NNS (plural common noun) and coordinating conjunction (CC), respectively. There is one case in which our concern with tagging by syntactic function has led us to bifurcate Brown Corpus categories rather than to collapse them: namely, in the case of the uninflected form of verbs. Whereas the Brown Corpus tags the bare form of a verb as VB regardless of whether it occurs in a tensed clause, the Penn Treebank tagset distinguishes VB (infinitive or imperative) from VBP (non-third person singular present ense). 2.1.4 Indeterminacy. A final difference between the Penn Treebank tagset and all other tagsets we are aware of concerns the issue of indeterminacy: both POS ambiguity in the text and annotator uncertainty. In many cases, POS ambiguity can be resolved with reference to the linguistic context. So, for instance, in Katharine Hepburns witty line Grant can be outspoken--but not by anyone I know, the presence of the by-phrase forces us to consider outspoken as the past participle of a transitive derivative of speak-- outspeak rather than as the adjective outspoken. However, even given explicit criteria for assigning POS tags to potentially ambiguous words, it is not always possible to assign a unique tag to a word with confidence. Since a major concern of the Treebank is to avoid requiring annotators to make arbitrary decisions, we allow words to be associated with more than one POS tag. Such multiple tagging indicates either that the words part of speech simply cannot be decided or that the annotator is unsure which of the alternative tags is the correct one. In principle, annotators can tag a word with any number of tags, but in practice, multiple tags are restricted to a small number of recurring two-tag combinations: JJINN (adjective or noun as prenominal modifier), JJIVBG (adjective or gerund/present participle), JJ[VBN (adjective or past participle), NNIVBG (noun or gerund), and RBIRP (adverb or particle). 2.2 The POS Tagset The Penn Treebank tagset is given in Table 2. It contains 36 POS tags and 12 other tags (for punctuation and currency symbols). A detailed description of the guidelines governing the use of the tagset is available in Santorini (1990). 7 2.3 The POS Tagging Process The tagged version of the Penn Treebank corpus is produced in two stages, using a combination of automatic POS assignment and manual correction. 7 In versions of the tagged corpus distributed before November 1992, singular proper nouns, plural proper nouns, and personal pronouns were tagged as ""NP,"" ""NPS,"" and ""PP,"" respectively. The current tags ""NNP,"" ""NNPS,"" and ""PRP"" were introduced in order to avoid confusion with the syntactic tags ""NP"" (noun phrase) and ""PP"" (prepositional phrase) (see Table 3). 316 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English Table 2 The Penn Treebank POS tagset. CC Coordinating conjunction 25. CD Cardinal number 26. UH Interjection 3. DT Determiner 27. VB Verb, base form 4. EX Existential there 28. VBD Verb, past tense 5. FW Foreign word 29. VBG Verb, gerund/present 6. IN Preposition/subordinating participle conjunction 30. VBN Verb, past participle 7. JJ Adjective 31. VBP Verb, non-3rd ps. JJR Adjective, comparative 32. VBZ Verb, 3rd ps. JJS Adjective, superlative 33. WDT wh-determiner 10. LS List item marker 34. WP wh-pronoun 11. WP$ Possessive wh-pronoun 12. NN Noun, singular or mass 36. WRB wh-adverb 13. NNS Noun, plural 37. # Pound sign 14. NNP Proper noun, singular 38. $ Dollar sign 15. NNPS Proper noun, plural 39.. Sentence-final punctuation 16. PDT Predeterminer 40. , Comma 17. POS Possessive nding 41. : Colon, semi-colon 18. PRP Personal pronoun 42. ( Left bracket character 19. PP$ Possessive pronoun 43. ) Right bracket character 20. RB Adverb 44. "" Straight double quote 21. RBR Adverb, comparative 45. Left open single quote 22. RBS Adverb, superlative 46. "" Left open double quote 23. RP Particle 47. Right close single quote 24. SYM Symbol (mathematical or scientific) 48. "" Right close double quote 2.3.1 Automated Stage. During the early stages of the Penn Treebank project, the initial automatic POS assignment was provided by PARTS (Church 1988), a stochastic algorithm developed at AT&T Bell Labs. PARTS uses a modif ied version of the Brown Corpus tagset close to our own and assigns POS tags with an error rate of 3-5%. The output of PARTS was automatically tokenized 8 and the tags assigned by PARTS were automatically mapped onto the Penn Treebank tagset. This mapp ing  introduces about 4% error, since the Penn Treebank tagset makes certain distinctions that the PARTS tagset does not. 9 A sample of the resulting tagged text, which has an error rate of 7-9%, is shown in Figure 1. More recently, the automatic POS assignment is provided by a cascade of stochastic and rule-driven taggers developed on the basis of our early experience. Since these taggers are based on the Penn Treebank tagset, the 4% error rate introduced as an artefact of mapping from the PARTS tagset o ours is eliminated, and we obtain error rates of 2-6%. 2.3.2 Manual Correction Stage. The result of the first, automated stage of POS tagging is given to annotators to correct. The annotators use a mouse-based package written 8 In contrast to the Brown Corpus, we do not allow compound tags of the sort illustrated above for Im. Rather, contractions and the Anglo-Saxon genitive of nouns are automatically split into their component morphemes, and each morpheme is tagged separately. Thus, childrens is tagged ""children/NNS s/POS,"" and wont is tagged ""wo-/MD nt/RB."" 9 The two largest sources of mapping error are that the PARTS tagset distinguishes neither infinitives from non-third person singular present tense forms of verbs, nor prepositions from particles in cases like run up a hill and run up a bill. 317 Computational Linguistics Volume 19, Number 2 Battle-tested/NNP industrial/JJ managers/NNS here/RB always/RB buck/VB up/IN nervous/JJ newcomers/NNS with/IN the/DT tale/NN of/IN the/DT first/JJ of/IN their/PP$ countrymen/NNS to/TO visit/VB Mexico/NNP ,/, a/DT boatload/NN of/IN samurai/NNS warriors/NNS blown/VBN ashore/RB 375/CD years/NNS ago/RB ./. ""/"" From/IN the/DT beginning/NN ,/, it/PRP took/VBD a/DT man/NN with/IN extraordinary/JJ qualities/NNS to/TO succeed/VB in/IN Mexico/NNP ,/, ""/"" says/VBZ Kimihide/NNP Takimura/NNP ,/, president/NN of/IN Mitsui/NNS group/NN s/POS Kensetsu/NNP Engineering/NNP Inc./NNP unit/NN ./. Figure 1 Sample tagged text--before correction. Battle-tested/NNP*/JJ industrial/JJ managers/NNS here/RB always/RB buck/VB*/VBP up/IN*/RP nervous/JJ newcomers/NNS with/IN the/DT tale/NN of/IN the/DT first/JJ of/IN their/PP$ countrymen/NNS to/TO visit/VB Mexico/NNP ,/, a/DT boatload/NN of/IN samurai/NNS*/FW warriors/NNS blown/VBN ashore/RB 375/CD years/NNS ago/RB ./. ""/"" From/IN the/DT beginning/NN ,/, it/PRP took/VBD a/DT man/NN with/IN extraordinary/JJ qualities/NNS to/TO succeed/VB in/IN Mexico/NNP ,/, "" /""  says/VBZ Kimihide/NNP Takimura/NNP ,/, president/NN of/IN Mitsui/NNS*/NNP group/NN s/POS Kensetsu/NNP Engineering/NNP Inc./NNP unit/NN ./. Figure 2 Sample tagged text--after correction. in GNU Emacs Lisp, which is embedded within the GNU Emacs editor (Lewis et al. The package allows annotators to correct POS assignment errors by positioning the cursor on an incorrectly tagged word and then entering the desired correct ag (or sequence of multiple tags). The annotators input is automatically checked against the list of legal tags in Table 2 and, if valid, appended to the original word-tag pair separated by an asterisk. Appending the new tag rather than replacing the old tag allows us to easily identify recurring errors at the automatic POS assignment s age. We believe that the confusion matrices that can be extracted from this information should also prove useful in designing better automatic taggers in the future. The result of this second stage of POS tagging is shown in Figure 2. Finally, in the distribution version of the tagged corpus, any incorrect tags assigned at the first, automatic stage are removed. The learning curve for the POS tagging task takes under a month (at 15 hours a week), and annotation speeds after a month exceed 3,000 words per hour. Two Modes of AnnotationwAn Experiment To determine how to maximize the speed, inter-annotator consistency, and accuracy of POS tagging, we performed an experiment a the very beginning of the project o com- pare two alternative modes of annotation. In the first annotation mode (""tagging""), annotators tagged unannotated text entirely by hand; in the second mode (""correct- ing""), they verified and corrected the output of PARTS, modified as described above. 318 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English This experiment showed that manual tagging took about twice as long as correcting, with about twice the inter-annotator disagreement rate and an error rate that was about 50% higher. Four annotators, all with graduate training in linguistics, participated in the exper- iment. All completed a training sequence consisting of 15 hours of correcting followed by 6 hours of tagging. The training material was selected from a variety of nonfiction genres in the Brown Corpus. All the annotators were familiar with GNU Emacs at the outset of the experiment. Eight 2,000-word samples were selected from the Brown Cor- pus, two each from four different genres (two fiction, two nonfiction), none of which any of the annotators had encountered in training. The texts for the correction task were automatically tagged as described in Section 2.3. Each annotator first manually tagged four texts and then corrected four automatically tagged texts. Each annotator completed the four genres in a different permutation. A repeated measures analysis of annotation speed with annotator identity, genre, and annotation mode (tagging vs. correcting) as classification variables howed a sig- nificant annotation mode effect (p = .05). No other effects or interactions were signif- icant. The average speed for correcting was more than twice as fast as the average speed for tagging: 20 minutes vs. 44 minutes per 1,000 words. (Median speeds per 1,000 words were 22 vs. 42 minutes.) A simple measure of tagging consistency is inter-annotator disagreement rate, the rate at which annotators disagree with one another over the tagging of lexical tokens, expressed as a percentage of the raw number of such disagreements over the number of words in a given text sample. For a given text and n annotators, there are disagreement ratios (one for each possible pair of annotators). Mean inter-annotator disagreement was 7.2% for the tagging task and 4.1% for the correcting task (with me- dians 7.2% and 3.6%, respectively). Upon examination, a disproportionate amount of disagreement i  the correcting case was found to be caused by one text that contained many instances of a cover symbol for chemical and other formulas. In the absence of an explicit guideline for tagging this case, the annotators had made different decisions on what part of speech this cover symbol represented. When this text is excluded from consideration, mean inter-annotator disagreement for the correcting task drops to 3.5%, with the median unchanged at 3.6%. Consistency, while desirable, tells us nothing about he validity of the annotators corrections. We therefore compared each annotators output not only with the output of each of the others, but also with a benchmark version of the eight texts. This benchmark version was derived from the tagged Brown Corpus by (1) mapping the original Brown Corpus tags onto the Penn Treebank tagset and (2) carefully hand- correcting the revised version in accordance with the tagging conventions in force at the time of the experiment. Accuracy was then computed as the rate of disagreement between each annotators results and the benchmark version. The mean accuracy was 5.4% for the tagging task (median 5.7%) and 4.0% for the correcting task (median 3.4%). Excluding the same text as above gives a revised mean accuracy for the correcting task of 3.4%, with the median unchanged. We obtained a further measure of the annotators accuracy by comparing their error rates to the rates at which the raw output of Churchs PARTS program--appropri- ately modified to conform to the Penn Treebank tagset--disagreed with the benchmark version. The mean disagreement rate between PARTS and the benchmark version was 319 Computational Linguistics Volume 19, Number 2 9.6%, while the corrected version had a mean disagreement rate of 5.4%, as noted above. The annotators were thus reducing the error rate by about 4.2%. Bracketing 4.1 Basic Methodology The methodology for bracketing the corpus is completely parallel to that for tagging-- hand correction of the output of an errorful automatic process. Fidditch, a deterministic parser developed by Donald Hindle first at the University of Pennsylvania nd sub- sequently at AT&T Bell Labs (Hindle 1983, 1989), is used to provide an initial parse of the material. Annotators then hand correct he parsers output using a mouse-based interface implemented in GNU Emacs Lisp. Fidditch has three properties that make it ideally suited to serve as a preprocessor to hand correction: ? Fidditch always provides exactly one analysis for any given sentence, so that annotators need not search through multiple analyses. Fidditch never attaches any constituent whose role in the larger structure it cannot determine with certainty. In cases of uncertainty, Fidditch chunks the input into a string of trees, providing only a partial structure for each sentence. Fidditch has rather good grammatical coverage, so that the grammatical chunks that it does build are usually quite accurate. Because of these properties, annotators do not need to rebracket much of the parsers output--a relatively time-consuming task. Rather, the annotators main task is to ""glue"" together the syntactic hunks produced by the parser. Using a mouse-based interface, annotators move each unattached chunk of structure under the node to which it should be attached. Notational devices allow annotators to indicate uncertainty concerning constituent labels, and to indicate multiple attachment sites for ambiguous modifiers. The bracketing process is described in more detail in Section 4.3. 4.2 The Syntactic Tagset Table 3 shows the set of syntactic tags and null elements that we use in our skeletal bracketing. More detailed information on the syntactic tagset and guidelines concern- ing its use are to be found in Santorini and Marcinkiewicz (1991). Although different in detail, our tagset is similar in delicacy to that used by the Lancaster Treebank Project, except hat we allow null elements in the syntactic anno- tation. Because of the need to achieve a fairly high output per hour, it was decided not to require annotators to create distinctions beyond those provided by the parser. Our approach to developing the syntactic tagset was highly pragmatic and strongly influenced by the need to create a large body of annotated material given limited hu- man resources. Despite the skeletal nature of the bracketing, however, it is possible to make quite delicate distinctions when using the corpus by searching for combinations of structures. For example, an SBAR containing the word to immediately before the VP will necessarily be infinitival, while an SBAR containing a verb or auxiliary with a 10 We would like to emphasize that the percentage given for the modified output of PARTS does not represent an error rate for PARTS. It reflects not only true mistakes in PARTS performance, but also the many and important differences in the usage of Penn Treebank POS tags and the usage of tags in the original Brown Corpus material on which PARTS was trained. 320 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English Table 3 The Penn Treebank syntactic tagset. X Null elements 2. NIL Adjective phrase Adverb phrase Noun phrase Prepositional phrase Simple declarative clause Clause introduced by subordinating conjunction or 0 (see below) Direct question introduced by wh-word or wh-phrase Declarative sentence with subject-aux inversion Subconstituent of SBARQ excluding wh-word or wh-phrase Verb phrase wh-adverb phrase wh-noun phrase wh-prepositional phrase Constituent of unknown or uncertain category ""Understood"" subject of infinitive or imperative Zero variant of that in subordinate clauses Trace--marks position where moved wh-constituent is interpreted Marks position where preposition is interpreted in pied-piping contexts tense feature will necessarily be tensed. To take another example, so-called that-clauses can be identified easily by searching for SBARs containing the word that or the null element 0 in initial position. As can be seen from Table 3, the syntactic tagset used by the Penn Treebank in- cludes a variety of null elements, a subset of the null elements introduced by Fidditch. While it would be expensive to insert null elements entirely by hand, it has not proved overly onerous to maintain and correct hose that are automatically provided. We have chosen to retain these null elements because we believe that they can be exploited in many cases to establish a sentences predicate-argument structure; at least one recipient of the parsed corpus has used it to bootstrap the development of lexicons for partic- ular NLP projects and has found the presence of null elements to be a considerable aid in determining verb transitivity (Robert Ingria, personal communication). While these null elements correspond more directly to entities in some grammatical theories than in others, it is not our intention to lean toward one or another theoretical view in producing our corpus. Rather, since the representational framework for grammatical structure in the Treebank is a relatively impoverished flat context-free notation, the eas- iest mechanism to include information about predicate-argument structure, although indirectly, is by allowing the parse tree to contain explicit null items. 4.3 Sample Bracketing Output Below, we illustrate the bracketing process for the first sentence of our sample text. Figure 3 shows the output of Fidditch (modified slightly to include our POS tags). As Figure 3 shows, Fidditch leaves very many constituents unattached, labeling them as ""? "", and its output is perhaps better thought of as a string of tree fragments than as a single tree structure. Fidditch only builds structure when this is possible for a purely syntactic parser without access to semantic or pragmatic information, and it 321 Computational Linguistics Volume 19, Number 2 ( (s (NP (? Figure 3 (NBAK (ADJP (ADJ ""Battle-tested/JJ"") (ADJ ""industrial/JJ"")) (NPL ""managers/NNS""))) (? (ADV ""here/KB"")) (? (ADV ""always/KB"")) (AUX (TNS *)) (VP (VPKES ""buck/VBP""))) (? (PP (PKEP ""up/KP"") (NP (NBAR (ADJ ""nervous/JJ"") (NPL ""newcomers/NNS""))))) (? (PP (PREP ""with/IN"") (NP (DART ""the/DT"") (NBAK (N ""tale/NN"")) (PP of/PKEP (NP (DART ""the/DT"") (NBAK (ADJP (ADJ ""first/JJ"")))))))) (? (PP of/PREP (NP (PROS ""their/PP$"") (NBAK (NPL ""countrymen/NNS"")))) (? (S (NP (PRO *)) (AUX to/TNS) (VP (V ""visit/VB"") (NP (PNP ""Mexico/NNP""))))) (? (MID "",/,"")) (? (NP (IAKT ""a/DT"") (NBAK (N ""boatload/NN"")) (PP of/PKEP (NP (NBAK (NPL ""warriors/NNS"")))) (VP (VPPKT ""blown/VBN"") (? (ADV ""ashore/KB"")) (NP (NBAR (CARD ""375/CD"") (NPL ""years/NNS"")))))) (ADV ""ago/KB"")) (FIN ""./.""))) Sample bracketed text--full structure provided by Fidditch. always errs on the side of caution. Since determining the correct attachment point of prepositional phrases, relative clauses, and adverbial modifiers almost always requires extrasyntactic information, Fidditch pursues the very conservative strategy of always leaving such constituents unattached, even if only one attachment point is syntacti- cally possible. However, Fidditch does indicate its best guess concerning a fragments attachment site by the fragments depth of embedding. Moreover, it attaches preposi- tional phrases beginning with of if the preposition immediately follows a noun; thus, tale of... and boatload of... are parsed as single constituents, while first of... is not. Since Fidditch lacks a large verb lexicon, it cannot decide whether some constituents serve as adjuncts or arguments and hence leaves subordinate clauses such as infini- 322 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English tives as separate fragments. Note further that Fidditch creates adjective phrases only when it determines that more than one lexical item belongs in the ADJP. Finally, as is well known, the scope of conjunctions and other coordinate structures can only be determined given the richest forms of contextual information; here again, Fidditch simply turns out a string of tree fragments around any conjunction. Because all de- cisions within Fidditch are made locally, all commas (which often signal conjunction) must disrupt he input into separate chunks. The original design of the Treebank called for a level of syntactic analysis compa- rable to the skeletal analysis used by the Lancaster Treebank, but a limited experiment was performed early in the project o investigate the feasibility of providing greater levels of structural detail. While the results were somewhat unclear, there was ev- idence that annotators could maintain a much faster rate of hand correction if the parser output was simplified in various ways, reducing the visual complexity of the tree representations and eliminating a range of minor decisions. The key results of this experiment were as follows: ? Annotators take substantially longer to learn the bracketing task than the POS tagging task, with substantial increases in speed occurring even after two months of training. Annotators can correct he full structure provided by Fidditch at an average speed of approximately 375 words per hour after three weeks and 475 words per hour after six weeks. Reducing the output from the full structure shown in Figure 3 to a more skeletal representation similar to that used by the Lancaster UCREL Treebank Project increases annotator productivity by approximately 100-200 words per hour. It proved to be very difficult for annotators to distinguish between a verbs arguments and adjuncts in all cases. Allowing annotators to ignore this distinction when it is unclear (attaching constituents high) increases productivity by approximately 150-200 words per hour. Informal examination of later annotation showed that forced distinctions cannot be made consistently. As a result of this experiment, the originally proposed skeletal representation was adopted, without a forced distinction between arguments and adjuncts. Even after extended training, performance varies markedly by annotator, with speeds on the task of correcting skeletal structure without requiring a distinction between arguments and adjuncts ranging from approximately 750 words per hour to well over 1,000 words per hour after three or four months experience. The fastest annotators work in bursts of well over 1,500 words per hour alternating with brief rests. At an average rate of 750 words per hour, a team of five part-time annotators annotating three hours a day should maintain an output of about 2.5 million words a year of ""treebanked"" sentences, with each sentence corrected once. It is worth noting that experienced annotators can proofread previously corrected material at very high speeds. A parsed subcorpus of over I million words was recently proofread at an average speed of approximately 4,000 words per annotator per hour. At this rate of productivity, annotators are able to find and correct gross errors in parsing, but do not have time to check, for example, whether they agree with all prepositional phrase attachments. 323 Computational Linguistics Volume 19, Number 2 (S (NP (ADJP Battle-tested industrial) managers) (? always) (VP buck)) (? (PP up (NP nervous newcomers))) (? (PP with (NP the tale (PP of (NP the (ADJP first)))))) (? (PP of (NP their countrymen))) (? (S (NP *) to (VP visit (NP Mexico)))) (? (NP a boatload (PP of (NP warriors)) (VP blown (? ashore) (NP 375 years)))) (? Figure 4 Sample bracketed text--after simplification, before correction. The process that creates the skeletal representations to be corrected by the anno- tators simplifies and flattens the structures shown in Figure 3 by removing POS tags, nonbranching lexical nodes, and certain phrasal nodes, notably NBAR. The output of the first automated stage of the bracketing task is shown in Figure 4. Annotators correct his simplified structure using a mouse-based interface. Their primary job is to ""glue"" fragments ogether, but they must also correct incorrect parses and delete some structure. Single mouse clicks perform the following tasks, among others. The interface correctly reindents the structure whenever necessary. Attach constituents labeled ?. This is done by pressing down the appropriate mouse button on or immediately after the ?, moving the mouse onto or immediately after the label of the intended parent and releasing the mouse. Attaching constituents automatically deletes their ? Promote a constituent up one level of structure, making it a sibling of its current parent. Delete a pair of constituent brackets. 324 Mitchell R Marcus et al. Building a Large Annotated Corpus of English (S (NP Battle-tested industrial managers here) always (VP buck up (NP nervous newcomers) (PP with (NP the tale (PP of (NP (NP the (ADJP first (PP of (NP (S (NP *) to (VP visit (NP (VP-1 .) Figure 5 Sample bracketed text--after correction. their countrymen))) (NP Mexico)))) (NP a boatload (PP of (NP (NP warriors) (VP-I blown ashore (ADVP (NP 375 years) ago))))) *pseudo-attach*)))))))) ? Create a pair of brackets around a constituent. This is done by typing a constituent tag and then sweeping out the intended constituent with the mouse. The tag is checked to assure that it is a legal label. Change the label of a constituent. The new tag is checked to assure that it is legal. The bracketed text after correction is shown in Figure 5. The fragments are now connected together into one rooted tree structure. The result is a skeletal analysis in that much syntactic detail is left unannotated. Most prominently, all internal structure of the NP up through the head and including any single-word post-head modifiers is left unannotated. As noted above in connection with POS tagging, a major goal of the Treebank project is to allow annotators only to indicate structure of which they were certain. The Treebank provides two notational devices to ensure this goal: the X constituent label and so-called ""pseudo-attachment."" The X constituent label is used if an annotator is sure that a sequence of words is a major constituent but is unsure of its syntactic category; in such cases, the annotator simply brackets the sequence and labels it X. The second notational device, pseudo-attachment, hastwo primary uses. On the one hand, 325 Computational Linguistics Volume 19, Number 2 it is used to annotate what Kay has called permanent predictable ambiguities, allowing an annotator to indicate that a structure isglobally ambiguous even given the surrounding context (annotators always assign structure to a sentence on the basis of its context). An example of this use of pseudo-attachment is shown in Figure 5, where the participial phrase blown ashore 375 years ago modifies either warriors or boatload, but there is no way of settling the question--both attachments mean exactly the same thing. In the case at hand, the pseudo-attachment notation indicates that the annotator of the sentence thought hat VP-1 is most likely a modifier of warriors, but that it is also possible that it is a modifier of boatload. 11A second use of pseudo-attachment is toallow annotators to represent the ""underlying"" position of extraposed elements; in addition to being attached in its superficial position in the tree, the extraposed constituent is pseudo- attached within the constituent to which it is semantically related. Note that except for the device of pseudo-attachment, the skeletal analysis of the Treebank is entirely restricted to simple context-free trees. The reader may have noticed that the ADJP brackets in Figure 4 have vanished in Figure 5. For the sake of the overall efficiency of the annotation task, we leave all ADJP brackets in the simplified structure, with the annotators expected to remove many of them during annotation. The reason for this is somewhat complex, but provides a good example of the considerations that come into play in designing the details of annotation methods. The first relevant fact is that Fidditch only outputs ADJP brackets within NPs for adjective phrases containing more than one lexical item. To be consistent, he final structure must contain ADJP nodes for all adjective phrases within NPs or for none; we have chosen to delete all such nodes within NPs under normal circumstances. (This does not affect the use of the ADJP tag for predicative adjective phrases outside of NPs.) In a seemingly unrelated guideline, all coordinate structures are annotated in the Treebank; such coordinate structures are represented by Chomsky-adjunction when the two conjoined constituents bear the same label. This means that if an NP contains coordinated adjective phrases, then an ADJP tag will be used to tag that coordination, even though simple ADJPs within NPs will not bear an APJP tag. Experience has shown that annotators can delete pairs of brackets extremely quickly using the mouse-based tools, whereas creating brackets is a much slower operation. Because the coordination of adjectives i quite common, it is more efficient o leave in ADJP labels, and delete them if they are not part of a coordinate structure, than to reintroduce them if necessary. Progress to Date 5.1 Composition and Size of Corpus Table 4 shows the output of the Penn Treebank project at the end of its first phase. All the materials listed in Table 4 are available on CD-ROM to members of the Linguistic Data Consortium. 12About 3 million words of POS-tagged material and a small sam- piing of skeletally parsed text are available as part of the first Association for Com- putational Linguistics/Data Collection Initiative CD-ROM, and a somewhat larger subset of materials is available on cartridge tape directly from the Penn Treebank Project. For information, contact he first author of this paper or send e-mail to tree- bank@unagi.cis.upenn.edu. 11 This use of pseudo-attachment is identical to its original use in Churchs parser (Church 1980). 12 Contact he Linguistic Data Consortium, 441 Williams Hall, University of Pennsylvania, Philadelphia PA 19104-6305, or send e-mail to ldc@unagi.cis.upenn.edu for more information. 326 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English Table 4 Penn Treebank (as of 11/92). Description Tagged for Skeletal Part-of-Speech Parsing (Tokens) (Tokens) Dept. of Energy abstracts Dow Jones Newswire stories Dept. of Agriculture bulletins Library of America texts MUC-3 messages IBM Manual sentences WBUR radio transcripts ATIS sentences Brown Corpus, retagged 231,404 231,404 3,065,776 1,061,166 78,555 78,555 105,652 105,652 111,828 111,828 89,121 89,121 11,589 11,589 19,832 19,832 1,172,041 1,172,041 Total: 4,885,798 2,881,188 Some comments on the materials included: ? Department of Energy abstracts are scientific abstracts from a variety of disciplines. All of the skeletally parsed Dow Jones Newswire materials are also available as digitally recorded read speech as part of the DARPA WSJ-CSR1 corpus, available through the Linguistic Data Consortium. The Department of Agriculture materials include short bulletins on such topics as when to plant various flowers and how to can various vegetables and fruits. The Library of America texts are 5,000-10,000 word passages, mainly book chapters, from a variety of American authors including Mark Twain, Henry Adams, Willa Cather, Herman Melville, W. E. B. Dubois, and Ralph Waldo Emerson. The MUC-3 texts are all news stories from the Federal News Service about terrorist activities in South America. Some of these texts are translations of Spanish news stories or transcripts of radio broadcasts. They are taken from training materials for the Third Message Understanding Conference. The Brown Corpus materials were completely retagged by the Penn Treebank project starting from the untagged version of the Brown Corpus (Francis 1964). The IBM sentences are taken from IBM computer manuals; they are chosen to contain a vocabulary of 3,000 words, and are limited in length. The ATIS sentences are transcribed versions of spontaneous sentences collected as training materials for the DARPA Air Travel Information System project. The entire corpus has been tagged for POS information, at an estimated error rate 327 Computational Linguistics Volume 19, Number 2 of approximately 3%. The POS-tagged version of the Library of America texts and the Department of Agriculture bulletins have been corrected twice (each by a different annotator), ""and the corrected files were then carefully adjudicated; we estimate the error rate of the adjudicated version at well under 1%. Using a version of PARTS retrained on the entire preliminary corpus and adjudicating between the output of the retrained version and the preliminary version of the corpus, we plan to reduce the error rate of the final version of the corpus to approximately 1%. All the skeletally parsed materials have been corrected once, except for the Brown materials, which have been quickly proofread an additional time for gross parsing errors. 5.2 Future Direct ions A large number of research efforts, both at the University of Pennsylvania nd else- where, have relied on the output of the Penn Treebank Project o date. A few examples already in print: a number of projects investigating stochastic parsing have used either the POS-tagged materials (Magerman and Marcus 1990; Brill et al. 1990; Brill 1991) or the skeletally parsed corpus (Weischedel et al. 1991; Pereira and Schabes 1992). The POS-tagged corpus has also been used to train a number of different POS taggers in- cluding Meteer, Schwartz, and Weischedel (1991), and the skeletally parsed corpus has been used in connection with the development of new methods to exploit intonational cues in disambiguating the parsing of spoken sentences (Veilleux and Ostendorf 1992). The Penn Treebank has been used to bootstrap the development of lexicons for particu- lar applications (Robert Ingria, personal communication) and is being used as a source of examples for linguistic theory and psychological modelling (e.g. To aid in the search for specific examples of grammatical phenomena using the Treebank, Richard Pito has developed tgrep, a tool for very fast context-free pattern matching against he skeletally parsed corpus, which is available through the Linguistic Data Consortium. While the Treebank is being widely used, the annotation scheme mployed has a variety of limitations. Many otherwise clear argument/adjunct relations in the corpus are not indicated because of the current Treebanks essentially context-free represen- tation. For example, there is at present no satisfactory representation for sentences in which complement oun phrases or clauses occur after a sentential level adverb. Either the adverb is trapped within the VP, so that the complement can occur within the VP where it belongs, or else the adverb is attached to the S, closing off the VP and forcing the complement to attach to the S. This ""trapping"" problem serves as a limitation for groups that currently use Treebank material semiautomatically to derive lexicons for particular applications. For most of these problems, however, solutions are possible on the basis of mechanisms already used by the Treebank Project. For example, the pseudo-attachment no ation can be extended to indicate a variety of crossing depen- dencies. We have recently begun to use this mechanism to represent various kinds of dislocations, and the Treebank annotators themselves have developed a detailed proposal to extend pseudo-attachment to a wide range of similar phenomena. A variety of inconsistencies in the annotation scheme used within the Treebank have also become apparent with time. The annotation schemes for some syntactic categories should be unified to allow a consistent approach to determining predicate- argument structure. To take a very simple example, sentential adverbs attach under VP when they occur between auxiliaries and predicative ADJPs, but attach under S when they occur between auxiliaries and VPs. These structures need to be regularized. As the current Treebank has been exploited by a variety of users, a significant number have expressed a need for forms of annotation richer than provided by the projects first phase. Some users would like a less skeletal form of annotation of surface 328 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English grammatical structure, expanding the essentially context-free analysis of the current Penn Treebank to indicate a wide variety of noncontiguous structures and dependen- cies. A wide range of Treebank users now strongly desire a level of annotation that makes explicit some form of predicate-argument structure. The desired level of rep- resentation would make explicit the logical subject and logical object of the verb, and would indicate, at least in clear cases, which subconstituents serve as arguments of the underlying predicates and which serve as modifiers. During the next phase of the Treebank project, we expect o provide both a richer analysis of the existing corpus and a parallel corpus of predicate-argument structures. This will be done by first enriching the annotation of the current corpus, and then automatically extracting predicate-argument structure, at the level of distinguishing logical subjects and objects, and distinguishing arguments from adjuncts for clear cases. Enrichment will be achieved by automatically transforming the current Penn Treebank into a level of structure close to the intended target, and then completing the conversion by hand. Acknowledgments The work reported here was partially supported by DARPA grant No. N0014-85-K0018, byDARPA and AFOSR jointly under grant No. AFOSR-90-0066 and by ARO grant No. DAAL 03-89-C0031 PRI. Seed money was provided by the General Electric Corporation under grant No. We gratefully acknowledge this support. We would also like to acknowledge the contribution of the annotators who have worked on the Penn Treebank Project: Florence Dong, Leslie Dossey, Mark Ferguson, Lisa Frank, Elizabeth Hamilton, Alissa Hinckley, Chris Hudson, Karen Katz, Grace Kim, Robert MacIntyre, Mark Parisi, Britta Schasberger, Victoria Tredinnick and Matt Waters; in addition, Rob Foye, David Magerman, Richard Pito and Steven Shapiro deserve our special thanks for their administrative and programming support. We are grateful to AT&T Bell Labs for permission to use Kenneth Churchs PARTS part-of-speech labeler and Donald Hindles Fidditch parser. Finally, we would like to thank Sue Marcus for sharing with us her statistical expertise and providing the analysis of the time data of the experiment reported in Section 3. The design of that experiment is due to the first two authors; they alone are responsible for its shortcomings. References Brill, Eric (1991). ""Discovering the lexical features of a language."" In Proceedings, 29th Annual Meeting of the Association for Computational Linguistics. Brill, Eric; Magerman, David; Marcus, Mitchell P.; and Santorini, Beatrice (1990). ""Deducing linguistic structure from the statistics of large corpora."" In Proceedings, DARPA Speech and Natural Language Workshop. June 1990, 275-282. Church, Kenneth W. (1980). Memory limitations in natural language processing. Masters dissertation, Massachusetts Institute of Technology, Cambridge MA. Church, Kenneth W. (1988). ""A stochastic parts program and noun phrase parser for unrestricted text."" In Proceedings, Second Conference on Applied Natural Language Processing. Francis, W. Nelson (1964). ""A standard sample of present-day English for use with digital computers."" Report o the U.S Office of Education on Cooperative Research Project No. Brown University, Providence RI. Francis, W. Nelson, and Ku~era, Henry (1982). Frequency Analysis of English Usage: Lexicon and Grammar. Houghton Mifflin. Garside, Roger; Leech, Geoffrey; and Sampson, Geoffrey (1987). The Computational Analysis of English: A Corpus-Based Approach. Hindle, Donald (1983). ""User manual for Fidditch."" Technical memorandum 7590-142, Naval Research Laboratory. Hindle, Donald (1989). ""Acquiring disambiguation rules from text."" In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics. Lewis, Bil; LaLiberte, Dan; and the GNU Manual Group (1990). The GNU Emacs Lisp reference manual. Free Software Foundation, Cambridge, MA. Magerman, David, and Marcus, Mitchell P. (1990). ""Parsing a natural language using 329 Computational Linguistics Volume 19, Number 2 mutual information statistics."" In Proceedings of AAAI-90. Meteer, Marie; Schwartz, Richard; and Weischedel, Ralph (1991). ""Studies in part of speech labelling."" In Proceedings, Fourth DARPA Speech and Natural Language Workshop. Niv, Michael (1991). ""Syntactic disambiguation."" In The Penn Review of Linguistics, 14, 120-126. Pereira, Fernando, and Schabes, Yves (1992). ""Inside-outside r estimation from partially bracketed corpora."" In Proceedings, 30th Annual Meeting of the Association for Computational Linguistics. Santorini, Beatrice (1990). ""Part-of-speech tagging guidelines for the Penn Treebank Project."" Technical report MS-CIS-90-47, Department of Computer and Information Science, University of Pennsylvania. Santorini, Beatrice, and Marcinkiewicz, Mary Ann (1991). ""Bracketing uidelines for the Penn Treebank Project."" Unpublished manuscript, Department of Computer and Information Science, University of Pennsylvania. Veilleux, N. M., and Ostendorf, Mari (1992). ""Probabilistic parse scoring based on prosodic features."" In Proceedings, Fifth DARPA Speech and Natural Language Workshop. Weischedel, Ralph; Ayuso, Damaris; Bobrow, R.; Boisen, Sean; Ingria, Robert; and Palmucci, Jeff (1991). ""Partial parsing: a report of work in progress."" In Proceedings, Fourth DARPA Speech and Natural Language Workshop.","Building A Large Annotated Corpus Of English: The Penn Treebank
","After presenting the set of syntactic tags that we use, we illustrate and discuss the bracketing process.The pioneering Brown Corpus distinguishes 87 simple tags (Francis 1964; Francis and Ku~era 1982) and allows the formation of compound tags; thus, the contraction I m is tagged as PPSS+BEM (PPSS for ""non-third person nominative per- pora.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1consisting of over 4.5 million words of American English.In addition, over half of it has been annotated for skeletal syntactic structure",0.7871315479278564,0.8704456686973572,0.8266948461532593
"The Automated Acquisit ion of Topic Signatures for Text Summarizat ion Chin -Yew L in  and  Eduard  Hovy In fo rmat ion  S(:i(umes I l l s t i tu te Un ivers i ty  of Southern  Ca l i fo rn ia Mar ina  del Rey, CA  90292, USA { cyl,hovy }C~isi.edu Abst rac t In order to produce, a good summary, one has to identify the most relevant portions of a given text. We describe in this t)at)er a method for au- tomatically training tel)it, signatures--sets of related words, with associated weights, organized around head topics and illustrate with signatmes we cre- ;tt.ed with 6,194 TREC collection texts over 4 se- lected tot)ics. We descril)e the l)ossible integration of tolli(: signatures with ontoh)gies and its evaluaton on an automate(l text summarization system. 1 I n t roduct ion This t)aper describes the automated (:reation of what we call topic signatures, constructs that can I)lay a central role. in automated text summarization and information retrieval. ToI)ic signatures can lie used to identify the t)resence of a (:omph~x conce.pt a concept hat consists of several related coinl)onents in fixed relationships. ]~.c.stauvant-uisit, for examph~, invoh,es at h,ast the concel)ts lltCgFIt, t.(tt, pay, and possibly waiter, all(l Dragon Boat PcstivaI (in Tat- wan) involves the Ct)llC(!l)t,S cal(tlztlt,s (a talisman to ward off evil), rnoza (something with the t)ower of preventing pestilen(:e and strengthening health), pic- tures of Ch, un9 Kuei (a nemesis of evil spirits), eggs standing on end, etc. Only when the concepts co- occur is one licensed to infer the comph:x concept; cat or moza alone, for example, are not sufficient. At this time, we do not c.onsider the imerrelationships among tile concepts. Since many texts may describe all the compo- nents of a comI)lex concept without ever exI)lic- itly mentioning the mlderlying complex concel/t--a tol)ic--itself, systems that have to identify topic(s), for summarization or information retrieval, require a method of infcuring comt)hx concelltS flom their component words in the text. 2 Re la ted  Work In late 1970s, ])e.long (DeJong, 1982) developed a system called I""tIUMP (Fast Reading Understand- ing and Memory Program) to skim newspaper sto- ries and extract the main details. FRUMP uses a data structure called sketchy script to organize its world knowhdge. Each sketchy script is what FRUMI ) knows al)out what can occur in l)articu- lar situations such as denmnstrations, earthquakes, labor strike.s, an(t so on. FRUMP selects a t)artic- ular sketchy script based on clues to styled events in news articles. In other words, FRUMP selects an eml)t3 ~ t(uni)late 1whose slots will be tilled on the fly as t""F[UMP reads a news artMe. A summary is gen- erated })ased on what has been (:al)tured or filled in the teml)Iate. The recent success of infornmtion extractk)n re- search has encoreaged the FI{UM1 ) api)roach. The SUMMONS (SUMMarizing Online News artMes) system (McKeown and Radev, 1999) takes tem- l)late outputs of information extra(:tion systems de- velofmd for MUC conference and generating smn- maries of multit)le news artMes. FRUMP and SUM- MONS both rely on t/rior knowledge of their do- mains, th)wever, to acquire such t)rior knowledge is lal)or-intensive and time-consuming. I~)r exam-- l)le, the Unive.rsity of Massa(:husetts CIRCUS sys- l.enl use(l ill the MUC-3 (SAIC, 1998) terrorism do- main required about 1500 i)erson-llours to define ex- traction lmtterns 2 (Rilotf, 1996). In order to make them practical, we need to reduce the knowhxlge n- gineering bottleneck and iml)rove the portability of FI{UMI ) or SUMMONS-like systems. Since the worhi contains thousands, or perhal)s millions, of COml)lex (:on(:et)ts , it is important; to be able to learn sketchy scripts or extraction patterns automatically from corpora -no existing knowledge base contains nearly enough information. (Rilotf aim Lorenzen, 1999) 1)resent a system AutoSlog-TS that generates extraction i)atterns and learns lexical con- straints automatically flom t)rec]assified text to al- leviate the knowledge ngineering I)ottleneck men- tioned above. Although Riloff al)plied AutoSlog-TS lVe viewed sketchy s(:lil)tS and teml)lates as equivalent (ollstrllctS ill the sense that they sl)ecil ~, high level entities and relationships for specific tot)its. 2Aii extra(:l;iOll pattt!rlk is essentially ;t case fraine contains its trigger word, enabling conditions, variable slots, and slot constraints. C IRCUS uses a database of extraction patterns to t~alSe texts (l{ilolI, 1996). 495 to text categorization and information extraction, the concept of relevancy signatures introduced by her is very similar to the topic si.qnatures we pro- posed in this paper. Relevancy signatures and topic signatures arc both trained on preclassitied ocu- ments of specific topics and used to identify the presence of the learned topics in previously unseen documents. The main differences to our approach are: relevancy signatures require a parser. They are sentence-based and applied to text categorization. On the contrary, topic signatures only rely on cor- pus statistics, arc docmnent-based a and used in text smnmarization. In the next section, we describe the automated text smmnarization system SUMMARIST that we used in the experiments to provide the context of discussion. We then define topic signatures and de- tail the procedures for automatically constructing topic signatures. In Section 5, we give an overview of the corpus used in the evaluation. In Section 6 we present he experimental results and the possibility of enriching topic signatures using an existing ontol- ogy. Finally, we end this paper with a conclusion. 3 SUMMARIST SUMMARIST (How and Lin, 1999) is a system designed to generate summaries of multilingual in- put texts. At this time, SUMMARIST can process English, Arabic, Bahasa Indonesia, Japanese, Ko- rean, and Spanish texts. It combines robust natural language processing methods (morl)hologieal trans- formation and part-of-speech tagging), symbolic world knowledge, and information retrieval tech- niques (term distribution and frequency) to achieve high robustness and better concept-level generaliza-- tion. The core of SUMMARIST is based on the follow- ing equation! : summarization = topic identification + topic interpretation + generation. These three stages are: Topic Ident i f ieat lon:  Identify the most imtmrtant (central) topics of the texts. SUMMARIST uses positional importance, topic signature, and term frequency. Importance based on discourse structure will be added later. This is tile most developed stage in SUMMARIST. Topic I n te rpretat ion :  ~i~-) fllse concepts such as waiter, menu, and food into one generalized concept restaurant, we need more than the sin> pie word aggregation used in traditional infor- mation retrieval. We have investigated concept aWe would like to use only the relevant parts of documents to generate topic signatures in the future, qkext segmentation algorithms uch as TextTiling (Ilearst, 1997) can be used to find subtopic segments in text. ABCNEWS.cona  : De lay  in  Hand ing  F l ight  990   [  robe  to  FB I NI SI3 C l la i tn lan  Jarl leS t la l l  says  Egypt ian  clff icials Iv811l to  I,~view res t l l t s of  t i le  invest igat ion  intcl lhe  cras l l  o f  l lggyptA i r  F l ight  990 before  t i le  case i~ lu r l led  over  Ic, t i le Fi31, Nt lv. IG - U S. i lxvestigl~lo[~ lLppear to  be leat l i l lg i i Iore thg l l  eveF low~trd t i le poss ib i l i ty  that  one  o f  the  cc~-pilot~ o f  EgyptA i r  F l ight  990 may have de  [ ihera le ly  c rashed t i le p lane  las t  I lafl l lth, k i l l i ng  all 217 peop le  on  board . f la i l  ever . o f f i c ia ls  say  t i le  Nat iona l  T ran~por  ta t ion  Sa fety  Board  wi l l de lay  t rans fer r ing  tile invegt iga l ion  o f  the  Oct  31 c rash  to  tilt: FI31 - the agency  that  wot l id  lead i~ c r imina l  p robe  - for at  least  tt few days . to  M Iow Egypt ian  exper ts  to rev iew ev idence  ill t i le case. gtts l ) ic iot l~ of  fou l  p lay  were  ra i sed  a f te r  invest igators  l i s ten ing  to  rt tape ftol l l  l i l t  cockp i t  vo ice recorder  i so la ted  a re l ig ious  prayer  or s ta te l l l e l l t made by t i le co -p i lo t  jus t  be fore  t i le  p lane s  autop i lo t  was turned  o f f s l id  the  p lane  began i ts  in i t ia l  p lunge  in to  t i le A t lant i c  Ocean of f  Mas - s r tcht tset t$   Na l l tucket   [s ia l ld . Over  tile pas t  week . a f te r  muc i l  e f fo r t ,  t i le  NTSJB and  t i le  Navy  succeeded ill I ocat i l lg  the  p lane s  two  ""b lack  boxes , ""  th~ cockp i t  vo ice recorder  and lhe  f l ight  data  recorder . The  tape  ind icates  t l l a t  shor t ly  a f te r  the  p lane  leve led ~ff  a t  i ts c ru i s ing a l t i tude  o f  as ,000  feet ,  t i le  cl~ief p i lo t  o f  t i le a i rc ra f t  left  the  p lane s cockp i t ,  l eav ing  one  o f  t i le  twc~ co-p i lo ts  nIol le t i lere as the  a i rc ra f t  began its descent . Figure 1: A Nov. 16 1999 ABC News page sumnmry generated by SUMMARIST. counting and topic signatures to tackle tile fll- sion problem. Summary Generat ion :  SUMMARIST can pro- duce keyword and extract type summaries. Figure 1 shows an ABC News page summary about EgyptAir Flight 990 by SUMMARIST. SUM- MARIST employs several different heuristics in tile topic identification stage to score terms and sen- tences. The score of a sentence is simply the sum of all the scores of content-bearing terms in the sen- tence. These heuristics arc implemented in separate modules using inputs from preprocessing modules such as tokenizer, part-of-speech tagger, morpholog- ical analyzer, term frequency and tfidf weights cal- culator, sentence length calculator, and sentence lo- cation identifier. Ve only activate the position mod- ule, tile tfidfmodule, and the. topic signature module for comparison. We discuss the effectiveness of these modules in Section 6. 4 Top ic  S ignatures Before addressing the problem of world knowledge acquisition head-on, we decided to investigate what type of knowledge would be useflfl for summariza- tion. After all, one can spend a lifetime acquir- ing knowledge in just a small domain. But what is tile minimum amount of knowledge we need to enable effective topic identification ms illustrated by the restaurant-visit example? Our idea is simple. We would collect a set of terms 4 that were typi- cally highly correlated with a target concept from a preclassified corpus such as TREC collections, and then, during smnmarization, group the occurrence of the related terms by the target concept. For exam- pie, we would replace joint instances of table, inertu, waiter, order, eat, pay, tip, and so on, by the single phrase restaurant-visit, in producing an indicative 4Terms can be stemmed words, bigrams, or trigrams. 496 sulnlllary. Ve thus defined a tot)it signat.ure as a family of related terms, as follows: ~IS = { topic, sifl~zutu.rc. } = {topic,< ( t , ,w l ) , . , ( t , , ,w , , )  >} (1) where topic is the target concet)t and .,d.q)zat~Lrc is a vector of related ternls. Each t, is an term ldghly correlated to topic with association weight w/. The number of related terms 7z can tie set empirically according to a cutot[ associated weight. describe how to acquire related terms and their associated weights in the next section. 4.1  S ignature  Term Ext rac t ion  and  Weight Es t imat ion ()n the assumption that semantically related terms tend to co-occur, on( can construct topic signa- tures flom preclassified text using the X 2 test, mu-. tual information, or other standard statistic tests and infornlation-theoreti(: measures. Instead of X 2, we use likclih.ood ratio (Dunniug, 1993) A, sin(:e A i,; more apI)rot)riate for si/arse data than X 2 test and the quantity -21o9A is asymi)t(/tically X~ dis- tril)ute(15. Therefore, we Call (leterndnc the (:onti- ( lence level for a specific -21o9A value l/y looking ut) X :~ (tistril)ution table and use tlm value to sel(,,ct an at)i)rot)riate cutoff associated weight. We have documents l)[e.classitied into a :;(~t, ""R. of relevant exts and a set ~. of nonrelewmt exl;s for a given topic. Assuming the following two hyl)othe,~es: t typothes is  1 ( I f l ) :  t(~Pvlti) = P = P(PvltT/), i.e. the r(.,lewmcy of a d()(:|lment is in(teI)en(hmt, of t i . I  ]  [ypothes is  2 ( t t2) :  I(Pv[ti) == lh ~ 1)2 - t)(Pvlt, i), i.e. :;(;n(:(~ of t i indi(:~Lt(.~. ; strong r(~levan(:y ~ssunling ]h >> 1)2 ? and the following 2-10=2 contingency tabl(;: where Ol~ is the fiequency of term ti occurring in the. l e lev;tnt  set ,  012 is the  [ r ( !qu(nlcy of Lerm t i t)c- curring in the  ] lol lreleval lt ,  set ,  O21 is the  f le(l l lel l( :y of tt;rnl  [ i? ti occurring in the rtdevant set, O._,~ is the flequ(mcy of term l.i ? ti o(:curring in the non- l  e leva i i t  seL. -kssmning a l)inomial distril)ution: C;) b(~; ,,., :/.) = :,:~(1 - .~:)("" "") (2 ) 5This assumes |ha l  the ratio is between the inaximuni like> [ihood est, im&t.(! over a .qll})part of l;}l(! i)alatlllCt(~r sl)a(:(~ ;tll(] l.h(! lllaxillUllll likelihood (}sI.i|II}tlA~ ov(!r the (Hltill! i)alaillt~tt!r si);t(:e. Set! (Manning ;tnd Sch/itze, I999) t)ag, es 172 l.o 175 for d(!t.ails. then the likelihood for HI is: L(H~) = b(Ot~; 0~ + Ou,,p)b(O:,~; 0:,, + Om,,p) and for //2 is: L(H2)  = D(OI 1; O11 Jr"" ()12, Pl )b(O21; ()21 Jr- (,)22,1)2) The -2log, value is then computed ms follows: 1. (f/1 ) m --21o 9 - - L( i t  2 ) b(O 11 ; O I  1 + O12,  P) I J (021 : O21 + 022 , P) - -21o 9 1((-)1 l ; ( )11  + O1-),  P I )h (O21 ; O21 q- ( )22  , P2 ) : - -2 ( (O l l  +021) lo r_ Jp+( ( )12+022) lo9(1 - - l~) - -  (,~1) (? ) l l l o  JP l+Ol21og( l  "" t 1 )+0211ogp2-~0221o0(1- f~2) ) ) -- .2.,~ x (~  i (7~) -  ;~(~19- ) )  (4 ) = 2,v x Z(P~;  T )  (5 ) whel e  N = O l t  -F O12 -1- O21 -I- 022 is the  to ta l  l lum-. her of t, ernl occurrence, in the corpus, 7/(/~) is the entropy of terms over relevant and nonrelevant sets of documents, 7/ (  fe l t  ) is the entropy of a given term OVel"" relev;inL ~/nd nonl  ( .qeval l t  sets  of doel l inel lLS, ~tll(1 Z(R.; T) i:; the inutual information between docu- ment relevancy and a given t(.rm. Equation 5 indi- cates that mutual inforntation 6 is an e(tuiwdent mea- sur(. t() lik(.qiho(id ratio when we assume a binomial distribution and a 2-by-2 (ontingency table. To crest(; topic .~dgnature for a given tot)ic , we: 1. (:lassify doctunents as relevant or nonrclcwmt according t() tile given topic 2. comt)ut.e the -21oflA wdue using Equation 3 for each Lcrm in the document colle(:Lion ""{. rank t, erms according 1o their -2lo9~ value 4. select a c(mfid(mce l , vel fiom the A;: (listril)utiotl table; (letermin(~ the cutotf associated weight, mid the numl)(n"" of t(nms to he included iIl the signatures 5 The Corpus The training data derives Kern the Question and Answering summary evahmtion data provided l)y T IPSTEI / . -SUMMAC (Mani et al., 1998) that is a sttbset of the TREC collectioliS. The TREC data is a collection of texts, classified into various topics, used for formal ewduaLions of information retrieval sys- tems in a seri(~s of annual (:omparisons. This data set: contains essential text fragnients (phrases, (:Iausos, iuld sentences) which must 1)e included in SUllltIlarios to ~tnswer some TI{EC tel)its. These flagments are each judged 1)y a hmnan judge. As described in Se(:- tion 3, SUMMAI~IST employs several independent nlo(hlles to assign a score to each SelltA:llCe~ and Chell COlll})illeS the st.or(..% L() decide which sentences to ex- tract from the input text;. can gauge the efticacy (>lhe lllll[lla} inrormalion is defined according to chapter 2 of ((;over and Thomas, i991) and is not tile i)airwis(~ mutual inforlnalion us(!d in ((;hur(:h and llanks, 1990). 497 TREC Top ic  Da~cr lp t ion (nunQ Number :  151 ( t i t le}  Top ic :  Co, p ing  w i th  overc rowded pr i sons (dese} Deser i l l t io l l : The  doeu l laent  will p rov ide  in f ,~rn lat ion ol~ jai l  and  pr ison overc rowd iuK and  how i r lmates  are forced to cope  wi th  th,~se cond i t ions ;  or it wil l revea l  p lan~ to  re l ieve  ti le overc rowded ?o l ld i t lon . (nar t )  Nar ra t ive : A re levant  docunaent  will descr ibe  scene~ of overcro~vdi l lg that  have beco lne  all too  crlllllllOll ill ja i l s  and  pr i sons  a ro t tnd  the  count ry ,  T i le document  will i dent i fy  how inmates  are forced to  cope w i th  those  over - crowded cond i t ion~,  and/or  what  ti le Cc l r reet iona l  Syste l l l  is do ing ,  or ph lnn ing  to do,  to a l lev ia te  ti le c rowded col ld i t io l l . (/top) Test  Quest ions QI  Mehat are  name and/or  locat ion  of ti le cor rec t ion  fae i l i l ies where  the  repor ted  ~vercrowd ing  ex is ts? Q2 x~Vhat negat ive  exper iences  have  there  been  at t i le overc rowded fac i l i t ies  (whether  or not tile)"" are thought  to have  been  caused by  the  overc rowd lng)? Q3 What  measures  have  been  taken/p la ia i led / recommended (e tc . ) to aecon l lnod~te  more  i l l l l la Ies zlt pena l  fac i l i t ies ,  e .g . ,  doub l i l l g tip, Ile~y COllStructlon? Q,I ~,Vhat measures  have  been  taken/planned/rec~mnlel,ded (etc .} to reduce  ti le l lt l l l lber of Dew il l l i ]ate$, e .g . ,  morator iums on admisMon,  a l te rnat ive  pena l t ies ,  p rograme to reduce c r ime/ rec ld iv i sm? Q5 What  measures  have  been  taken/p lanned/ recommended (e tc . ) to reduce  ti le number  of ex i s t ing  inmates  at an overcrc~wded fac i l i ty ,  e .g . g rant ing  ear ly  re lease ,  t rnns fer ing  to  uncrowded fac i l i t ies? Sample  Answer  Keys (DOCNO)  AP891027-0063 ( /DOCNO) (F ILE ID)  AP -NR-  10-27-89 0615EDT( /F ILE ID) ( IST_L INE) r  a PM-Cha ined Inmates  10-27 0335( / IST .L INE) (2ND-L INE)PM-Cha ined  lnmates ,0344 ( /2ND_L INE) ( I IEAD)  lnmates  Cha ined  to 1.Vails in 13Mtimore Po l i ce S ta t ions ( / l lEAD) (DATEL INE)BALT IMOIT IE  (AP)  ( /DATEL INE} (tEXT) (Q ,q )pr i soner~ are  kept  cha ined  to the wall~ of local po l ice  lcJekup~ for as long as th ree  days  at a t ln~e I)ecattse of overc rowd ing  ill regu la r  je l l cel ls ,  pol ice sa id . ( /Q3} Overcrowd ing  at  the  (Q1) lqMt l rnore  County  Detent ion  Center ( /Q1) h~ forced pn l lee  tn  . (/TEXT) Table 1: TREC topic description for topic 151, test questions expected to be answered by relewmt doc- uments, and a smnple document with answer key, s. of each module by comparing, for ditferent amounts of extraction, how many :good sentences the module selects by itself. We rate a sentence as good simply if it also occurs in the ideal human-made xtract, and measure it using combined recall and precision (F-score). We used four topics r of total 6,194 doc- uments from the TREC collection. 138 of them are relevant documents with T IPSTER-SUMMAC pro- vided answer keys for the question and answering evaluation. Model extracts are created automati- cally from sentences contailfing answer keys. Table 1 shows TREC topic description for topic 151, test questions expected to be answered by relevant doc- uments , and a sample relevant document with an- swer keys markup. 7These four topics are: topic 151: Overcrowded Prisons, 1211 texts, 85 relevant; topic 257: Cigarette Consumption, 1727 texts, 126 relevant; topic 258: Computer Security, 1701 texts, 49 relevant; topic 271: Solar Power, 1555 texts, 59 relevant. SA relevant: document only needs to answer at least one of the five questions. 6 Experimental Results In order to assess the utility of topic signatures in text sununarization, we follow the procedure de- scribed at the end of Section 4.1 to create topic signature for each selected TREC topic. Docu- ments are separated into relevant and nomelevant sets according to their TREC relevancy judgments for each topic. We then run each document hrough a part-of-speech tagger and convert each word into its root form based on the \h)rdNct lexical database. We also collect individual root word (unigram) fie- quency, two consecutive non-stopword 9 (bigram) fie- quency, and three consecutive non-stopwords (tri- gram) fiequeney to facilitate the computation of the -21ogA value for each term. We expect high rank- ing bigram and trigram signature terms to be very informative. We set the cutoff associated weight at 10.83 with confidence level ~t = 0.001 by looking up a X 2 statistical table. Table 2 shows the top 10 unigrmn, bigram, and tri- gram topic signature terms for each topic m. Several conclusions can be drawn directly. Terms with high -21ogA are indeed good indicators for their corre- sponding topics. The -2logA values decrease as the number of words in a term increases. This is rea- sonable, since longer terms usually occur less often than their constituents. However, bigram terms are more informative than nnigrant erms as we can ob- serve: jail//prison overervwding of topic 151, tobacco industry of topic 257, computer security of topic 258, and solar en, ergy/imwer of topic 271. These mLto- matically generated signature terms closely resemble or equal the given short TREC topic descriptions. Although trigram terms shown in the table, such as federal court order, philip morris 7~r, jet propul.. sion laboratory, and mobile telephone s:qstem are also meaningflfl, they do not demonstrate he closer term relationship among other terms in their respective topics that is seen in tlm bigram cases. We expect that more training data can improve tile situation. We notice that the -2logA values for topic 258 are higher than those of the other three topics. As indicated by (Mani et al., 1998) the majority of rel- evant documents for topic 258 have the query topic as their main theme; while the others mostly have the query topics as their subsidiary themes. This implies that it is too liberal to assume all the terms in relevant documents of the other three topics are relevant. We plan to apply text segmentation algo- rithms such as TextTiling (Hearst, t997) to segment documents into subtopic units. We will then per- form the topic signature creation procedure only on tile relevant units to prevent inchlsion of noise terms. 9,Ve use the stopword list supplied with the SMAIIT re- trieval system. l?qhe -2logA values are not comparable across ngram cat- egories, since each ngraln category has its own sample space. 498 Top ic I :ll h~l al l l  -21,~gX  ] l i~ la l l I  -21,,9X j a i l  t)3L I)1,1 e()tH~t 2, ja i l  Dit) 27:1 c+,l l l l l} .IIJN ~21 eae ly  le+]+.~lSt ? N,~ :{t;] , )v , . ) , ~ , ,wd ln~;  :?12:1. , ,n  7.1 R72 i l l ln?lt ,"" 2 : t l  7d5  s ta l , ""  1,) i~, ,n, . i  67  ,3(~t~ ~h+. l i f  [  IF, I . i l o  ,1:~ 3 fill,"" l ; l  t(;2"") s ta le  151 9t t~ ia i l  r l%l  lctr~%vI] l r ld I;1 ~[ i I}l l~t l l i l  l  I I  ~"" I "";~ C(,tlt I + , l , i "" t  t{ll.O! )l} i+tl-s,,rl 1,17, 3t),i h . .a l  j a i l  56  t i t+ C l )y  133177 p l l . , )D  ( )vcy lc l , , i v th l l~  55 37:  +, , , v , . r , ,wd ,+d 12N I)t)S i-(*lllt :l[ fac i l i t  3 52 9o9 10 S ignat t t ro  Torms o f  Tup ic  151  Ovorcrowdod Pr i sons ""II I~I al I l  -21,,~11 f - , I t . l~t l  c , ,u l~ <, l t t , . l  -I., :),;11 C, , l l l p  ]y  c+,ll~(lll ,]+c[+++"" 3,5 12L +l,.kali+ ii)iI i,[~ +h,  l l  [  [  [15 121 ~,11  i,) t l ; ,nk  :;.5 L21 j , )o t l ; l l l k  IH ) l i5  :~.,.121 pl l~C, l l ,  r  c+)l l : l l~ la i l  :~5 121 91: i f , .   ] ) l l~t ) l l  i21) l l l t l~ ~N t).l[] t put  pl is+, l l  .2t~ :t-II c+~uuly  jaiL ~l ; l l , . 2 t~31 l h,,hl l,~e~l ja i l  2d  :l I I Top ic  10  S ig l ln t t l re  Tern ls  o f  Top ic  257  - ( l igar~t t~ Co| l s l l l ) l | ) t lo | l l :n i<r tun  21ogX I+i ~.rarll -- 2 / , , f / . i r i4~am - 21, ,~A c lgrt l , . I t?+ .171; [}:iN ~tlb:xtc+) LIt(  ] l l~l l~ ~il 7)iN I ,h i l ip  I I l , ) l l i+ t j l  2.~ ~DSI l ( )ht lcc~) : l l ; l  017  hn  t - lg / l l , - l l t -  t ;7 t2}I I r ) l  ] i l I l a l l s  beDs~, l l  h<.([~f. 211 ~)t~[l s I I IOk i l l~  28.t  19~ ph i l ip  t l l ( , l  [ i~  5t  ()7;~ [1111~ Likll(e[ d + [ l l l l  22 21. t ~n l~,ke  15913.1  clarxl<t1, :  %, at  t80 . t5  q t t  iri l ln cl l~ .21 I IS I ,~ lh l l l a l l? )375 to lh l l l l l l l~  i t l Y ,  l I l a t l t ) l lgk l  -t.t .13.1 qt l  q t t  f i~ ln  21 - I lS , ,~ha  I . , )  elll()k.~ 112){}I  bll  b[i bl l  20  22t i s ,~i la 12)i .121 ~il pat r i ck  t0 . t55  c+)l lst l l l l l} l lo l l  bn  c lgar , . l te  2022d Illtll 113 ~+1~) c l~at+ l l~""  c~l l lpa l lV  :ID [$1)D ~[t+gtt a l l l . r  [ iCtt l l  ~llI,lk? (}llt 20226 al l l , )k(  l  10.1 I i0  (el l l  l l l a lk+ l  36223 [ l l l l~ Ca l l t e  [  ht:gl[ I  2(~ 22{i b[~t 79 .90:1  ?~IN illt+ll+il++t :1t;.22:1 i l l a  [ay~ia l l  +illk~[tl>,ll+e t4)l l lpi l l IV 2( I .22t  ] Top ic  I0 S ignatur . ""I~r)ns of Top ic  258 -- Co)nputor  Secur i ty I ~llial /lilt ""2Ionia I t  i+/,r al l l  21,QIX ""I1 i~ratn  --21o9, X (:+lll l l)l ltOr 115!~ :151 C4, l l lp I l l l  t  ae l  t l l l l y  213331 )e l  I l t t /p l l  [~i() l l   ] l th t )h l t ( l l y   [~  ~5.t v i rus  927.G7-1 ~[ ; idt lgt l , ""  s l t l  [ t l  l l l  17~ 5NN I l lh . l l  I lilt) 9R 85, t hacker  867 .377 FOl l lp t l t ,  t  +yS le l l l  1 -16.32~ C,+ltl++]l I l l l iVet~il~,  ~ lad l l  [ t le  7}) IJNI in,) l  rl+ +i+;+~ 2i13.! ) l , .~+-arch c,+ulte[  l ; i2  .l I :i l awte l l c l ""  b ,  rk ,   *j~ lal l , ) l  al+,l  ,,. 79.0N [ c , , rn ,  l l  3P+5 6+4 c , : ,ml ,u t ,  r  wrus  12~k033 I~+,++, je t  p tO l , t l L+ io t+ 79 .0~1 un lv+ l? i ty  31)5 .95~ corne l i  U lX iVe le i ty  1(1~4 7-t l  U l ; iV ,q+i )y  ~; radu lx t , . lll 79U~1 +ysl+ l l l  290 .3""17  Iltl(:l,P;ll %t++npl)ll 107 .283 lawt l l l e ,+ l i v+: r tn ( . te  I igt l i () l lal  i][) l[I;~ I / tb , . ra lL : ) ly  2N7 521 in i l i ta ry  t  ( , l l lp , l l . : r  106 .522 l iv~qll l ,) l?"" i lu) i ,maL  lubora lo ry  {59195 [ab  225 .51) ;  v i tu~ plo~t< l l l l  1U6 522 c,) l l lp l l I (~r S ,~CUl i ly  eXpet l  66 .19G mecLa ly  128 .515 %vesl ~et l l l a l l  82  2  [0  ~ecu [ i t? ,   cenl{~[  13ethesda  -19423 Top ic  10  S ignature  Ter lns  o f  Top ic  271  So lar  Power I  l l i g ta ln  - -21oqX t i ig t  ~ltn --2logX ""Ir i ~;r hi l l  - -21o!~A so la r  -1S- l .315 e,~la~ e l te t l4y  2{Di 521 d iv i~ i ,m Inu l l ip l ,~  acress  31 3-17 ) t lazd i t  :10Pt 0IY) s<,lal l , t lw ,  t  9,1 210  n l , )b i l , :  l , , l , -ph , ,n , . #c iv ic , ,  313 .17 le,) 271; .932  ( h r i~t ia l l  a id  8 f i .211  b l i l l sh  It .cl l l l i l l l )R} g  [ , , l l p  23510 it JtLi It l l l  2.5N.71):""+ l++,a S3Sl,*III 711 5:{5 el l l I} l  he iNht  llXile 23+5111 pax+lh , ,n  2133 81 I ill++tlllt. Ie i t  j ) l l l ) l le  (115;l+i IillllllCilll I lack i l l+;  I l Jd l l l l l l  22i+51(1 i)(~tltld 12 / ,121  i t i , l i un l  p l , , j , . c l  112.697 ~l,~l lal  In r )h i l ,  + sa l ,  l l i te  23  511J t , lw~r  12G.35:1 lei l i  <+, , ,d -  61.~111 ha l ld l le ld  IIled~il,"" t ,  l eph , , l l , :  23510 [ , , , , k , ,u t  125 .ll3t; scie. l lc, ,  pa lk  ~>.1 NS{) i l l ( ,h i le  ~ate l l l l . v>tetn  23  510 i i l  [ l l l i lSRl  1O9728 ~()llkl t  i l l l t  l  l l t l i l I l , l  51t ~5{} I l l l l t l l lvl i l l  i g id i l ln l  I> l , l j ec t  23 ,510 hc ,ydsh , t l  7N :173 l)p s l l la l  ?+1 ; /17  act iv t -  s+,la[ *ys tern  15673 Tattle 2: Top 10 signat.me t.erm.~; of mfigram, bigram, and trigram for fore"" TREe  t.opics. 6.1 Comparing Summary Extraction Effectiveness Using Topic Signatures+ TFII)t"",  and Bas(,line Algor i thms In orde)"" I() (~vahla(. (~ the (d[+:ct.iv(,im.~s nf l(>l)i(: .~dgna- l;lll(~S llS(~(] ill SlllllIIN/ly (~Xtlit(:t;iOll, W{,  ~ CtIllll)~ll(~ +flit! Sllltllll~tly StHII~011(CS ex(~ract,(~d 1)y the tol) ic si~Ilil[lll0, module+, basulin(. module, and tfidf lnothll(~s with lm- ntan annot,  at(~(l lllo(lo,] Sllllllll}llios. VC III(+~}/SIlI(+ + l;h(; l)crfl)rmanc(~ using a c()ml)ined umasure of lncall (I~) and pr(~cisi(m (P), F. F-score is defined by: I "" - -  (1 +H2)Il? where /3-P + I~ t ) 2 7 . ) f~rln fVln ~,, # of  .sc,tcncc.~ c:rtratcd th,t  olso atqwar in. tim model ,s.mn)?lr!l # of  sc+lt(!ncc,s i11 tim nlo,h:l .~um.tav!l # of  ,s(./Itclwcs c:rlv?lclcd t)1,t ll*c .Sll.Slcln rclaticc iml,ortancc of  l~ aml 1: (6) (7) Ve as.~um(~ (,(lual importance of re(:all iIIld preci- sion aim set H to 1 in our (+,Xl)(+rimtml;s. The Imselitm (I)ositi(m) module scores (at:h S(!llt(:llC{} hy its I)osi- ti(>n in the text. The first sent(race gets the high- esc s(:ortL the last S(HIt(H1Co the lowest. The l)as(~liIl(~ method is eXlmCted to lm (.f[ectiv(~ for news geme. The tfidf module assigns a score t.o a tt++rllI ti at:cord- ing to the product; of its flequc, ncy within a dot:- lllll(Hlt .j ( t f i j )  and its illV(~IS(} doctmmnt  t?equoncy (idfi lo.q ,~). N is the total mmfl)or of document.s in the (:()rlms and dfj is the, numl)er of (Io(:HnloAll;.q (:OlH:nining te rm ti. The topic sigjlla(.lll(++ module sciliis each ,q(~llt;(H1C(~: assigning to (ach word that occurs in a topic signa- (ure thu weigh(, of that, keyword in t.hc tol)ic signa- tltltL Eit{h s(++llt(,+ItC(~ Ill(ill l(:c(:ive.q a top ic  s ignature score equal to tlm total of all signature word scores it (:Olllailis, normalizcd 1) 3 the. highest sentence score. This s(:ol( 3 indical.es l;h(~ l(!l(wall(:(~ of l.h(; S(!llt.t~n(:(! to t, lw sigmmlre topic. SU.~[.MAt/IST Inoduced (!xttat:ts of tlm samu l(~xI.q sui)aralely for each ,,lodul0, for a s(~li(,s of ex- tracts ranging from ()cX; to 100% of the. original l;(}xI. Althottgh many rel<want docttments are avaita})l+, for each t01>ic, Ollly SOlll0 o[ [h0111 htlv(~ allSWOl kc!y 499 markut)s. The mnnber of documents with answer keys are listed in the row labeled: ""# of Relevant Does Used in Training"". To ensure we utilize all the available data and conduct a sound evaluation, we perform a three-fold (:ross validation. We re- serve one-third of documents as test set, use the rest as training set, and ret)eat three times with non- overlapl)ing test set. Furthernmre, we use only uni- gram topic signatures fin"" evaluation. The result is shown in Figure 2 and TaMe 3. We find that the topic signature method outperforms the other two methods and the tfidfmethod performs poorly. Among 40 possibh,, test points fl)r four topics with 10% SUmlnary length increment (0% means se- lect at least one sentence) as shown in Table 3, the topic signature method beats the baseline method 34 times. This result is really encouraging and in- dicates that the topic signature method is a worthy addition to a variety of text summarization methods. 6.2 Enriching Topic Signatures Using Existing Ontologies We have shown in the previous sections that topic signatures can be used to al)I)roximate topic iden- tification at the lexieal level. Although the au- tomatically acquired signature terms for a specific topic seem to 1)e bound by unknown relationships as shown in Table 2, it is hard to image how we can enrich the inherent fiat structure of tol)ie signatures as defined in Equation 1 to a construct as complex as a MUC template or script. As discussed in (Agirre et al., 2000), we propose using an existing ontology such as SENSUS (Knight and Luk, 1994) to identify signature term relations. The external hierarchical framework can be used to generalize topic signatures and suggest richer rep- resentations for topic signatures. Automated entity recognizers can be used to (:lassify unknown enti- ties into their appropriate SENSUS concept nodes. We are also investigating other approaches to attto- matieally learn signature term relations. The idea mentioned in this paper is just a starting point. 7 Conc lus ion In this paI)er we l)resented a t)rocedure to automati- (:ally acquire topic signatures and valuated the eflk~c- tiveness of applying tol)i(: signatures to extract ot)i(: relevant senten(:es against two other methods. The tot)ie signature method outt)erforms the baseline and the tfidfmethods for all test topics. Topic signatures can not only recognize related terms (topic identifi- (:ation), but grout) related terms togetlmr under one target concept (topic interpretation). IbI)i(: identi- fication and interpretation are two essential steps in a typical automated text summarization system as we l)resent in Section 3. ]))pic: signatures (:an also been vie.wed as an in- verse process of query expansion. Query expansion intends to alleviate the word mismatch lnoblenl in infornmtion retrieval, since documents are normally written in different vocabulary, ttow to atttomati- (ally identify highly e(nrelated terms and use them to improve information retrieval performance has been a main research issue since late 19611s. Re- cent advances in the query expansion (Xu and Croft, 1996) can also shed some light on the creation of topic signatures. Although we focus the ltse of topic signatures to aid text summarization i this paper, we plan to explore the possibility of applying topic signatures to perform query expansion in the future. The results reported are encouraging enough to allow us to contimm with topic signatures as the ve- hMe for a first approximation to worht knowledge. We are now busy creating a large nmnber of signa- ture.s to overcome the world knowledge acquisition problem and use them in topic interpretation. 8 Acknowledgements YVe thank the anonymous reviewers for very use- tiff suggestions. This work is supported in part by DARPA contract N66001-97-9538. References Eneko .~girre, Olatz Ansa, Edumd Hovy, and David Martinez. Enriching very large ontologies using the www. In Proceedings of the Work,,;hop on Ontology Construction of the European Con- fl:rencc of AI (ECAI). Kenneth Church and Patrick Hanks. Word as- sociation IIOrlllS, mutual information and lexicog- raphy. In Proceedings of the 28th Annual Meeting of the Association for Computational Lingui.vtic.~"" (,4CL-90), pages 76~-83. Thomas Cover and Joy A. Thomas. Elcment.~ of Information Theory..John Wiley & Sons. An overview of the FRUMP system. In ~2mdy G. Lehnert and Martin H. Ringle, editors, Strategies for natural language processing, pages 149-76. Lawrence Erlbaum A.s- so(lares. A~i:eurate methods for the statistics of surprise and coincidence. Computa- tional Linguistics, 19:61--74. TextTiling: Segmenting text into nmlti-l)aragraph subtopic passages. Compu- tational Linguistics, 23:33-64. Eduard Hovy and Chin-Yew Lin. Automated text summarization i SUMMAIRIST. In Inder- jeer Mani and Mark T. Maybury, editors, Ad- vances in Automatic 71xxt Summarization, chap- ter 8, pages 81 94. Kevin Knight and Steve K. Luk. Building a large knowledge base for machine translation, ht Proceedings of the Eleventh National Coy@renee on Arti]icial Intelligence (AAAI-9/~). 500 -~  ..~:,., . - -=-""  _..  _ . .&ass 0 50000 n ~ .~ . 1,* +  .~  *+-  . ; -5; , :~:;  . :~.7~.~ ~ ~ ^ - -~- . o 400OO f ,  "" +- ~-"" + ~ -, ""~2x-+, [ ? : [ - ...... ; """"7 ........ 2,=_ ~ 0 =0000 j   +J"" J j  1"" "" .,::iff ""4. -a  + -a--  -#. -~-- - .a  ~  . 0 ~o00o d-;9~7~ -7 + 5~:7~:=-+: ; :  ~ . =-~++:7:: ~ -:~ +--~ ....... "" ~5_~Ztt::~:ll;: ; i I "" , ; . A  / , -?~-  <F"" ~. "" ""~"" ~ 257 44"" ? o ;oo~ ._~-.c_-__~ / 0 00000 I 000 005 010 015 020 025 030 ,335 040 045 050 055 060 065 070 o75 050 085 090 095 ~00 .~ umrn~i-~ Lenqth Figure 2:  F-measur(: w;. summary length for all fimr topics. ~bi)ic signature cl(mrly outperforin tfidf and baselin(, ex(:ei)t for tit(: case of topic 258 where t)(~rforman(:(; for tim thr(;e methods are roughly equal. I__ I - - - - - -~~g-  lO% I ___~o~a -ao~~a--- -  4o~ I ~0%- -  [  ~o~ [ ~,o~ ~o% I 9o~ I lOO% I [ ~.+,_~.,~dl .... i . :ms o.a-~9 I o..~.o o.aa4 o . ,~:c -  I ...ao=, [ __2 :~r  I~  o.ara oar , ;  I . .a t , ,  I e.a.~w-I +4.58 +7.48 +15.6a +14.17 +8.66 +3. s i~  I -2 ,7d  -2 .19- - [ 257-h , , * , , l i  .... r--- (1.1-}98 {~.15.__.5 I c,,, ,,.is., "".~L I o.,~~--F--,~.~,, I - -o  t,l o.1~, I ?. !s~ [ _,.~r_,a,,r [ -55.11- -38.56 I -"".5U ~""> ~"".0;   "" +   I S ~:    I ~ ~ "" "" ""  I +r  0 ~t  . , 257_,~i,ic.~ig +45.5~ +64.06 +31.88 ~ +20.40 [ +20.60 [ 4_-~01 +12.4&- I14 .24  - O. (h.~] [_ 25u_h~,~.,li . L_  o l  tk_ o 270 I ""4-2 ~ *~:~ I ,, ~,r_, L_  ""47t_ J  .4 r , ,  1 - - ~ -  1  o.~,__,+~ o s_,Z._J [ 271_l,aseli . I <,at tT_.._,~. :,,~; T--,Ta77--- ..a:~ _L ,, :s:,r, .L .... ~~~~:~- i~-  T -  o.ae~ ] , ) . lO  j _ _  + 4 ~ _ ~ ~ s . ~  +~.a,~ I +~.~o_ l l  0.,~, ] Table 3: F..measule t)erformanc(~ differen(:e compared to 1)aselin(~ nt(:thod in t)ercentage. Cohmms indicate at diffe.rent summary lengths related to fldl length docum(mts. Values in the 1)aselin(,. rows are F-measure s(:ores. Vahms in the tfidf and tot)i(: signatur(~ rows arc i)(.rformmlc(~ increase or (h,.crease divide(l by their (:orr(.,sI)ontling baseline scores and shown in I)er(:(mtag(!. Inderje(?t Mani, David House, Gary KMn, Lyn(~tt(~ ttirschman, Leo ()brst, Thdr6se Firmin, Micha(d Chrzanowski, and Beth Sundheim. The T IPSTER SUMMAC t~xl smmnmiza- tion evaluation final r(:t)ort. %~(:hnical I/,ol)orl; MTR98W0000138, The MITRE Corporation. Christopher Manning and Hinrich Schiitzc. 1999. t}mdatious of Statistical Natural Language Pro~ cessing. Kathh~(m M(:K(!own and l)rag(mfir R. I ladev. I I (  ra t ,  i l l g  S l l l l l l l l ; l l  i ( : s  o f  I l t l l l t ,  i  [ ) l  [~  l l ( !~vs  articles. In hMtu.iet~t Mani and Mark T. Maybury, edi t,ors, Admm.ces in Automatic Text Sv,.mmarization, chapter 24, pagc+s 381 :/89. Ellen Riloff and Jeffrey Lorenzen. Ext:raction- t)a:;e,d text cateI,dorization: Generating donmin- qmcitic role relationships atttonmtically. In Tomek Strzalkowski, editor, Natural Language In- formation, Retrieval. Kluwer Academic Publishc, r.q. An ompirical study of automated dictionary construction for information extraction in three domains. Artificial Intelligence ,Journal, 85, August. Introduction to information extraction. http://www.mu(:.sai(:.(:om. Jinxi Xu and W. Bruc(! Query ex- pal>ion using local and gh)bal document analysis. In lrocee.dings of the 17th Annual International A(JM SIGIR Cot@rence. on Research and Devel- opment in Information l{etrieval, pages 4 -11.","The Automated Acquisition Of Topic Signatures For Text Summarization
In order to produce a good summary, one has to identify the most relevant portions of a given text.
We describe in this paper a method for automatically training topic signatures -- sets of related words, with associated weights, organized around head topics and illustrate with signature we created with 6,194 TREC collection texts over 4 selected topics.
We describe the possible integration of topic signatures with ontologies and its evaluaton on an automated text summarization system.
We first introduced topic signatures which are topic relevant terms for summarization.
","The Automated Acquisit ion of Topic Signatures for Text Summarizat ion Chin -Yew L in  and  Eduard  Hovy In fo rmat ion  S(:i(umes I l l s t i tu te Un ivers i ty  of Southern  Ca l i fo rn ia Mar ina  del Rey, CA  90292, USA { cyl,hovy }C~isi.edu Abst rac t In order to produce, a good summary, one has to identify the most relevant portions of a given text.We describe in this t)at)er a method for au- tomatically training tel)it, signatures--sets of related words,",0.7646130919456482,0.8699267506599426,0.8138772249221802
"Improving Machine Translation Performance By Exploiting Non-Parallel Corpora We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. Parallel texts—texts that are translations of each other—are an important resource in many NLP applications. They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001). Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register. There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations). This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., parliamentary proceedings) will perform poorly when translating texts from a different domain (e.g., news articles). One way to alleviate this lack of parallel data is to exploit a much more available and diverse resource: comparable non-parallel corpora. Comparable corpora are texts that, while not parallel in the strict sense, are somewhat related and convey overlapping information. Good examples are the multilingual news feeds produced by news agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc. Such texts are widely available on the Web for many language pairs and domains. They often contain many sentence pairs that are fairly good translations of each other. The ability to reliably identify these pairs would enable the automatic creation of large and diverse parallel corpora. However, identifying good translations in comparable corpora is hard. Even texts that convey the same information will exhibit great differences at the sentence level. Consider the two newspaper articles in Figure 1. They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang. The lines in the figure connect sentence pairs that are approximate translations of each other. Discovering these links automatically is clearly non-trivial. Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document. Thus, they perform poorly on comparable, non-parallel texts. What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context. This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004). We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context. Using this classifier, we extract parallel sentences from very large comparable corpora of newspaper articles. We demonstrate the quality of our A pair of comparable texts. extracted sentences by showing that adding them to the training data of an SMT system improves the system’s performance. We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method; by running our extraction system on a large comparable corpus in a bootstrapping manner, we can obtain performance improvements of more than 50% over a baseline MT system trained only on existing parallel data. Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains. In such a situation, the test data is in-domain, and the training data is out-of-domain. The problem is that in such conditions, translation performance is quite poor; the out-of-domain data doesn’t really help the system to produce good translations. What is needed is additional in-domain training data. Our goal is to get such data from a large in-domain comparable corpus and use it to improve the performance of an out-of-domain MT system. We work in the context of Arabic-English and Chinese-English statistical machine translation systems. Our out-of-domain data comes from translated United Nations proceedings, and our indomain data consists of news articles. In this experimental framework we have access to a variety of resources, all of which are available from the Linguistic Data Consortium:1 In summary, we call in-domain the domain of the test data that we wish to translate; in this article, that in-domain data consists of news articles. Out-of-domain data is data that belongs to any other domain; in this article, the out-of-domain data is drawn from United Nations (UN) parliamentary proceedings. We are interested in the situation that exists when we need to translate news data but only have UN data available for training. The solution we propose is to get comparable news data, automatically extract parallel sentences from it, and use these sentences as additional training data; we will show that doing this improves translation performance on a news test set. The Arabic-English and Chinese-English resources described in the previous paragraph enable us to simulate our conditions of interest and perform detailed measurements of the impact of our proposed solution. We can train baseline systems on UN parallel data (using the data from the first bullet in the previous paragraph), extract additional news data from the large comparable corpora (the fourth bullet), accurately measure translation performance on news data against four reference translations (the third bullet), and compare the impact of the automatically extracted news data with that of similar amounts of human-translated news data (the second bullet). In the next section, we give a high-level overview of our parallel sentence extraction system. In Section 3, we describe in detail the core of the system, the parallel sentence classifier. In Section 4, we discuss several data extraction experiments. In Section 5, we evaluate the extracted data by showing that adding it to out-of-domain parallel data improves the in-domain performance of an out-of-domain MT system, and in Section 6, we show that in certain cases, even larger improvements can be obtained by using bootstrapping. In Section 7, we present examples of sentence pairs extracted by our method and discuss some of its weaknesses. Before concluding, we discuss related work. The general architecture of our extraction system is presented in Figure 2. Starting with two large monolingual corpora (a non-parallel corpus) divided into documents, we begin by selecting pairs of similar documents (Section 2.1). From each such pair, we generate all possible sentence pairs and pass them through a simple word-overlapbased filter (Section 2.2), thus obtaining candidate sentence pairs. The candidates are presented to a maximum entropy (ME) classifier (Section 2.3) that decides whether the sentences in each pair are mutual translations of each other. The resources required by the system are minimal: a bilingual dictionary and a small amount of parallel data (used for training the ME classifier). The dictionaries used in our experiments are learned automatically from (out-of-domain) parallel corpora;2 thus, the only resource used by our system consists of parallel sentences. 2 If such a resource is unavailable, other dictionaries can be used. Our comparable corpus consists of two large, non-parallel, news corpora, one in English and the other in the foreign language of interest (in our case, Chinese or Arabic). The parallel sentence extraction process begins by selecting, for each foreign article, English articles that are likely to contain sentences that are parallel to those in the foreign one. This step of the process emphasizes recall rather than precision. For each foreign document, we do not attempt to find the best-matching English document, but rather a set of similar English documents. The subsequent components of the system are robust enough to filter out the extra noise introduced by the selection of additional (possibly bad) English documents. We perform document selection using the Lemur IR toolkit3 (Ogilvie and Callan 2001). We first index all the English documents into a database. For each foreign document, we take the top five translations of each of its words (according to our probabilistic dictionary) and create an English language query. The translation probabilities are only used to choose the word translations; they do not appear in the query. We use the query to run TF-IDF retrieval against the database, take the top 20 English documents returned by Lemur, and pair each of them with the foreign query document. This document matching procedure is both slow (it looks at all possible document pairs, so it is quadratic in the number of documents) and imprecise (due to noise in the dictionary, the query will contain many wrong words). We attempt to fix these problems by using the following heuristic: we consider it likely that articles with similar content have publication dates that are close to each other. Thus, each query is actually run only against English documents published within a window of five days around the publication date of the foreign query document; we retrieve the best 20 of these documents. Each query is thus run against fewer documents, so it becomes faster and has a better chance of getting the right documents at the top. Our experiments have shown that the final performance of the system does not depend too much on the size of the window (for example, doubling the size to 10 days made no difference). However, having no window at all leads to a decrease in the overall performance of the system. From each foreign document and set of associated English documents, we take all possible sentence pairs and pass them through a word-overlap filter. The filter verifies that the ratio of the lengths of the two sentences is no greater than two. It then checks that at least half the words in each sentence have a translation in the other sentence, according to the dictionary. Pairs that do not fulfill these two conditions are discarded. The others are passed on to the parallel sentence selection stage. This step removes most of the noise (i.e., pairs of non-parallel sentences) introduced by our recall-oriented document selection procedure. It also removes good pairs that fail to pass the filter because the dictionary does not contain the necessary entries; but those pairs could not have been handled reliably anyway, so the overall effect of the filter is to improve the precision and robustness of the system. However, the filter also accepts many wrong pairs, because the word-overlap condition is weak; for instance, stopwords almost always have a translation on the other side, so if a few of the content For each candidate sentence pair, we need a reliable way of deciding whether the two sentences in the pair are mutual translations. This is achieved by a Maximum Entropy (ME) classifier (described at length in Section 3), which is the core component of our system. Those pairs that are classified as being translations of each other constitute the output of the system. In the Maximum Entropy (ME) statistical modeling framework, we impose constraints on the model of our data by defining a set of feature functions. These feature functions emphasize properties of the data that we believe to be useful for the modeling task. For example, for a sentence pair sp, the word overlap (the percentage of words in either sentence that have a translation in the other) might be a useful indicator of whether the sentences are parallel. We therefore define a feature function f (sp), whose value is the word overlap of the sentences in sp. According to the ME principle, the optimal parametric form of the model of our data, taking into account the constraints imposed by the feature functions, is a log linear combination of these functions. Thus, for our classification problem, we have: where ci is the class (c0=”parallel”, c1=”not parallel”), Z(sp) is a normalization factor, and fij are the feature functions (indexed both by class and by feature). The resulting model has free parameters λj, the feature weights. The parameter values that maximize the likelihood of a given training corpus can be computed using various optimization algorithms (see [Malouf 2002] for a comparison of such algorithms). For our particular classification problem, we need to find feature functions that distinguish between parallel and non-parallel sentence pairs. For this purpose, we compute and exploit word-level alignments between the sentences in each pair. A word alignment between two sentences in different languages specifies which words in one sentence are translations of which words in the other. Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of a translation model (Brown et al. 1990). Since then, they were found useful in many other NLP applications (e.g., word sense tagging [Diab and Resnik 2002] and question answering [Echihabi and Marcu 2003]). Figures 3 and 4 give examples of word alignments between two English-Arabic sentence pairs from our comparable corpus. Each figure contains two alignments. The one on the left is a correct alignment, produced by a human, while the one on the right Alignments between two parallel sentences. was computed automatically. As can be seen from the gloss next to the Arabic words, the sentences in Figure 3 are parallel while the sentences in Figure 4 are not. In a correct alignment between two non-parallel sentences, most words would have no translation equivalents; in contrast, in an alignment between parallel sentences, most words would be aligned. Automatically computed alignments, however, may have incorrect connections; for example, on the right side of Figure 3, the Arabic word issue is connected to the comma; and in Figure 4, the Arabic word at is connected to the English phrase its case to the. Such errors are due to noisy dictionary entries and to Alignments between two non-parallel sentences. shortcomings of the model used to generate the alignments. Thus, merely looking at the number of unconnected words, while helpful, is not discriminative enough. Still, automatically produced alignments have certain additional characteristics that can be exploited. We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to. The presence, in an automatically computed alignment between a pair of sentences, of words of high fertility (such as the Arabic word at in Figure 4) is indicative of non-parallelism. Most likely, these connections were produced because of a lack of better alternatives. Another aspect of interest is the presence of long contiguous connected spans, which we define as pairs of bilingual substrings in which the words in one substring are connected only to words in the other substring. Such a span may contain a few words without any connection (a small percentage of the length of the span), but no word with a connection outside the span. Examples of such spans can be seen in Figure 3: the English strings after saudi mediation failed or to the international court ofjustice together with their Arabic counterparts. Long contiguous connected spans are indicative of parallelism, since they suggest that the two sentences have long phrases in common. And, in contrast, long substrings whose words are all unconnected are indicative of non-parallelism. To summarize, our classifier uses the following features, defined over two sentences and an automatically computed alignment between them. General features (independent of the word alignment): In order to compute word alignments we need a simple and efficient model. We want to align a large number of sentences, with many out-of-vocabulary words, in reasonable time. We also want a model with as few parameters as possible—preferably only wordfor-word translation probabilities. One such model is the IBM Model 1 (Brown et al. 1993). According to this model, given foreign sentence (fj1<=j<=m), English sentence (ei1<=i<=l), and translation probabilities t(fj|ei), the best alignment f → e is obtained by linking each foreign word fj to its most likely English translation argmaxeit(fj|ei). Thus, each foreign word is aligned to exactly one English word (or to a special NULL token). Due to its simplicity, this model has several shortcomings, some more structural than others (see Moore [2004] for a discussion). Thus, we use a version that is augmented with two simple heuristics that attempt to alleviate some of these shortcomings. One possible improvement concerns English words that appear more than once in a sentence. According to the model, a foreign word that prefers to be aligned with such an English word could be equally well aligned with any instance of that word. In such situations, instead of arbitrarily choosing the first instance or a random instance, we attempt to make a ”smarter” decision. First, we create links only for those English words that appear exactly once; next, for words that appear more than once, we choose which instance to link with so that we minimize the number of crossings with already existing links. The second heuristic attempts to improve the choice of the most likely English translation of a foreign word. Our translation probabilities are automatically learned from parallel data, and we learn values for both t(fj|ei) and t(ei|fj). We can therefore decide that the most likely English translation of fj is argmaxei{t(fj|ei),t(ei|fj)}. Using both sets of probabilities is likely to help us make a better-informed decision. Using this alignment strategy, we follow (Och and Ney 2003) and compute one alignment for each translation direction (f - 4e and e -4 f), and then combine them. Och and Ney present three combination methods: intersection, union, and refined (a form of intersection expanded with certain additional neighboring links). Thus, for each sentence pair, we compute five alignments (two modified-IBMModel-1 plus three combinations) and then extract one set of general features and five sets of alignment features (as described in the previous section). We create training instances for our classifier from a small parallel corpus. The simplest way to obtain classifier training data from a parallel corpus is to generate all possible sentence pairs from the corpus (the Cartesian product). This generates 5,0002 training instances, out of which 5,000 are positive (i.e., belong to class ”parallel”) and the rest are negative. One drawback of this approach is that the resulting training set is very imbalanced, i.e., it has many more negative examples than positive ones. Classifiers trained on such data do not achieve good performance; they generally tend to predict the majority class, i.e., classify most sentences as non-parallel (which has indeed been the case in our experiments). Our solution to this is to downsample, i.e., eliminate a number of (randomly selected) negative instances. Another problem is that the large majority of sentence pairs in the Cartesian product have low word overlap (i.e., few words that are translations of each other). As explained in Section 2 (and shown in Figure 2), when extracting data from a comparable corpus, we only apply the classifier on the output of the word-overlap filter. Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples. We therefore use for training only those pairs from the Cartesian product that are accepted by the word-overlap filter. This has the additional advantage that, since all these pairs have many words in common, the classifier learns to make distinctions that cannot be made based on word overlap alone. To summarize, we prepare our classifier training set in the following manner: starting from a parallel corpus of about 5,000 sentence pairs, we generate all the sentence pairs in the Cartesian product; we discard the pairs that do not fulfill the conditions of the word-overlap filter; if the resulting set is imbalanced, i.e., the ratio of non-parallel to parallel pairs is greater than five, we balance it by removing randomly chosen nonparallel pairs. We then compute word alignments and extract feature values. Using the training set, we compute values for the classifier feature weights using the YASMET4 implementation of the GIS algorithm (Darroch and Ratcliff 1974). Since we are dealing with few parameters and have sufficiently many training instances, using more advanced training algorithms is unlikely to bring significant improvements. We test the performance of the classifier by generating test instances from a different parallel corpus (also around 5,000 sentence pairs) and checking how many of these instances are correctly classified. We prepare the test set by creating the Cartesian product of the sentences in the test parallel corpus and applying the word-overlap filter (we do not perform any balancing). Although we apply the filter, we still conceptually classify all pairs from the Cartesian product in a two-stage classification process: all pairs discarded by the filter are classified as ”non-parallel,” and for the rest, we obtain predictions from the classifier. Since this is how we apply the system on truly unseen data, this is the process in whose performance we are interested. We measure the performance of the classification process by computing precision and recall. Precision is the ratio of sentence pairs correctly judged as parallel to the total number of pairs judged as parallel by the classifier. Recall is the ratio of sentence pairs correctly identified as parallel by the classifier to the total number of truly parallel pairs—i.e., the number of pairs in the parallel corpus used to generate the test instances. Both numbers are expressed as percentages. More formally: let classified parallel be the total number of sentence pairs from our test set that the classifier judged as parallel, classified well be the number of pairs that the classifier correctly judged as parallel, and true parallel be the total number of parallel pairs in the test set. Then: classified parallel true parallel There are two factors that influence a classifier’s performance: dictionary coverage and similarity between the domains of the training and test instances. We performed evaluation experiments to account for both these factors. All our dictionaries are automatically learned from parallel data; thus, we can create dictionaries of various coverage by learning them from parallel corpora of different sizes. We use five dictionaries, learned from five initial out-of-domain parallel corpora, whose sizes are 100k, 1M, 10M, 50M, and 95M tokens, as measured on the English side. Since we want to use the classifier to extract sentence pairs from our in-domain comparable corpus, we test it on instances generated from an in-domain parallel corpus. In order to measure the effect of the domain difference, we use two training sets: one generated from an in-domain parallel corpus and another one from an out-ofdomain parallel corpus. In summary, for each language pair, we use the following corpora: Precision and recall of the Arabic-English classifiers. From each initial, out-of-domain corpus, we learn a dictionary. We then take the classifier training and test corpora and, using the method described in the previous section, create two sets of training instances and one set of test instances. We train two classifiers (one on each training set) and evaluate both of them on the test set. The parallel corpora used for generating training and test instances have around 5k sentence pairs each (approximately 150k English tokens), and generate around 10k training instances (for each training set) and 8k test instances. Precision and recall of the Chinese-English classifiers. Figures 5 and 6 show the recall and precision of our classifiers, for both ArabicEnglish and Chinese-English. The results show that the precision of our classification process is robust with respect to dictionary coverage and training domain. Even when starting from a very small initial parallel corpus, we can build a high-precision classifier. Having a good dictionary and training data from the right domain does help though, mainly with respect to recall. The classifiers achieve high precision because their positive training examples are clean parallel sentence pairs, with high word overlap (since the pairs with low overlap are filtered out); thus, the classification decision frontier is pushed towards “goodlooking” alignments. The low recall results are partly due to the word-overlap filter (the first stage of the classification process), which discards many parallel pairs. If we don’t apply the filter before the classifier, the recall results increase by about 20% (with no loss in precision). However, the filter plays a very important role in keeping the extraction pipeline robust and efficient (as shown in Figure 7, the filter discards 99% of the candidate pairs), so this loss of recall is a price worth paying. Classifier evaluations using different subsets of features show that most of the classifier performance comes from the general features together with the alignment features concerning the percentage and number of words that have no connection. However, we expect that in real data, the differences between parallel and non-parallel pairs are less clear than in our test data (see the discussion in Section 7) and can no The amounts of data processed by our system during extraction from the Chinese-English comparable corpus. longer be accounted for only by counting the linked words; thus, the other features should become more important. The comparable corpora that we use for parallel sentence extraction are collections of news stories published by the Agence France Presse and Xinhua News agencies. They are parts of the Arabic, English, and Chinese Gigaword corpora which are available from the Linguistic Data Consortium. From these collections, for each language pair, we create an in-domain comparable corpus by putting together articles coming from the same agency and the same time period. Table 1 presents in detail the sources and sizes of the resulting comparable corpora. The remainder of the section presents the various data sets that we extracted automatically from these corpora, under various experimental conditions. In the experiments described in Section 3.4, we started out with five out-of-domain initial parallel corpora of various sizes and obtained five dictionaries and five out-ofdomain trained classifiers (per language pair). We now plug in each of these classifiers (and their associated dictionaries) in our extraction system (Section 2) and apply it to our comparable corpora. We thus obtain five Arabic-English and five Chinese-English extracted corpora. Note that in each of these experiments the only resource used by our system is the initial, out-of-domain parallel corpus. Thus, the experiments fit in the framework of interest described in Section 1, which assumes the availability of (limited amounts of) out-of-domain training data and (large amounts of) in-domain comparable data. Table 2 shows the sizes of the extracted corpora for each initial corpus size, for both Chinese-English and Arabic-English. As can be seen, when the initial parallel corpus is very small, the amount of extracted data is also quite small. This is due to the low coverage of the dictionary learned from that corpus. Our candidate pair selection step (Section 2.2) discards pairs with too many unknown (or unrelated) words, according to the dictionary; thus, only few sentences fulfill the word-overlap condition of our filter. As mentioned in Section 1, our goal is to use the extracted data as additional MT training data and obtain better translation performance on a given in-domain MT test set. A simple way of estimating the usefulness of the data for this purpose is to measure its coverage of the test set, i.e., the percentage of running n-grams from the test corpus that are also in our corpus. Tables 3 and 4 present the coverage of our extracted corpora. For each initial corpus size, the first column shows the coverage of that initial corpus, and the second column shows the coverage of the initial corpus plus the extracted corpus. Each cell contains four numbers that represent the coverage with respect to unigrams, bigrams, trigrams, and 4-grams. The numbers show that unigram coverage depends only on the size of the corpus (and not on the domain), but for longer n-grams, our in-domain extracted data brings significant improvements in coverage. The extraction experiments from the previous section are controlled experiments in which we only use limited amounts of parallel data for our extraction system. In this section, we describe experiments in which the goal is to assess the applicability of our method to data that we mined from the Web. We obtained comparable corpora from the Web by going to bilingual news websites (such as Al-Jazeera) and downloading news articles in each language independently. In order to get as many articles as possible, we used the web site’s search engine to get lists of articles and their URLs, and then crawled those lists. We used the AgentBuilder tool (Ticrea and Minton 2003; Minton, Ticrea, and Beach 2003) for crawling. The tool can be programmed to automatically initiate searches with different parameters and to identify and extract the desired article URLs (as well as other information such as dates and titles) from the result pages. Table 5 shows the sources, time periods, and size of the datasets that we downloaded. For the extraction experiments, we used dictionaries of high coverage, learned from all our available parallel training data. The sizes of these training corpora, measured in number of English tokens, are as follows: We applied our extraction method on both the LDC-released Gigaword corpora and the Web-downloaded comparable corpora. For each language pair, we used the highest precision classifier from those presented in Section 3.4. In order to obtain data of higher quality, we didn’t use all the sentences classified as parallel, but only those for which the probability computed by our classifier was higher than 0.70. Table 6 shows the amounts of extracted data, measured in number of English tokens. For ArabicEnglish, we were able to extract from the Gigaword corpora much more data than in our previous experiments (see Table 2), clearly due to the better dictionary. For ChineseEnglish, there was no increase in the size of extracted data (although the amount from Table 6 is smaller than that from Table 2, it counts only sentence pairs extracted with confidence higher than 0.70). In the previous section, we measured, for our training corpora, their coverage of the test set (Tables 3 and 4). We repeated the measurements for the training data from Table 6 and obtained very similar results: using the additional extracted data improves coverage, especially for longer n-grams. To give the reader an idea of the amount of data that is funneled through our system, we show in Figure 7 the sizes of the data processed by each of the system’s components during extraction from the Gigaword and Web-based Chinese-English comparable corpora. We use a dictionary learned from a parallel corpus on 190M English tokens and a classifier trained on instances generated from a parallel corpus of 220k English tokens. We start with a comparable corpus consisting of 500k Chinese articles and 600k English articles. The article selection step (Section 2.1) outputs 7.5M similar article pairs; from each article pair we generate all possible sentence pairs and obtain 2,400M pairs. Of these, less than 1% (17M) pass the candidate selection stage (Section 2.2) and are presented to the ME classifier. The system outputs 430k sentence pairs (9.5M English tokens) that have been classified as parallel (with probability greater than 0.7). The figure also presents, in the lower part, the parameters that control the filtering at each stage. the particular sentence pair to be parallel; the higher the value, the higher the classifier’s confidence. Thus, in order to obtain higher precision, we can choose to define as parallel only those pairs for which the classifier probability is above a certain threshold. In the experiments from Section 4.1, we use the (default) threshold of 0.5, while in Section 4.2 we use 0.7. Our main goal is to extract, from an in-domain comparable corpus, parallel training data that improves the performance of an out-of-domain-trained SMT system. Thus, we evaluate our extracted corpora by showing that adding them to the out-of-domain training data of a baseline MT system improves its performance. We first evaluate the extracted corpora presented in Section 4.1. The extraction system used to obtain each of those corpora made use of a certain initial out-of-domain parallel corpus. We train a Baseline MT system on that initial corpus. We then train another MT system (which we call PlusExtracted) on the initial corpus plus the extracted corpus. In order to compare the quality of our extracted data with that of human-translated data from the same domain, we also train an UpperBound MT system, using the initial corpus plus a corpus of in-domain, human-translated data. For each initial corpus, we use the same amount of human-translated data as there is extracted data (see Table 2). Thus, for each language pair and each initial parallel corpus, we compare 3 MT systems: Baseline, PlusExtracted, and UpperBound. All our MT systems were trained using a variant of the alignment template model described in (Och 2003). Each system used two language models: a very large one, trained on 800 million English tokens, which is the same for all the systems; and a smaller one, trained only on the English side of the parallel training data for that particular system. This ensured that any differences in performance are caused only by differences in the training data. The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.5 Translation performance was measured using the automatic BLEU evaluation metric (Papineni et al. 2002) on four reference translations. Figures 8 and 9 show the BLEU scores obtained by our MT systems. The 95% confidence intervals of the scores computed by bootstrap resampling (Koehn 2004) are marked on the graphs; the delta value is around 1.2 for Arabic-English and 1 for Chinese-English. As the results show, the automatically extracted additional training data yields significant improvements in performance over most initial training corpora for both language pairs. At least for Chinese-English, the improvements are quite comparable to those produced by the human-translated data. And, as can be expected, the impact of the extracted data decreases as the size of the initial corpus increases. In order to check that the classifier really does something important, we performed a few experiments without it. After the article selection step, we simply paired each foreign document with the best-matching English one, assumed they are parallel, sentence-aligned them with a generic sentence alignment method, and added the resulting data to the training corpus. The resulting BLEU scores were practically the same as the baseline; thus, our classifier does indeed help to discover higher-quality parallel data. We also measured the MT performance impact of the extracted corpora described in Section 4.2. We trained a Baseline MT system on all our available (in-domain and MT performance improvements for Arabic-English. out-of-domain) parallel data, and a PlusExtracted system on the parallel data plus the extracted in-domain data. Clearly, we have access to no UpperBound system in this case. The results are presented in the first two rows of Table 7. Adding the extracted corpus lowers the score for the Arabic-English system and improves the score for the Chinese-English one; however, none of the differences are statistically significant. Since the baseline systems are trained on such large amounts of data (see Section 4.2), it is not surprising that our extracted corpora have no significant impact. In an attempt to give a better indication of the value of these corpora, we used them alone as MT training data. The BLEU scores obtained by the systems we trained on them are presented in the third row of Table 7. For comparison purposes, the last line of the table shows the scores of systems trained on 10M English tokens of outof-domain data. As can be seen, our automatically extracted corpora obtain better MT performance than out-of-domain parallel corpora of similar size. It’s true that this is not a fair comparison, since the extracted corpora were obtained using all our available parallel data. The numbers do show, however, that the extracted data, although it was obtained automatically, is of good value for machine translation. As can be seen from Table 2, the amount of data we can extract from our comparable corpora is adversely affected by poor dictionary coverage. Thus, if we start with very little parallel data, we do not make good use of the comparable corpora. One simple way to alleviate this problem is to bootstrap: after we’ve extracted some in-domain data, we can use it to learn a new dictionary and go back and extract again. Bootstrapping was also successfully applied to this problem by Fung and Cheung (2004). We performed bootstrapping iterations starting from two very small corpora: 100k English tokens and 1M English tokens, respectively. After each iteration, we trained MT performance improvements for Chinese-English. (and evaluated) an MT system on the initial data plus the data extracted in that iteration. We did not use any of the data extracted in previous iterations since it is mostly a subset of that extracted in the current iteration. We iterated until there were no further improvements in MT performance on our development data. Figures 10 and 11 show the sizes of the data extracted at each iteration, for both initial corpus sizes. Iteration 0 is the one that uses the dictionary learned from the initial corpus. Starting with 100k words of parallel data, we eventually collect 20M words of in-domain Arabic-English data and 90M words of in-domain Chinese-English data. Figures 12 and 13 show the BLEU scores of these MT systems. For comparison purposes, we also plotted on each graph the performance of our best MT system for that language pair, trained on all our available parallel data (Table 7). As we can see, bootstrapping allows us to extract significantly larger amounts of data, which leads to significantly higher BLEU scores. Starting with as little as 100k English tokens of parallel data, we obtain MT systems that come within 7–10 BLEU points of systems trained on parallel corpora of more than 100M English tokens. This shows that using our method, a good-quality MT system can be built from very little parallel data and a large amount of comparable, non-parallel data. We conclude the description of our method by presenting a few sentence pairs extracted by our system. We chose the examples by looking for cases when a given foreign sentence was judged parallel to several different English sentences. Figures 14 and 15 show the foreign sentence in Arabic and Chinese, respectively, followed by a human-produced translation in bold italic font, followed by the automatically extracted matching English sentences in normal font. The sentences are picked from the data sets presented in Section 4.2. The examples reveal the two main types of errors that our system makes. The first type concerns cases when the system classifies as parallel sentence pairs that, although they share many content words, express slightly different meanings, as in Figure 15, example 7. The second concerns pairs in which the two sentences convey different amounts of information. In such pairs, one of the sentences contains a transSizes of the Chinese-English corpora extracted using bootstrapping, in millions of English tokens. BLEU scores of the Arabic-English MT systems using bootstrapping. lation of the other, plus additional (often quite long) phrases (Figure 15, examples 1 and 5). These errors are caused by the noise present in the automatically learned dictionaries and by the use of a weak word alignment model for extracting the classifier BLEU scores of the Chinese-English MT systems using bootstrapping. features. In an automatically learned dictionary, many words (especially the frequent, non-content ones) will have a lot of spurious translations. The IBM-1 alignment model takes no account of word order and allows a source word to be connected to arbitrarily many target words. Alignments computed using this model and a noisy, automatically learned, dictionary will contain many incorrect links. Thus, if two sentences share several content words, these incorrect links together with the correct links between the common content words will yield an alignment good enough to make the classifier judge the sentence pair as parallel. The effect of the noise in the dictionary is even more clear for sentence pairs with few words, such as Figure 14, example 6. The sentences in that example are tables of soccer team statistics. They are judged parallel because corresponding digits align to each other, and according to our dictionary, the Arabic word for “Mexico” can be translated as any of the country names listed in the example. These examples also show that the problem of finding only true translation pairs is hard. Two sentences may share many content words and yet express different meanings (see Figure 14, example 1). However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance. While there is a large body of work on bilingual comparable corpora, most of it is focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Koehn and Knight 2000; Gaussier et al. 2004). We are aware of only three previous efforts aimed at discovering parallel sentences. Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus. Utiyama et. al (2003) use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. Fung and Cheung (2004) present an extraction method similar to ours but focus on “very-non-parallel corpora,” aggregations of Chinese and English news stories from different sources and time periods. The first two systems extend algorithms designed to perform sentence alignment of parallel texts. They start by attempting to identify similar article pairs from the two corpora. Then they treat each of those pairs as parallel texts and align their sentences by defining a sentence pair similarity score and use dynamic programming to find the least-cost alignment over the whole document pair. In the article pair selection stage, the researchers try to identify, for an article in one language, the best matching article in the other language. Zhao and Vogel (2002) measure article similarity by defining a generative model in which an English story generates a Chinese story with a given probability. Utiyama et al. (2003) use the BM25 (Robertson and Walker 1994) similarity measure. The two works also differ in the way they define the sentence similarity score. Zhao and Vogel (2002) combine a sentence length model with an IBM Model 1-type translation model. Utiyama et al. (2003) define a score based on word overlap (i.e., number of word pairs from the two sentences that are translations of each other), which also includes the similarity score of the article pair from which the sentence pair originates. The performance of these approaches depends heavily on the ability to reliably find similar document pairs. Moreover, comparable article pairs, even those similar in content, may exhibit great differences at the sentence level (reorderings, additions, etc). Therefore, they pose hard problems for the dynamic programming alignment approach. In contrast, our method is more robust. The document pair selection part plays a minor role; it only acts as a filter. We do not attempt to find the best-matching English document for each foreign one, but rather a set of similar documents. And, most importantly, we are able to reliably judge each sentence pair in isolation, without need for context. On the other hand, the dynamic programming approach enables discovery of many-to-one sentence alignments, whereas our method is limited to finding one-toone alignments. The approach of Fung and Cheung (2004) is a simpler version of ours. They match each foreign document with a set of English documents, using a threshold on their cosine similarity. Then, from each document pair, they generate all possible sentence pairs, compute their cosine similarity, and apply another threshold in order to select the ones that are parallel. Using the set of extracted sentences, they learn a new dictionary, try to extend their set of matching document pairs (by looking for other documents that contain these sentences), and iterate. The evaluation methodologies of these previous approaches are less direct than ours. Utiyama et al. (2003) evaluate their sentence pairs manually; they estimate that about 90% of the sentence pairs in their final corpus are parallel. Fung and Cheung (2004) also perform a manual evaluation of the extracted sentences and estimate their precision to be 65.7% after bootstrapping. In addition, they also estimate the quality of a lexicon automatically learned from those sentences. Zhao and Vogel (2002) go one step further and show that the sentences extracted with their method improve the accuracy of automatically computed word alignments, to an F-score of 52.56% over a baseline of 46.46%. In a subsequent publication, Vogel (2003) evaluates these sentences in the context of an MT system and shows that they bring improvement under special circumstances (i.e., a language model constructed from reference translations) designed to reduce the noise introduced by the automatically extracted corpus. We go even further and demonstrate that our method can extract data that improves end-to-end MT performance without any special processing. Moreover, we show that our approach works even when only a limited amount of initial parallel data (i.e., a low-coverage dictionary) is available. The problem of aligning sentences in comparable corpora was also addressed for monolingual texts. Barzilay and Elhadad (2003) present a method of aligning sentences in two comparable English corpora for the purpose of building a training set of text-totext rewriting examples. Monolingual parallel sentence detection presents a particular challenge: there are many sentence pairs that have low lexical overlap but are nevertheless parallel. Therefore pairs cannot be judged in isolation, and context becomes an important factor. Barzilay and Elhadad (2003) make use of contextual information by detecting the topical structure of the articles in the two corpora and aligning them at paragraph level based on the topic assigned to each paragraph. Afterwards, they proceed and align sentences within paragraph pairs using dynamic programming. Their results show that both the induced topical structure and the paragraph alignment improve the precision of their extraction method. A line of research that is both complementary and related to ours is that of Resnik and Smith (2003). Their STRAND Web-mining system has a purpose that is similar to ours: to identify translational pairs. However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences. Resnik and Smith (2003) show that their approach is able to find large numbers of similar document pairs. Their system is potentially a good way of acquiring comparable corpora from the Web that could then be mined for parallel sentences using our method. The most important feature of our parallel sentence selection approach is its robustness. Comparable corpora are inherently noisy environments, where even similar content may be expressed in very different ways. Moreover, out-of-domain corpora introduce additional difficulties related to limited dictionary coverage. Therefore, the ability to reliably judge sentence pairs in isolation is crucial. Comparable corpora of interest are usually of large size; thus, processing them requires efficient algorithms. The computational processes involved in our system are quite modest. All the operations necessary for the classification of a sentence pair (filter, word alignment computation, and feature extraction) can be implemented efficiently and scaled up to very large amounts of data. The task can be easily parallelized for increased speed. For example, extracting data from 600k English documents and 500k Chinese documents (Section 4.2) required only about 7 days of processing time on 10 processors. The data that we extract is useful. Its impact on MT performance is comparable to that of human-translated data of similar size and domain. Thus, although we have focused our experiments on the particular scenario where there is little in-domain training data available, we believe that our method can be useful for increasing the amount of training data, regardless of the domain of interest. As we have shown, this could be particularly effective for language pairs for which only very small amounts of parallel data are available. By acquiring a large comparable corpus and performing a few bootstrapping iterations, we can obtain a training corpus that yields a competitive MT system. We suspect our approach can be used on comparable corpora coming from any domain. The only domain-dependent element of the system is the date window parameter of the article selection stage (Figure 7); for other domains, this can be replaced with a more appropriate indication of where the parallel sentences are likely to be found. For example, if the domain were that of technical manuals, one would cluster printer manuals and aircraft manuals separately. It is important to note that our work assumes that the comparable corpus does contain parallel sentences (which is the case for our data). Whether this is true for comparable corpora from other domains is an empirical question outside the scope of this article; however, both our results and those of Resnik and Smith (2003) strongly indicate that good data is available on the Web. Lack of parallel corpora is a major bottleneck in the development of SMT systems for most language pairs. The method presented in this paper is a step towards the important goal of automatic acquisition of such corpora. Comparable texts are available on the Web in large quantities for many language pairs and domains. In this article, we have shown how they can be efficiently mined for parallel sentences. This work was supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276. The experiments were run on University of Southern California’s high-performance computer cluster HPC (http://www.usc.edu/hpcc). We would like to thank Hal Daum´e III, Alexander Fraser, Radu Soricut, as well as the anonymous reviewers, for their helpful comments. Any remaining errors are of course our own.","Improving Machine Translation Performance By Exploiting Non-Parallel Corpora
We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.
We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.
Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.
We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.
Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.
We use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles.
We filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary).
We define features primarily based on IBM Model 1 alignments (Brown et al, 1993).
","We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-",0.960810124874115,0.890114963054657,0.9241124391555786
"Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950s. Sense disambiguation is an &quot;intermediate task&quot; (Wilks and Stevenson 1996), which is not an end in itself, but rather is necessary at one level or another to accomplish most natural language processing tasks. It is obviously essential for language understanding applications, such as message understanding and man-machine communication; it is at least helpful, and in some instances required, for applications whose aim is not language understanding: analysis is to analyze the distribution of predefined categories of words—i.e., words indicative of a given concept, idea, theme, etc.—across a text. The need for sense disambiguation in such analysis, in order to include only those instances of a word in its proper sense, has long been recognized (see, for instance, Stone et al. [1966], Stone [1969], Kelly and Stone [1975]; for a more recent discussion see Litowski [1997]). and is masculine in the former sense, feminine in the latter) to properly tag it as a masculine noun. Sense disambiguation is also necessary for certain syntactic analyses, such as prepositional phrase attachment (Jensen and Binot 1987; Whittemore, Ferrara, and Brunner 1990; Hindle and Rooth 1993), and, in general, restricts the space of competing parses (Alshawi and Carter 1994). The problem of word sense disambiguation (WSD) has been described as &quot;AI-complete,&quot; that is, a problem which can be solved only by first resolving all the difficult problems in artificial intelligence (Al), such as the representation of common sense and encyclopedic knowledge. The inherent difficulty of sense disambiguation was a central point in Bar-Hillel's well-known treatise on machine translation (Bar-Hillel 1960), where he asserted that he saw no means by which the sense of the word pen in the sentence The box is in the pen could be determined automatically. Bar-Hillel's argument laid the groundwork for the ALPAC report (ALPAC 1966), which is generally regarded as the direct cause for the abandonment of most research on machine translation in the early 1960s. At about the same time, considerable progress was being made in the area of knowledge representation, especially the emergence of semantic networks, which were immediately applied to sense disambiguation. Work on word sense disambiguation continued throughout the next two decades in the framework of AI-based natural language understanding research, as well as in the fields of content analysis, stylistic . and literary analysis, and information retrieval. In the past ten years, attempts to automatically disambiguate word senses have multiplied, due, like much other similar activity in the field of computational linguistics, to the availability of large amounts of machine-readable text and the corresponding development of statistical methods to identify and apply information about regularities in this data. Now that other problems amenable to these methods, such as part-of-speech disambiguation and alignment of parallel translations, have been fairly thoroughly addressed, the problem of word sense disambiguation has taken center stage, and it is frequently cited as one of the most important problems in natural language processing research today. Given the progress that has been recently made in WSD research and the rapid development of methods for solving the problem, it is appropriate at this time to stand back and assess the state of the field and to consider the next steps that need to be taken. To this end, this paper surveys the major, well-known approaches to word sense disambiguation and considers the open problems and directions of future research. In general terms, word sense disambiguation involves the association of a given word in a text or discourse with a definition or meaning (sense) which is distinguishable from other meanings potentially attributable to that word. The task therefore necessarily involves two steps: (1) the determination of all the different senses for every word relevant (at least) to the text or discourse under consideration; and (2) a means to assign each occurrence of a word to the appropriate sense. Much recent work on WSD relies on predefined senses for step (1), including: The precise definition of a sense is, however, a matter of considerable debate within the community. The variety of approaches to defining senses has raised concern about the comparability of much WSD work, and given the difficulty of the problem of sense definition, no definitive solution is likely to be found soon (see Section 3.2). However, since the earliest days of WSD work, there has been general agreement that the problems of morpho-syntactic disambiguation and sense disambiguation can be disentangled (see, e.g., Kelly and Stone [19751). That is, for homographs with different parts of speech (e.g., play as a verb and noun), morphosyntactic disambiguation accomplishes sense disambiguation, and therefore (especially since the development of reliable part-of-speech taggers), WSD work has focused largely on distinguishing senses among homographs belonging to the same syntactic category. Step (2), the assignment of words to senses, is accomplished by reliance on two major sources of information: All disambiguation work involves matching the context of the instance of the word to be disambiguated with either information from an external knowledge source (knowledge-driven WSD), or information about the contexts of previously disambiguated instances of the word derived from corpora (data-driven or corpus-based WSD). Any of a variety of association methods is used to determine the best match between the current context and one of these sources of information, in order to assign a sense to each word occurrence. The following sections survey the approaches applied to date. The first attempts at automated sense disambiguation were made in the context of machine translation (MT). In his famous memorandum (available mimeographed in 1949, but not printed until 1955) Weaver discusses the need for WSD in machine translation and outlines the basis of an approach to WSD that underlies all subsequent work on the topic: If one examines the words in a book, one at a time as through an opaque mask with a hole in it one word wide, then it is obviously impossible to determine, one at a time, the meaning of the words.... But if one lengthens the slit in the opaque mask, until one can see not only the central word in question but also say N words on either side, then if N is large enough one can unambiguously decide the meaning of the central word.... The practical question is: &quot;What minimum value of N will, at least in a tolerable fraction of cases, lead to the correct choice of meaning for the central word?&quot; (1955, 20) A well-known early experiment by Kaplan (1950) attempted to answer this question at least in part, by presenting ambiguous words in their original context and in a variant context providing one or two words on either side to seven translators. Kaplan observed that sense resolution given two words on either side of the word was not significantly better or worse than when given the entire sentence. The same phenomenon has been reported by several researchers since Kaplan's work appeared: e.g., Masterman (1962), Koutsoudas and Korthage (1956) on Russian, and Gougenheim and Michea (1961) and Choueka and Lusignan (1985) on French. Reifler 's (1955) &quot;semantic coincidences&quot; between a word and its context quickly became the determining factor in WSD. The complexity of the context, and in particular the role of syntactic relations, was also recognized; for example, Reifler (1955) says: Grammatical structure can also help disambiguate, as, for instance, the word keep, which can be disambiguated by determining whether its object is gerund (He kept eating), adjectival phrase (He kept calm), or noun phrase (He kept a record). The goal of MT was initially modest, focused primarily on the translation of technical texts and in all cases dealing with texts from particular domains. Weaver discusses the role of the domain in sense disambiguation, making a point that was reiterated several decades later by Gale, Church, and Yarowsky (1992c): In mathematics, to take what is probably the easiest example, one can very nearly say that each word, within the general context of a mathematical article, has one and only one meaning. (1955, 20) Following directly from this observation, much effort in the early days of machine translation was devoted to the development of specialized dictionaries or &quot;microglossaries&quot; (Oswald 1952, 1957; Oswald and Lawson 1953; Oettinger 1955; Dostert 1955; Gould 1957; Panov 1960). Such microglossaries contain only the meaning of a given word relevant for texts in a particular domain of discourse; e.g., a microglossary for the domain of mathematics would contain only the relevant definition of triangle, and not the definition of triangle as a musical instrument. The need for knowledge representation for WSD was also acknowledged from the outset: Weaver concludes by noting the &quot;tremendous amount of work [needed] in the logical structure of languages&quot; (1995, 23). Several researchers attempted to devise Ide and Veronis Introduction an &quot;interlingua&quot; based on logical and mathematical principles that would solve the disambiguation problem by mapping words in any language to a common semantic/conceptual representation. Among these efforts, those of Richens and Masterman eventually led to the notion of the &quot;semantic network&quot; (Richens [1958], Masterman [1962]; see Section 2.2.1); following on this, the first machine-implemented knowledge base was constructed from Roget's Thesaurus (Masterman 1957). Masterman applied this knowledge base to the problem of WSD: in an attempt to translate Virgil's Georgics by machine, she looked up, for each Latin word stem, the translation in a Latin-English dictionary and then looked up this word in the word-to-head index of Roget's. In this way, each Latin word stem was associated with a list of Roget head numbers associated with its English equivalents. The numbers for words appearing in the same sentence were then examined for overlaps. Finally, English words appearing under the multiply-occurring head categories were chosen for the translation.' Masterman's methodology is strikingly similar to that underlying much of the knowledge-based WSD accomplished recently (see Section 2.3). It is interesting to note that Weaver's text also outlined the statistical approach to language analysis prevalent now, nearly fifty years later: This approach brings into the foreground an aspect of the matter that probably is absolutely basic—namely, the statistical character of the problem.... And it is one of the chief purposes of this memorandum to emphasize that statistical semantic studies should be undertaken, as a necessary primary step. (1955, 22) Several authors followed this approach in the early days of machine translation (e.g., Richards 1953; Yngve 1955; Parker-Rhodes 1958). Estimations of the degree of polysemy in texts and dictionaries were made: Harper, working on Russian texts, determined the number of polysemous words in an article on physics to be approximately 30% (Harper 1957a) and 43% in another sample of scientific writing (Harper 1957b); he also found that Callaham's Russian-English dictionary provides, on average, 8.6 English equivalents for each Russian word, of which 5.6 are quasi-synonyms, thus yielding approximately three distinct English equivalents for each Russian word. Bel'skaja (1957) reports that in the first computerized Russian dictionary, 500 out of 2,000 words are polysemous. Pimsleur (1957) introduced the notion of levels of depth for a translation: level 1 uses the most frequent equivalent (e.g., German schwer heavy), producing a text where 80% of the words are correctly translated; level 2 distinguishes additional meanings (e.g., schwer = difficult), producing a translation which is 90% correct; etc. Although the terminology is different, this is very similar to the notion of baseline tagging used in modern work (see, e.g., Gale, Church, and Yarowsky [1992b]). A convincing implementation of many of these ideas was made several years later, paradoxically at the moment when MT began its decline. Madhu and Lytle (1965), working from the observation that domain constrains sense, calculated sense frequency for texts in different domains and applied a Bayesian formula to determine the probability of each sense in a given context—a technique similar to that applied in much later work and which yielded a similar 90% correct disambiguation result (see Section 2.4). The striking fact about this early work on WSD is the degree to which the fundamental problems and approaches to the problem were foreseen and developed at that time. However, without large-scale resources, most of these ideas remained untested and to a large extent, forgotten, until several decades later. Al methods began to flourish in the early 1960s and began to attack the problem of language understanding. As a result, WSD in Al work was typically accomplished in the context of larger systems intended for full language understanding. In the spirit of the times, such systems were almost always grounded in some theory of human language understanding that they attempted to model, and often involved the use of detailed knowledge about syntax and semantics to perform their task, which was exploited for WSD. in the late 1950s and were immediately applied to the problem of representing word meanings.2 Masterman (1962), working in the area of machine translation, used a semantic network to derive the representation of sentences in an interlingua comprised of fundamental language concepts; sense distinctions are implicitly made by choosing representations that reflect groups of closely related nodes in the network. She developed a set of 100 primitive concept types (THING, DO, etc. ), in terms of which her group built a 15,000-entry concept dictionary, where concept types are organized in a lattice with inheritance of properties from superconcepts to subconcepts. Building on this and on work on semantic networks by Richens (1958), Quillian (1961, 1962a, 1962b, 1967, 1968, 1969) built a network that includes links among words (tokens) and concepts (types), in which links are labeled with various semantic relations or simply indicate associations between words. The network is created starting from dictionary definitions, but is enhanced by human knowledge that is hand-encoded. When two words are presented to the network, Quillian's program simulates the gradual activation of concept nodes along a path of links originating from each input word by means of marker passing; disambiguation is accomplished because only one concept node associated with a given input word is likely to be involved in the most direct path found between the two input words. Quillian's work informed later dictionary-based approaches to WSD (see Section 2.3.1). Subsequent AI-based approaches exploited the use of frames containing information about words and their roles and relations to other words in individual sentences. For example, Hayes (1976, 1977a, 1977b, 1978) uses a combination of a semantic network and case frames. The network consists of nodes representing noun senses and links represented by verb senses; case frames impose IS-A and PART-OF relations on the network. As in Quillian's system, the network is traversed to find chains of connections between words. Hayes work shows that homonyms can be fairly accurately disambiguated using this approach, but it is less successful for other kinds of polysemy. Hirst (1987) also uses a network of frames and, again following Quillian, marker passing to find minimum-length paths of association between frames for senses of words in context in order to choose among them. He introduces &quot;polaroid words,&quot; a mechanism which progressively eliminates inappropriate senses based on syntactic evidence provided by the parser, together with semantic relations found in the frame network. Eventually only one sense remains; however, Hirst reports that in cases where some word (including words other than the target) in the sentence is used metaphorically, metonymically, or in an unknown sense, the polaroids often end by eliminating all possible senses, and fail. Wilks' preference semantics ([1968, 1969, 1973, 1975a, 1975b, 1975c, 1975d]; see the survey by Wilks and Fass [1990]), which uses Masterman's primitives, is essentially a case-based approach to natural language understanding and one of the first specifically designed to deal with the problem of sense disambiguation. Preference semantics specifies selectional restrictions for combinations of lexical items in a sentence that can be relaxed when a word with the preferred restrictions does not appear, thus enabling, especially, the handling of metaphor (as in My car drinks gasoline, where the restrictions on drink prefer an animate subject but allow an inanimate one). Boguraev (1979) shows that preference semantics is inadequate to deal with polysemous verbs and attempts to improve on Wilks' method by using a combination of evidence, including selectional restrictions, preferences, case frames, etc. He integrates semantic disambiguation with structural disambiguation to enable judgments about the semantic coherence of a given sense assignment. Like many other systems of the era, these systems are sentencebased and do not account for phenomena at other levels of discourse, such as topical and domain information. The result is that some kinds of disambiguation are difficult or impossible to accomplish. A rather different approach to language understanding, which contains a substantial sense discrimination component, is the Word Expert Parser (Small 1980, 1983; Small and Reiger 1982; Adriaens 1986, 1987, 1989; Adriaens and Small 1988). The approach derives from the somewhat unconventional theory that human knowledge about language is organized primarily as knowledge about words rather than rules. Their system models what its authors feel is the human language understanding process: a co-ordination of information exchange among word experts about syntax and semantics as each determines its involvement in the environment under question. Each expert contains a discrimination net for all senses of the word, which is traversed on the basis of information supplied by the context and other word experts, ultimately arriving at a unique sense, which is then added to a semantic representation of the sentence. The well-known drawback of the system is that the word experts need to be extremely large and complex to accomplish the goal, which is admittedly greater than sense disambiguation.' Dahlgren's (1988) language understanding system includes a sense disambiguation component that uses a variety of types of information: fixed phrases, syntactic information (primarily, selectional restrictions), and commonsense reasoning. The reasoning module, because it is computationally intensive, is invoked only in cases where the other two methods fail to yield a result. Although her original assumption was that much disambiguation could be accomplished based on paragraph topic, she found that half of the disambiguation was actually accomplished using fixed phrase and syntactic information, while the other half was accomplished using commonsense reasoning. Reasoning often involves traversing an ontology to find common ancestors for words in context; her work anticipates Resnik's (1993a, 1993b, 1995a) results by determining that ontological similarity, involving a common ancestor in the ontology, is a powerful disambiguator. She also notices that verb selectional restrictions are an lished that semantic priming—a process in which the introduction of a certain concept will influence and facilitate the processing of subsequently introduced concepts that are semantically related—plays a role in disambiguation by humans (see, e.g., Meyer and Schvaneveldt [19711). This idea is realized in spreading activation models (see Collins and Loftus [1975]; Anderson [1976, 1983]), where concepts in a semantic network are activated upon use, and activation spreads to connected nodes. Activation is weakened as it spreads, but certain nodes may receive activation from several sources and be progressively reinforced. McClelland and Rumelhart (1981) added to the model by introducing the notion of inhibition among nodes, where the activation of a node might suppress, rather than activate, certain of its neighbors (see also Feldman and Ballard [1982]). Applied to lexical disambiguation, this approach assumes that activating a node corresponding to, say, the concept THROW will activate the &quot;physical object&quot; sense of ball, whose activation would in turn inhibit the activation of other senses of ball, such as &quot;social event.&quot; Quillian's semantic network, described above, is the earliest implementation of a spreading activation network used for word sense disambiguation. A similar model is implemented by Cottrell and Small (1983); see also Cottrell (1985). In both of these models, each node in the network represents a specific word or concept.' Waltz and Pollack (1985) and Bookman (1987) hand-encode sets of semantic &quot;microfeatures,&quot; corresponding to fundamental semantic distinctions (animate/inanimate, edible/inedible, threatening/safe, etc. ), characteristic durations of events (second, minute, hour, day, etc. ), locations (city, country, continent, etc. ), and other similar distinctions, in their networks. In Waltz and Pollack (1985), sets of microfeatures have to be manually primed by a user to activate a context for disambiguating a subsequent input word, but Bookman (1987) describes a dynamic process in which the microfeatures are automatically activated by the preceding text, thus acting as a short-term context memory. In addition to these local models (i.e., models in which one node corresponds to a single concept), distributed models have also been proposed (see, for example, Kawamoto [1988]). However, whereas local models can be constructed a priori, distributed models require a learning phase using disambiguated examples, which limits their practicality. The difficulty of hand-crafting the knowledge sources required for AI-based systems restricted them to &quot;toy&quot; implementations handling only a tiny fraction of the language. Consequently, disambiguation procedures embedded in such systems are most usually tested on only a very small test set in a limited context (most often, a single sentence), making it impossible to determine their effectiveness on real texts. For less obvious reasons, many of the AI-based disambiguation results involve highly ambiguous words and fine sense distinctions (e.g., ask, idea, hand, move, use, work, etc.) and unlikely test sentences (The astronomer married the star), which make the results even less easy to evaluate in the light of the now-known difficulties of discriminating even gross sense distinctions. The AI-based work of the 1970s and 1980s was theoretically interesting but not at all practical for language understanding in any but extremely limited domains. A significant roadblock to generalizing WSD work was the difficulty and cost of hand-crafting the enormous amounts of knowledge required for WSD: the so-called &quot;knowledge acquisition bottleneck&quot; (Gale, Church, and Yarowsky 1993). Work on WSD reached a turning point in the 1980s when large-scale lexical resources, such as dictionaries, thesauri, and corpora, became widely available. Attempts were made to automatically extract knowledge from these sources (Sections 2.3.1 and 2.3.2) and, more recently, to construct large-scale knowledge bases by hand (Section 2.3.3). A corresponding shift away from methods based in linguistic theories and towards empirical methods also occurred at this time, as well as a decrease in emphasis on do-all systems in favor of &quot;intermediate&quot; tasks such as WSD. Heidon 1985; Markowitz, Ahlswede, and Evens 1986; Byrd et al. 1987; Nakamura and Nagao 1988; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1990). This work contributed significantly to lexical semantic studies, but it appears that the initial goal—the automatic extraction of large knowledge bases—was not fully achieved: the only currently widely available large-scale lexical knowledge base (WordNet, see below) was created by hand. We have elsewhere demonstrated the difficulties of automatically extracting relations as simple as hyperonymy (Wronis and Ide 1991; Ide and Wronis 1993a, 1993b), in large part due to the inconsistencies in dictionaries themselves (well-known to lexicographers, cf. Atkins and Levin [19881, Kilgarriff [1994]) as well as the fact that dictionaries are created for human use, and not for machine exploitation. Despite its shortcomings, the machine-readable dictionary provides a ready-made source of information about word senses and therefore rapidly became a staple of WSD research. The methods employed attempt to avoid the problems cited above by using the text of dictionary definitions directly, together with methods sufficiently robust to reduce or eliminate the effects of a given dictionary's inconsistencies. All of these methods (and many of those cited elsewhere in this paper) rely on the notion that the most plausible sense to assign to multiple co-occurring words is the one that maximizes the relatedness among the chosen senses. Lesk (1986) created a knowledge base that associated with each sense in a dictionary a &quot;signature&quot;6 composed of the list of words appearing in the definition of that sense. Disambiguation was accomplished by selecting the sense of the target word whose signature contained the greatest number of overlaps with the signatures of neighboring words in its context. The method achieved 50-70% correct disambiguation, using a relatively fine set of sense distinctions such as those found in a typical learner's dictionary. Lesk's method is very sensitive to the exact wording of each definition: the presence or absence of a given word can radically alter the results. However, Lesk's method has served as the basis for most subsequent MRD-based disambiguation work. Wilks et al. (1990) attempted to improve the knowledge associated with each sense by calculating the frequency of co-occurrence for the words in definition texts, from which they derive several measures of the degree of relatedness among words. This metric is then used with the help of a vector method that relates each word and its context. In experiments on a single word (bank), the method achieved 45% accuracy on sense identification, and 90% accuracy on homograph identification. Lesk's method has been extended by creating a neural network from definition texts in the Collins English Dictionary (CED), in which each word is linked to its senses, which are themselves linked to the words in their definitions, which are in turn linked to their senses, etc. (Veronis and Ide 1990).7 Experiments on 23 ambiguous words, each in six contexts (138 pairs of words), produced correct disambiguation, using the relatively fine sense distinctions in the CED, in 71.7% of the cases (three times better than chance: 23.6%) (Ide and Veronis 1990b); in later experiments, improving the parameters and only distinguishing homographs enabled a rate of 85% (vs. chance: 39%) (Veronis and Ide 1995). Applied to the task of mapping the senses of the CED and OALD for the same 23 words (59 senses in all), this method obtained a correct correspondence in 90% of the cases at the sense level, and 97% at the level of homographs (Ide and Veronis 1990a). Sutcliffe and Slater (1995) replicated this method on full text (samples from Orwell's Animal Farm) and found similar results (72% correct sense assignment, compared with a 33% chance baseline, and 40% using Lesk's method). Several authors (for example, Krovetz and Croft [1989], Guthrie et al. [1991], Slator [19921, Cowie, Guthrie, and Guthrie [1992], Janssen [1992], Braden-Harder [1993], Liddy and Paik [19931) have attempted to improve results by using supplementary fields of information in the electronic version of the Longman Dictionary of Contemporary English (LDOCE), in particular, the box codes and subject codes provided for each sense. Box codes include primitives such as ABSTRACT, ANIMATE, HUMAN, etc., and encode type restrictions on nouns and adjectives and on the arguments of verbs. Subject codes use another set of primitives to classify senses of words by subject (ECONOMICS, ENGINEERING, etc.). Guthrie et al. (1991) demonstrate a typical use of this information: in addition to using the Lesk-based method of counting overlaps between definitions and contexts, they impose a correspondence of subject codes in an iterative process. No quantitative evaluation of this method is available, but Cowie, Guthrie, and Guthrie (1992) improve the method using simulated annealing and report results of 47% for sense distinctions and 72% for homographs. The use of LDOCE box codes, however, is problematic: the codes are not systematic (see, for example, Fontenelle [1990]); in later work, Braden-Harder (1993) showed that simply matching box or subject codes is not sufficient for disambiguation. For example, in I tipped the driver, the codes for several senses of the words in the sentence satisfy the necessary constraints (e.g., tip-money + human object or tip-tilt + movable solid object). 7 Note that the assumptions underlying this method are very similar to Quillian's (1968): Thus one may think of a full concept analogically as consisting of all the information one would have if he looked up what will be called the &quot;patriarch&quot; word in a dictionary, then looked up every word in each of its definitions, then looked up every word found in each of these, and so on, continually branching outward... (p. 238). However, Quillian's network also keeps track of semantic relationships among the words encountered along the path between two words, which are encoded in his semantic network; the neural network avoids the overhead of creating the semantic network but loses this relational information. Ide and Wronis Introduction In many ways, the supplementary information in the LDOCE, and in particular the subject codes, is similar to that in a thesaurus, which, however, is more systematically structured. Inconsistencies in dictionaries, noted earlier, are not the only and perhaps not the major source of their limitations for WSD. While dictionaries provide detailed information at the lexical level, they lack pragmatic information that enters into sense determination (see, e.g., Hobbs [1987]). For example, the link between ash and tobacco, cigarette, or tray in a network such as Quillian's is very indirect, whereas in the Brown corpus, the word ash co-occurs frequently with one of these words. It is therefore not surprising that corpora have become a primary source of information for WSD; this development is outlined below in Section 2.3. 2.3.2 Thesauri. Thesauri provide information about relationships among words, most notably synonymy. Roget's International Thesaurus, which was put into machine-tractable form in the 1950s and has been used in a variety of applications including machine translation (Masterman 1957), information retrieval (Sparck-Jones 1964, 1986), and content analysis (Sedelow and Sedelow [1969], see also Sedelow and Sedelow [1986, 1992]), also supplies an explicit concept hierarchy consisting of up to eight increasingly refined levels.8 Typically, each occurrence of the same word under different categories of the thesaurus represents different senses of that word; i.e., the categories correspond roughly to word senses (Yarowsky 1992). A set of words in the same category are semantically related. The earliest known use of Roget's for WSD is the work of Masterman (1957), described above in Section 2.1. Several years later, Patrick (1985) used Roget's to discriminate among verb senses, by examining semantic clusters formed by &quot;e-chains&quot; derived from the thesaurus (Bryan [1973, 1974]; see also Sedelow and Sedelow [1986]). He uses &quot;word-strong neighborhoods,&quot; comprising word groups in low-level semicolon groups, which are the most closely related semantically in the thesaurus, and words connected to the group via chains. He is able, he claims, to discriminate the correct sense of verbs such as inspire (to raise the spirits vs. to inhale, breathe in, sniff, etc. ), and question (to doubt vs. to ask a question) with high reliability. Bryan's earlier work had already demonstrated that homographs can be distinguished by applying a metric based on relationships defined by his chains (Bryan 1973, 1974). Similar work is described in Sedelow and Mooney (1988). Yarowsky (1992) derives classes of words by starting with words in common categories in Roget's (4th edition). A 100-word context of each word in the category is extracted from a corpus (the 1991 electronic text of Grolier's Encyclopedia), and a mutualinformation-like statistic is used to identify words most likely to co-occur with the category members. The resulting classes are used to disambiguate new occurrences of a polysemous word: the 100-word context of the polysemous occurrence is examined for words in various classes, and Bayes' Rule is applied to determine the class most likely to be that of the polysemous word. Since class is assumed by Yarowsky to represent a particular sense of a word, assignment to a class identifies the sense. He reports 92% accuracy on a mean three-way sense distinction. Yarowsky notes that his method is best for extracting topical information, which is in turn most successful for disambiguating nouns (see Section 3.1.2). He uses the broad category distinctions supplied by Roget's, although he points out that the lower-level information may provide rich information for disambiguation. Patrick's much earlier study, on the other hand, exploits the lower levels of the concept hierarchy, in which words are more closely related semantically, as well as connections among words within the thesaurus itself; however, despite its promise this work has not been built upon since. Like machine-readable dictionaries, a thesaurus is a resource created for humans and is therefore not a source of perfect information about word relations. It is widely recognized that the upper levels of its concept hierarchy are open to disagreement (although this is certainly true for any concept hierarchy), and that they are so broad as to be of little use in establishing meaningful semantic categories. Nonetheless, thesauri provide a rich network of word associations and a set of semantic categories potentially valuable for language-processing work; however, Roget's and other thesauri have not been used extensively for WSD.9 WordNet combines the features of many of the other resources commonly exploited in disambiguation work: it includes definitions for individual senses of words within it, as in a dictionary; it defines &quot;synsets&quot; of synonymous words representing a single lexical concept, and organizes them into a conceptual hierarchy,1° like a thesaurus; and it includes other links among words according to several semantic relations, including hyponymy/hyperonymy, antonymy, and meronymy. As such, it currently provides the broadest set of lexical information in a single resource. Another, possibly more compelling, reason for WordNet's widespread use is that it is the first broad-coverage lexical resource that is freely and widely available; as a result, whatever its limitations, WordNet's sense divisions and lexical relations are likely to impact the field for several years to come.11 Some of the earliest attempts to exploit WordNet for sense disambiguation are in the field of information retrieval. Using the hyponomy links for nouns in WordNet, Voorhees (1993) defines a construct called a hood in order to represent sense categories, much as Roget's categories are used in the methods outlined above. A hood for a given word w is defined as the largest connected subgraph that contains w. For each content Ide and Veronis Introduction word in a document collection, Voorhees computes the number of times each synset appears above that word in the WordNet noun hierarchy, which gives a measure of the expected activity (global counts); she then performs the same computation for words occurring in a particular document or query (local counts). The sense corresponding to the hood root for which the difference between the global and local counts is the greatest is chosen for that word. Her results, however, indicate that her technique is not a reliable method for distinguishing WordNet's fine-grained sense distinctions. In a similar study, Richardson and Smeaton (1994) create a knowledge base from WordNet's hierarchy and apply a semantic similarity function (developed by Resnik—see below) to accomplish disambiguation, also for the purposes of information retrieval. They provide no formal evaluation but indicate that their results are &quot;promising.&quot; Sussna (1993) computes a semantic distance metric for each of a set of input text terms (nouns) in order to disambiguate them. He assigns weights based on the relation type (synonymy, hyperonymy, etc.) to WordNet links, and defines a metric that takes account of the number of arcs of the same type leaving a node and the depth of a given edge in the overall &quot;tree.&quot; This metric is applied to arcs in the shortest path between nodes (word senses) to compute semantic distance. The hypothesis is that for a given set of terms occurring near each other in a text, choosing the senses that minimize the distance among them selects the correct senses. Sussna's disambiguation results are demonstrated to be significantly better than chance. His work is particularly interesting because it is one of the few to date that utilizes not only WordNet's IS-A hierarchy, but other relational links as well. Resnik (1995a) draws on his body of earlier work on WordNet, in which he explores a measure of semantic similarity for words in the WordNet hierarchy (Resnik 1993a, 1993b, 1995a). He computes the shared information content of words, which is a measure of the specificity of the concept that subsumes the words in the WordNet IS-A hierarchy—the more specific the concept that subsumes two or more words, the more semantically related they are assumed to be. Resnik contrasts his method of computing similarity to those which compute path length (e.g., Sussna 1993), arguing that the links in the WordNet taxonomy do not represent uniform distances (cf. Resnik 1995b). Resnik's method, applied using WordNet's fine-grained sense distinctions and measured against the performance of human judges, approaches human accuracy. Like the other studies cited here, his work considers only nouns. WordNet is not a perfect resource for word sense disambiguation. The most frequently cited problem is the fine-grainedness of WordNet's sense distinctions, which are often well beyond what may be needed in many language-processing applications (see Section 3.2). Voorhees' (1993) hood construct is an attempt to access sense distinctions that are less fine-grained than WordNet's synsets, and less coarse-grained than the 10 WordNet noun hierarchies; Resnik's (1995a) method allows for detecting sense distinctions at any level of the WordNet hierarchy. However, it is not clear what the desired level of sense distinction should be for WSD (or if it is the same for all word categories, all applications, etc. ), or if this level is even captured in WordNet's hierarchy. Discussion within the language-processing community is beginning to address these issues, including the most difficult one of defining what we mean by &quot;sense&quot; (see Section 3.2). As outlined in Buitelaar (1997), sense disambiguation in the generative context starts with a semantic tagging that points to a complex knowledge representation reflecting all of a word's systematically related senses, after which semantic processing may derive a discourse-dependent interpretation containing more precise sense information about the occurrence. Buitelaar (1997) describes the use of CORELEX for underspecified semantic tagging (see also Pustejovsky, Boguraev, and Johnston [19951). Viegas, Mahesh, and Nirenburg (forthcoming) describe a similar approach to WSD undertaken in the context of their work on machine translation (see also Mahesh et al. [1997] and Mahesh, Nirenburg, and Beale [1997]). They access a large syntactic and semantic lexicon that provides detailed information about constraints, such as selectional restrictions, for words in a sentence, and then search a richly connected ontology to determine which senses of the target word best satisfy these constraints. They report a success rate of 97%. Like CORELEX, both the lexicon and the ontology are manually constructed, and therefore still limited, although much larger than the resources used in earlier work. However, Buitelaar (1997) describes means to automatically generate CORELEX entries from corpora in order to create domain-specific semantic lexicons, thus demonstrating the potential to access larger-scale resources of this kind. the nineteenth century, the manual analysis of corpora has enabled the study of words and graphemes (Kaeding 1897-1898, Estoup 1902, Zipf 1935) and the extraction of lists of words and collocations for the study of language acquisition or language teaching (Thorndike 1921; Fries and Traver 1940; Thorndike and Lorge 1938,1944; Gougenheim et al. 1956; etc.). Corpora have been used in linguistics since the first half of the twentieth century (e.g., Boas 1940; Fries 1952). Some of this work concerns word senses, and it is often strikingly modern: for example, Palmer (1933) studied collocations in English; Lorge (1949) computed sense frequency information for the 570 most common English words; Eaton (1940) compared the frequency of senses in four languages; and Thorndike (1948) and Zipf (1945) determined that there is a positive correlation between the frequency and the number of synonyms of a word, the latter of which is an indication of semantic richness (the more polysemous a word, the more synonyms it has). A corpus provides a bank of samples that enable the development of numerical language models, and thus the use of corpora goes hand-in-hand with empirical methods. Although quantitative/statistical methods were embraced in early MT work, in the mid-1960s interest in statistical treatment of language waned among linguists due to the trend toward the discovery of formal linguistic rules sparked by the theories of Zellig Harris (1951) and bolstered most notably by the transformational theories of Noam Chomsky (1957).12 Instead, attention turned toward full linguistic analysis and hence toward sentences rather than texts, and toward contrived examples and artificially limited domains instead of general language. During the following 10 to It would be difficult, indeed, in the face of today's activity, not to acknowledge the triumph of the theoretical approach, more precisely, of formal rules as the preferred successor of lexical and syntactic search algorithms in linguistic description. At the same time, common sense should remind us that hypothesis-making is not the whole of science, and that discipline will be needed if the victory is to contribute more than a haven from the rigors of experimentation (p. 313). Ide and VOronis Introduction 15 years, only a handful of linguists continued to work with corpora, most often for pedagogical or lexicographic ends (e.g., Quirk 1960; Michea 1964). Despite this, several important corpora were developed during this period, including the Brown Corpus (Kucera and Francis 1967), the Tresor de la Lan gue Francaise (Imbs 1971), and the Lancaster-Oslo-Bergen (LOB) corpus (Johansson 1980). In the area of natural language processing, the ALPAC report (1966) recommended intensification of corpus-based research for the creation of broad-coverage grammars and lexicons, but because of the shift away from empiricism, little work was done in this area until the 1980s. Until then, the use of statistics for language analysis was almost the exclusive property of researchers in the fields of literary and humanities computing, information retrieval, and the social sciences. Within these fields, work on WSD continued, most notably in the Harvard &quot;disambiguation project&quot; for content analysis (Stone et al. 1966; Stone 1969), and also in the work of Iker (1974, 1975), Choueka and Dreizin (1976) and Choueka and Goldberg (1979). In the context of the shift away from the use of corpora and empirical methods, the work of Weiss (1973) and Kelley and Stone (1975) on the automatic extraction of knowledge for word sense disambiguation seems especially innovative. Weiss (1973) demonstrated that disambiguation rules can be learned from a manually sense-tagged corpus. Despite the small size of his study (five words, a training set of 20 sentences for each word, and 30 test sentences for each word), Weiss's results are encouraging (90% correct). Kelley and Stone's (1975) work, which grew out of the Harvard &quot;disambiguation project&quot; for content analysis, is on a much larger scale; they extract KWIC concordances for 1,800 ambiguous words from a corpus of a half-million words. The concordances serve as a basis for the manual creation of disambiguation rules (&quot;word tests&quot;) for each sense of the 1,800 words. The tests—also very sophisticated for the time—examine the target word context for clues on the basis of collocational information, syntactic relations with context words, and membership in common semantic categories. Their rules perform even better than Weiss's, achieving 92% accuracy for gross homographic sense distinctions. In the 1980s, interest in corpus linguistics was revived (see, for example, Aarts [1990] and Leech [1991]). Advances in technology enabled the creation and storage of corpora larger than had been previously possible, enabling the development of new models most often utilizing statistical methods. These methods were rediscovered first in speech processing (e.g., Jelinek [1976]; see the overview by Church and Mercer [1993] and the collection of reprints by Waibel and Lee [1990]) and were immediately applied to written language analysis (e.g., in the work of Bahl and Mercer [1976], Debili [1977], etc.). For a discussion, see Ide and Walker (1992). In the area of word sense disambiguation, Black (1988) developed a model based on decision trees using a corpus of 22 million tokens, after manually sense-tagging approximately 2,000 concordance lines for five test words. Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik (1990, 1991), Hearst (1991), Leacock, Towell, and Voorhees (1993), Gale, Church, and Yarowsky (1992d, 1993), Bruce and Wiebe (1994), Miller et al. (1994), Niwa and Nitta (1994), Lehman (1994), among others. However, despite the availability of increasingly large corpora, two major obstacles impede the acquisition of lexical knowledge from corpora: the difficulties of manually sense-tagging a training corpus, and data sparseness. distributes a corpus of approximately 200,000 sentences from the Brown Corpus and the Wall Street Journal in which all occurrences of 191 words are hand-tagged with their WordNet senses (see Ng and Lee [1996]). Also, the Cognitive Science Laboratory at Princeton has undertaken the hand-tagging of 1,000 words from the Brown Corpus with WordNet senses (Miller et al. 1993) (so far, 200,000 words are available via ftp), and hand-tagging of 25 verbs in a small segment of the Wall Street Journal (12,925 sentences), is also underway (Wiebe et al. 1997). However, these corpora are far smaller than those typically used with statistical methods. Several efforts have been made to automatically sense-tag a training corpus via bootstrapping methods. Hearst (1991) proposed an algorithm (CatchWord) that includes a training phase during which each occurrence of a set of nouns to be disambiguated is manually sense-tagged in several occurrences.' Statistical information extracted from the context of these occurrences is then used to disambiguate other occurrences. If another occurrence can be disambiguated with certitude, the system automatically acquires additional statistical information from these newly disambiguated occurrences, thus improving its knowledge incrementally. Hearst indicates that an initial set of at least 10 occurrences is necessary for the procedure, and that 20 or 30 occurrences are necessary for high precision. This overall strategy is more or less that of most subsequent work on bootstrapping. Recently, a class-based bootstrapping method for semantic tagging in specific domains has been proposed (Basili et al. 1997). Schiitze (1992, 1993) proposes a method that avoids tagging each occurrence in the training corpus. Using letter fourgrams within a 1,001-character window, his method, building on the vector-space model from information retrieval (see Salton, Wong, and Yang [1975]), automatically clusters the words in the text (each target word is represented by a vector); a sense is then assigned manually to each cluster, rather than to each occurrence. Assigning a sense demands examining 10 to 20 members of each cluster, and each sense may be represented by several clusters. This method reduces the amount of manual intervention but still requires the examination of a hundred or so occurrences for each ambiguous word. A more serious issue for this method is that it is not clear what the senses derived from the clusters correspond to (see, for example Pereira, Tishby, and Lee [1993]); moreover, the senses are not directly usable by other systems, since they are derived from the corpus itself. Brown et al. (1991) and Gale, Church, and Yarowsky, (1992a, 1993) propose the use of bilingual corpora to avoid hand-tagging of training data. Their premise is that different senses of a given word often translate differently in another language (for example, pen in English is stylo in French for its 'writing implement' sense, and encls for its 'enclosure' sense). By using a parallel aligned corpus, the translation of each occurrence of a word such as pen can be used to automatically determine its sense. This method has some limitations, since many ambiguities are preserved in the target language (e.g., French souris—English mouse); furthermore, the few available large-scale parallel corpora are very specialized (for example, the Hansard corpus of Canadian Parliamentary Debates), which skews the sense representation.' Dagan, Itai, and Schwall (1991) and Dagan and Itai (1994) propose a similar method, but instead of a parallel corpus use two monolingual corpora and a bilingual dictionary. This solves, in part, the problems of availability and specificity of domain that plague the parallel corpus approach, since monolingual corpora, including corpora from diverse domains and genres, are much easier to obtain than parallel corpora. Ide and Veronis Introduction Other methods attempt to avoid entirely the need for a tagged corpus, such as many of those cited in the section below (e.g., Yarowsky [1992] who attacks both the tagging and data sparseness problems simultaneously). However, it is likely that, as noted for grammatical tagging (Merialdo 1994), even a minimal phase of supervised learning improves radically on the results of unsupervised methods. Research into means to facilitate and optimize tagging is ongoing; for example, an optimization technique called committee-based sample selection has recently been proposed (Engelson and Dagan 1996), which, based on the observation that a substantial portion of manually tagged examples contribute little to performance, enables avoiding the tagging of examples that carry more or less the same information. Such methods are promising, although to our knowledge they have not been applied to the problem of lexical disambiguation. for much corpus-based work, is especially severe for work in WSD. First, enormous amounts of text are required to ensure that all senses of a polysemous word are represented, given the vast disparity in frequency among senses. For example, in the Brown Corpus (one million words), the relatively common word ash occurs only eight times, and only once in its sense as tree. The sense ashes = remains of cremated body, although common enough to be included in learner's dictionaries such as the LDOCE and the OALD, does not appear, and it would be nearly impossible to find the dozen or so senses in many everyday dictionaries such as the CED. In addition, the many possible co-occurrences for a given polysemous word are unlikely to be found in even a very large corpus, or they occur too infrequently to be significant.' Smoothing is used to get around the problem of infrequently occurring events, and in particular to ensure that non-observed events are not assumed to have a probability of zero. The best-known smoothing methods are that of Turing-Good (Good 1953), which hypothesizes a binomial distribution of events, and that of Jelinek and Mercer (1985), which combines estimated parameters on distinct subparts of the training corpus.' However, these methods do not enable distinguishing between events with the same frequency, such as the ash-cigarette and ash-room example given in footnote 15. Church and Gale (1991) have proposed a means to improve methods for the estimation of bigrams, which could be extended to co-occurrences: they take into account the frequency of the individual words that compose the bigram and make the hypothesis that each word appears independently of the others. However, this hypothesis contradicts hypotheses of disambiguation based on co-occurrence, which rightly assume that some associations are more probable than others. Class-based models attempt to obtain the best estimates by combining observations of classes of words considered to belong to a common category. Brown et al. (1992), Pereira and Tishby (1992), and Pereira, Tishby, and Lee (1993) propose methods that derive classes from the distributional properties of the corpus itself, while other authors use external information sources to define classes: Resnik (1992) uses the taxonomy of WordNet; Yarowsky (1992) uses the categories of Roget's Thesaurus, Slator (1992) and Liddy and Paik (1993) use the subject codes in the LDOCE; Luk (1995) uses conceptual sets built from the LDOCE definitions. Class-based methods answer in part the problem of data sparseness and eliminate the need for pretagged 15 For example, in a window of five words to each side of the word ash in the Brown corpus, commonly associated words such as fire, cigar, volcano, etc., do not appear. The words cigarette and tobacco co-occur with ash only once, with the same frequency as words such as room, bubble, and house. 16 See the survey of methods in Chen and Goodman (1996). data. However, there is some information loss with these methods because the hypothesis that all words in the same class behave in a similar fashion is too strong. For example, residue is a hypernym of ash in WordNet; its hyponyms form the class {ash, cotton(seed) cake, dottle} . Obviously the members of this set of words behave very differently in context: volcano is strongly related to ash, but has little or no relation to the other words in the set. Similarity-based methods Dagan, Marcus, and Markovitch 1993, Dagan, Pereira, and Lee 1994, and Grishman and Sterling 1993 exploit the same idea of grouping observations for similar words, but without regrouping them into fixed classes. Each word has a potentially different set of similar words. Like many class-based methods, such as Brown et al. (1992), similarity-based methods exploit a similarity metric between patterns of co-occurrence. Dagan, Marcus, and Markovitch (1993) give the following example: the pair (chapter, describes) does not appear in their corpus; however, chapter is similar to book, introduction, and section, which are paired with describes in the corpus. On the other hand, the words similar to book are books, documentation, and manuals (see their Figure 1). Dagan, Marcus, and Markovitch's (1993) evaluation seems to show that similarity-based methods perform better than class-based methods. Karov and Edelman (this volume) propose an extension to similarity-based methods by means of an iterative process at the learning stage, which gives results that are 92% accurate on four test words—approximately the same as the best results cited in the literature to date. These results are particularly impressive given that the training corpus contains only a handful of examples for each word, rather than the hundreds of examples required by most methods. We have already noted various problems faced in current WSD research related to specific methodologies. Here, we discuss issues and problems that all approaches to WSD must face and suggest some directions for further work. Context is the only means to identify the meaning of a polysemous word. Therefore, all work on sense disambiguation relies on the context of the target word to provide information to be used for its disambiguation. For data-driven methods, context also provides the prior knowledge with which current context is compared to achieve disambiguation. Broadly speaking, context is used in two ways: Information from microcontext, topical context, and domain contributes to sense selection, but the relative roles and importance of information from the different contexts, and their interrelations, are not well understood. Very few studies have used Ide and Veronis Introduction information of all three types, and the focus in much recent work is on microcontext alone. This is another area where systematic study is needed for WSD. 3.1.1 Microcontext. Most disambiguation work uses the local context of a word occurrence as a primary information source for WSD. Local or &quot;micro&quot; context is generally considered to be some small window of words surrounding a word occurrence in a text or discourse, from a few words of context to the entire sentence in which the target word appears. Context is very often regarded as all words or characters falling within some window of the target, with no regard for distance, syntactic structure, or other relations. Early corpus-based work, such as that of Weiss (1973) used this approach; spreading activation and dictionary-based approaches also do not usually differentiate context input on any basis other than occurrence in a window. Schtitze's vector space method (this volume) is a recent example of an approach that ignores adjacency information. Overall, the bag-of-words approach has been shown to work better for nouns than for verbs (cf. Schtitze, this volume), and to be in general less effective than methods that take other relations into consideration. However, as demonstrated in Yarowsky's (1992) work, the approach is cheaper than those requiring more complex processing and can achieve sufficient disambiguation for some applications. We examine below some of the other parameters. Distance. It is obvious from the quotation in Section 2.1 from Weaver's memorandum that the notion of examining a context of a few words around the target to disambiguate has been fundamental to WSD work since its beginnings: it has been the basis of WSD work in MT, content analysis, AI-based disambiguation, and dictionary-based WSD, as well as the more recent statistical, neural network, and symbolic machine learning, approaches. However, following Kaplan's early experiments (Kaplan 1950), there have been few systematic attempts to answer Weaver's question concerning the optimal value of N. A notable exception is the study of Choueka and Lusignan (1985), who verified Kaplan's finding that 2-contexts are highly reliable for disambiguation, and even 1-contexts are reliable in 8 out of 10 cases. However, despite these findings, the value of N has continued to vary over the course of WSD work more or less arbitrarily. Yarowsky (1993, 1994a, 1994b) examines different windows of microcontext, including 1-contexts, k-contexts, and words pairs at offsets —1 and —2, —1 and +1, and +1 and +2, and sorts them using a log-likelihood ratio to find the most reliable evidence for disambiguation. Yarowsky makes the observation that the optimal value of k varies with the kind of ambiguity: he suggests that local ambiguities need only a window of k = 3 or 4, while semantic or topic-based ambiguities require a larger window of 20-50 words (see Section 3.1.2). No single best measure is reported, suggesting that for different ambiguous words, different distance relations are more efficient. Furthermore, because Yarowsky also uses other information (such as part of speech), it is difficult to isolate the impact of window-size alone. Leacock, Chodorow, and Miller (this volume) use a local window of ±3 open-class words, arguing that this number showed best performance in previous tests. Collocation. The term &quot;collocation&quot; has been used variously in WSD work. The term was popularized by J. R. Firth in his 1951 paper &quot;Modes of meaning&quot;: &quot;One of the meanings of ass is its habitual collocation with an immediately preceding you silly. . . .&quot; He emphasizes that collocation is not simple co-occurrence but is &quot;habitual&quot; or &quot;usual.&quot;17 Halliday's (1961) definition of collocation as &quot;the syntagmatic association of lexical items, quantifiable, textually, as the probability that there will occur at n removes (a distance of n lexical items) from an item x, the items a, b, c. . .&quot; is more workable in computational terms. Based on this definition, a significant collocation can be defined as a syntagmatic association among lexical items, where the probability of item x co-occurring with items a, b, c . . . is greater than chance (Berry-Rogghe 1973). It is in this sense that most WSD work uses the term. There is some psychological evidence that collocations are treated differently from other co-occurrences. For example, Kintsch and Mross (1985) show that priming words that enter frequent collocations with test words (i.e., iron-steel, which they call associative context) activate these test words in lexical decision tasks. Conversely, priming words that are in the thematic context (i.e., relations determined by the situation, scenario, or script such as plane-gate) do not facilitate the subjects' lexical decisions (see also Fischler [1977], Seidenberg et al. [1982], De Groot [1983], Lupker [19841). Yarowsky (1993) explicitly addresses the use of collocations in WSD work, but admittedly adapts the definition to his purpose as &quot;the co-occurrence of two words in some defined relation.&quot; As noted above, he examines a variety of distance relations, but also considers adjacency by part of speech (e.g., first noun to the left). He determines that in cases of binary ambiguity, there exists one sense per collocation, that is, in a given collocation, a word is used with only one sense with 90-99% probability. Syntactic Relations. Earl (1973) used syntax exclusively for disambiguation in machine translation. In most WSD work to date, syntactic information is used in conjunction with other information. The use of selectional restrictions weighs heavily in AI-based work that relies on full parsing, frames, semantic networks, the application of selectional preferences, etc. (Hayes 1977a, 1997b; Wilks 1973 and 1975b; Hirst 1987). In other work, syntax is combined with frequent collocation information: Kelley and Stone (1975), Dahlgren (1988), and Atkins (1987) combine collocation information with rules for determining, for example, the presence or absence of determiners, pronouns, noun complements, as well as prepositions, subject-verb and verb-object relations. More recently, researchers have avoided complex processing by using shallow or partial parsing. In her disambiguation work on nouns, Hearst (1991) segments text into noun phrases, prepositional phrases, and verb groups, and discards all other syntactic information. She examines items that are within ±3 phrase segments from the target and combines syntactic evidence with other kinds of evidence, such as capitalization. Yarowsky (1993) determines various behaviors based on syntactic category; for example, that verbs derive more disambiguating information from their objects than from their subjects, adjectives derive almost all disambiguating information from the nouns they modify, and nouns are best disambiguated by directly adjacent adjectives or nouns. In recent work, syntactic information most often is simply part of speech, used invariably in conjunction with other kinds of information (McRoy 1992; Bruce and Wiebe 1994; Leacock, Chodorow, and Miller, this volume). Evidence suggests that different kinds of disambiguation procedures are needed depending on the syntactic category and other characteristics of the target word (Yarowsky 1993; Leacock, Chodorow, and Miller, this volume)—an idea reminiscent of the word expert approach. However, to date there has been little systematic study Ide and Wronis Introduction of the contribution of different information types for different types of target words. It is likely that this is a next necessary step in WSD work. a given sense of a word, usually within a window of several sentences. Unlike microcontext, which has played a role in disambiguation work since the early 1950s, topical context has been less consistently used. Methods relying on topical context exploit redundancy in a text—that is, the repeated use of words that are semantically related throughout a text on a given topic. Thus, base is ambiguous, but its appearance in a document containing words such as pitcher, and ball is likely to isolate a given sense for that word (as well as the others, which are also ambiguous). Work involving topical context typically uses the bag-of-words approach, in which words in the context are regarded as an unordered set. The use of topical context has been discussed in the field of information retrieval for several years (Anthony 1954; Salton 1968). Recent WSD work has exploited topical context: Yarowsky (1992) uses a 100-word window, both to derive classes of related words and as context surrounding the polysemous target, in his experiments using Roget's Thesaurus (see Section 2.3.2). Voorhees, Leacock, and Towell (1995) experiment with several statistical methods using a two-sentence window; Leacock, Towell, and Voorhees (1993, 1996) have similarly explored topical context for WSD. Gale, Church, and Yarowsky (1993), looking at a context of ±50 words, indicate that while words closest to the target contribute most to disambiguation, they improved their results from 86% to 90% by expanding context from ±6 (a typical span when only microcontext is considered) to ±50 words around the target. In a related study, they make a claim that for a given discourse, ambiguous words are used in a single sense with high probability (&quot;one sense per discourse&quot;) (Gale, Church, and Yarowsky 1992c). Leacock, Chodorow, and Miller (this volume) challenge this claim in their work combining topical and local context, which shows that both topical and local context are required to achieve consistent results across polysemous words in a text (see also Towell and Voorhees, this volume). Yarowsky's (1993) study indicates that while information within a large window can be used to disambiguate nouns, for verbs and adjectives the size of the usable window drops off dramatically with distance from the target word. This supports the claim that both local and topical context are required for disambiguation, and points to the increasingly accepted notion that different disambiguation methods are appropriate for different kinds of words. Methods utilizing topical context can be ameliorated by dividing the text under analysis into subtopics. The most obvious way to divide a text is by sections (Brown and Yule 1983), but this is only a gross division; subtopics evolve inside sections, often in unified groups of several paragraphs. Automatic segmentation of texts into such units would obviously be helpful for WSD methods that use topical context. It has been noted that the repetition of words within successive segments or sentences is a strong indicator of the structure of discourse (Skorochod'ko 1972; Morris 1988; Morris and Hirst 1991); methods exploiting this observation to segment a text into subtopics are beginning to emerge (see, for example, Hearst [1994], van der Eijk [1994], Richmond, Smith, and Amitay [1997]). In this volume, Leacock, Chodorow, and Miller consider the role of microcontext vs. topical context and attempt to assess the contribution of each. Their results indicate that for a statistical classifier, microcontext is superior to topical context as an indicator of sense. However, although a distinction is made between microcontext and topical context in current WSD work, it is not clear that this distinction is meaningful. It may be more useful to regard the two as lying along a continuum, and to consider the role 3.1.3 Domain. The use of domain for WSD is first evident in the microglossaries developed in early MT work (see Section 2.1). The notion of disambiguating senses based on domain is implicit in various AI-based approaches, such as Schank's script approach to natural language processing (Schank and Abelson 1977), which matched words to senses based on the context or &quot;script&quot; activated by the general topic of the discourse. This approach, which activates only the sense of a word relevant to the current discourse domain, demonstrates its limitations of this approach when used in isolation; in the famous example The lawyer stopped at the bar for a drink, the incorrect sense of bar will be assumed if one relies only on the information in a script concerned with law.18 Gale, Church, and Yarowsky's (1992c) claim for one sense per discourse is disputable. Dahlgren (1988) observes that domain does not eliminate ambiguity for some words: she remarks that the noun hand has 16 senses (or so) and retains 10 of them in almost any text. The influence of domain likely depends on factors such as the type of text (how technical the text is, etc. ), the relation among the senses of the target word (strongly or weakly polarized, common vs. specialized usage, etc.). For example, in the French Encyclopaedia Universalis, the word interet (&quot;interest&quot;) appears 62 times in the article on INTEREST—FINANCE, in all cases in its financial sense; the word appears 139 times in the article INTEREST—PHILOSOPHY AND HUMANITIES in its common, nonfinancial, sense. However, in the article THIRD WORLD, the word interet appears two times in each of these senses. 3.2.1 The Bank Model. Most researchers in WSD are currently relying on the sense distinctions provided by established lexical resources, such as machine-readable dictionaries or WordNet (which uses the OALD's senses), because they are widely available. The dominant model in these studies is the &quot;bank&quot; model, which attempts to extend the clear delineation between bank-money and bank-riverside to all sense distinctions. However, it is clear that this convenient delineation is by no means applicable to all or even most other words. Although there is some psychological validity to the notion of senses (Simpson and Burgess 1988; Jorgensen 1990), lexicographers themselves are well aware of the lack of agreement on senses and sense divisions (see, for example, Malakhovski [1987], Robins [1987], Ayto [1983], Stock [19831). The problem of sense division has been an object of discussion since antiquity: Aristotle' devoted a section of his Topics to this subject in 350 B.C. Since then, philosophers and linguists have continued to discuss the topic at length (see Quine [1960], Asprejan [1974], Lyons [1977], Weinrich [1980], Cruse [1986]), but the lack of resolution over 2,000 years is striking. 3.2.2 Granularity. One of the foremost problems for WSD is to determine the appropriate degree of sense granularity. Several authors (for example, Slator and Wilks [1987]) have remarked that the sense divisions one finds in dictionaries are often too fine for the purposes of NLP work. Overly fine sense distinctions create practical difficulIde and Veronis Introduction ties for automated WSD: they introduce significant combinatorial effects (for example, Slator and Wilks [1987] note that the sentence There is a huge envelope of air around the surface of the earth has 284,592 different potential combined sense assignments using the moderately-sized LDOCE); they require making sense choices that are extremely difficult, even for expert lexicographers; and they increase the amount of data required for supervised methods to unrealistic proportions. In addition, the sense distinctions made in many dictionaries are sometimes beyond those which human readers themselves are capable of making. In a well-known study, Kilgarriff (1992, 1993) shows that it is impossible for human readers to assign many words to a unique sense in LDOCE (see, however, the discussion in Wilks [forthcoming]). Recognizing this, Dolan (1994) proposes a method for &quot;ambiguating&quot; dictionary senses by combining them to create grosser sense distinctions. Others have used the grosser sense divisions of thesauri such as Roget's; however, it is often difficult to assign a unique sense, or even find an appropriate one among the options (see, for example, Yarowsky [1992]). Chen and Chang (this volume) propose an algorithm that combines senses in a dictionary (LDOCE) and links them to the categories of a thesaurus (LLOCE). Combining dictionary senses does not solve the problem. First of all, the degree of granularity required is task dependent. Only homograph distinction is necessary for tasks such as speech synthesis or restoration of accents in text, while tasks such as machine translation require fine sense distinctions—in some cases finer than what monolingual dictionaries provide (see, for example, ten Hacken [1990]). For example, the English word river is translated as fleuve in French when the river flows into the ocean, and otherwise as riviere. There is not, however, a strict correspondence between a given task and the degree of granularity required. For example, as noted earlier, the word mouse, although it has two distinct senses (animal, device), translates into French in both cases to souris. On the other hand, for information retrieval the distinction between these two senses of mouse is important, whereas it is difficult to imagine a reason to distinguish river (sense fleuve) - river (sense riviere). Second, and more generally, it is unclear when senses should be combined or split. Even lexicographers do not agree: Fillmore and Atkins (1991) identify three senses of the word risk but find that most dictionaries fail to list at least one of them. In many cases, meaning is best considered as a continuum along which shades of meaning fall (see, for example, Cruse [1986]), and the points at which senses are combined or split can vary dramatically. 3.2.3 Senses or usages? The Aristotelian idea that words correspond to specific objects and concepts was displaced in the twentieth century by the ideas of Saussure and others (Meillet [1926], Hjemslev [1953], Martinet [1960], etc.). For Antoine Meillet, for example, the sense of a word is defined only by the average of its linguistic uses. Wittgenstein takes a similar position in his Philosophische Utersuchungen' in asserting that there are no senses, but only usages: &quot;For a large class of cases—though not for all—in which we employ the word 'meaning' it can be defined thus: the meaning of a word is its use in the language&quot; (1953, Sect. 43). Similar views are apparent in more recent theories of meaning, for example, Bloomfield (1933) and Harris (1954), for whom meaning is a function of distribution; and in Barwise and Perry's (1953) situation semantics, where the sense or senses of a word are seen as an abstraction of the role that it plays systematically in the discourse. The COBUILD project (Sinclair 1987) adopts this view of meaning by attempting to anchor dictionary senses in current usage by creating sense divisions on the basis of clusters of citations in a corpus. Atkins (1987) and Kilgarriff (forthcoming) also implicitly adopt the view of Harris (1954), according to which each sense distinction is reflected in a distinct context. A similar view underlies the class-based methods cited in Section 2.4.3 (Brown et al. 1992; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993). In this volume, Schiitze continues in this vein and proposes a technique that avoids the problem of sense distinction altogether: he creates sense clusters from a corpus rather than relying on a pre-established sense list. 3.2.4 Enumeration or generation? The development of generative lexicons (Pustejovsky 1995) provides a view of word senses that is very different from that of almost all WSD work to date. The enumerative approach assumes an a priori, established set of senses that exist independent of context—fundamentally the Aristotelian view. The generative approach develops a discourse-dependent representation of sense, assuming only underspecified sense assignments until context is taken into account, and bears closer relation to distributional and situational views of meaning. Considering the difficulties of determining an adequate and appropriate set of senses for WSD, it is surprising that little attention has been paid to the potential of the generative view in WSD research. As larger and more complete generative lexicons become available, there is merit to exploring this approach to sense assignment. Given the variety in the studies cited throughout the previous survey, it is obvious that it is very difficult to compare one set of results, and consequently one method, with another. The lack of comparability results from substantial differences in test conditions from study to study. For instance, different types of texts are involved, including both highly technical or domain-specific texts where sense use is limited and general texts where sense use may be more variable. It has been noted that in a commonly used corpus such as the Wall Street Journal, certain senses of typical test words such as line are absent entirely.' When different corpora containing different sense inventories and very different levels of frequency for a given word and/or sense are used, it becomes futile to attempt to compare results. Test words themselves differ from study to study, including not only words whose assignment to clearly distinguishable senses varies considerably or which exhibit very different degrees of ambiguity (e.g., bank vs. line), but also words across different parts of speech and words that tend to appear more frequently in metaphoric, metonymic, and other nonliteral usages (e.g., bank vs. head). More seriously, the criteria for evaluating the correctness of sense assignment vary. Different studies employ different degrees of sense granularity (see Section 3.2 above), ranging from identification of homographs to fine sense distinctions. In addition, the means by which correct sense assignment is finally judged are typically unclear. Human judges must ultimately decide, but the lack of agreement among human judges is well documented: Amsler and White (1979) indicate that while there is reasonable consistency in sense assignment for a given expert on successive sense assignments (84%), agreement is significantly lower among experts. Ahlswede (1995) reports between 63.3% and 90.2% agreement among judges on his Ambiguity Questionnaire; when faced with on-line sense assignment in a large corpus, agreement among judges is far less, and in some cases worse than chance (see also Ahlswede [1992, 19931, Ahlswede and Lorand [19931). Jorgensen (1990) found the level of agreement in her experiment using data from the Brown Corpus to be about 68%. The difficulty of comparing results in WSD research has recently become a concern within the community, and efforts are underway to develop strategies for evaluation of WSD. Gale, Church, and Yarowsky (1992b) attempt to establish lower and upper bounds for evaluating the performance of WSD systems; their proposal for overcoming the problem of agreement among human judges in order to establish an upper bound provides a starting point, but it has not been widely discussed or implemented. A recent discussion at a workshop sponsored by the ACL Special Interest Group on the Lexicon (SIGLEX) on &quot;Evaluating Automatic Semantic Taggers&quot; (Resnik and Yarowsky [1997a]; see also Resnik and Yarowsky [1997b], Kilgarriff [19971) has sparked the formation of an evaluation effort for WSD (SENSEVAL), in the spirit of previous evaluation efforts such as the ARPA-sponsored Message Understanding Conferences (e.g., ARPA [1993]), and Text Retrieval Conferences (e.g. Harman [1993, 1995]). SENSEVAL will see its first results at a subsequent SIGLEX workshop to be held at Herstmonceux Castle, England in September, 1998. As noted above, WSD is not an end in itself but rather an &quot;intermediate task&quot; that contributes to an overall task such as information retrieval or machine translation. This opens the possibility of two types of evaluation for WSD work (using terminology borrowed from biology): in vitro evaluation, where WSD systems are tested independent of a given application, using specially constructed benchmarks; and evaluation in vivo, where, rather than being evaluated in isolation, results are evaluated in terms of their contribution to the overall performance of a system designed for a particular application, such as machine translation. 3.3.1 Evaluation In Vitro. In vitro evaluation, despite its artificiality, enables close examination of the problems plaguing a given task. In its most basic form, this type of evaluation (also called variously performance evaluation: Hirschman and Thompson [1996]; assessment: Bimbot, Chollet, and Paoloni [1994]; or declarative evaluation: Arnold, Sadler, and Humphreys [1993]) involves comparison of the output of a system for a given input, using measures such as precision and recall. SENSEVAL currently envisages this type of evaluation for WSD results. Alternatively, in vitro evaluation can focus on study of the behavior and performance of systems on a series of test suites representing the range of linguistic problems likely to arise in attempting WSD (diagnostic evaluation: Hirschman and Thompson [1996]; or typological evaluation: Arnold, Sadler, and Humphreys 1993). Considerably deeper understanding of the factors involved in the disambiguation task is required before appropriate test suites for typological evaluation of WSD results can be devised. Basic questions such as the role of part of speech in WSD, the treatment of metaphor, metonymy, and the like in evaluation, and how to deal with words of differing degrees and types of polysemy, must first be resolved. SENSEVAL will likely take us a step closer to this understanding; at the least, it will force consideration of what can be meaningfully regarded as an isolatable sense distinction and provide some measure of the distance between the performance of current systems and a predefined standard. The in vitro evaluation envisaged for SENSEVAL demands the creation of a manually sense-tagged reference corpus containing an agreed-upon set of sense distinctions. The difficulties of attaining sense agreement, even among experts, have already been outlined. Resnik and Yarowsky (1997b) have proposed that for WSD evaluation, it may be practical to retain only those sense distinctions that are lexicalized crosslinguistically. This proposal has the merit of being immediately usable, but in view of the types of problems cited in the previous section, systematic study of interlanguage relations will be required to determine its viability and generality. At present, the apparent best source of sense distinctions is assumed to be on-line resources such as LDOCE or WordNet, although the problems of utilizing such resources are well known, and their use does not address issues of more complex semantic tagging that goes beyond the typical distinctions made in dictionaries and thesauri. Resnik and Yarowsky (1997b) also point out that a binary evaluation (correct/incorrect) for WSD is not sufficient, and propose that errors be penalized according to a distance matrix among senses based on a hierarchical organization. For example, failure to identify homographs of bank (which would appear higher in the hierarchy) would be penalized more severely than failure to distinguish bank as an institution from bank as a building (which would appear lower in the hierarchy). However, despite the obvious appeal of this approach, it runs up against the same problem of the lack of an established, agreed-upon hierarchy of senses. Aware of this problem, Resnik and Yarowsky suggest creating the sense distance matrix based on results in experimental psychology such as Miller and Charles (1991) or Resnik (1995b). Even ignoring the cost of creating such a matrix, the psycholinguistic literature has made clear that these results are highly influenced by experimental conditions and the task imposed on the subjects (see, for example, Tabossi [1989, 1991], Rayner and Morris [1991]); in addition, it is not clear that psycholinguistic data can be of help in WSD aimed toward practical use in NLP systems. In general, WSD evaluation confronts difficulties of criteria that are similar to, but orders of magnitude greater than, those facing other tasks such as part-of-speech tagging, due to the elusive nature of semantic distinctions. It may be that at best we can hope to find practical solutions that will serve particular needs; this is considered more fully in the next section. 3.3.2 Evaluation In Vivo. Another approach to evaluation is to consider results insofar as they contribute to the overall performance in a particular application, such as machine translation, information retrieval, or speech recognition. This approach (also called adequacy evaluation: Hirschman and Thompson [1996]; or operational evaluation: Arnold, Sadler, and Humphreys [1993]), although it does not assure the general applicability of a method nor contribute to a detailed understanding of problems, does not demand agreement on sense distinctions or the establishment of a pretagged corpus. Only the final result is taken into consideration, subjected to evaluation appropriate to the task at hand. Methods for WSD have evolved largely independently of particular applications, especially in the recent past. It is interesting to note that few, if any, systems for machine translation have incorporated recent methods developed for WSD, despite the importance of WSD for MT noted by Weaver almost 50 years ago. The most obvious effort to incorporate WSD methods into larger applications is in the field of information retrieval, and the results are ambiguous: Krovetz and Croft (1992) report only a slight improvement in retrieval using WSD methods; Voorhees (1993) and Sanderson (1994) indicate that retrieval degrades if disambiguation is not sufficiently precise. SparckJones (forthcoming) questions the utility of any NLP technique for document retrieval. On the other hand, Schtitze and Pedersen (1995) show a marked improvement in retrieval (14.4%) using a method that combines search-by-word and search-by-sense. It remains to be seen to what extent WSD can improve results in particular applications. However, if meaning is largely a function of use, it may be that the only relevant evaluation of WSD results is achievable in the context of specific tasks. Work on automatic WSD has a history as long as automated language processing generally. Looking back, it is striking to note that most of the problems and the basic approaches to solving them were recognized at the outset. Since so much of the early work on WSD is reported in relatively obscure books and articles across several fields and disciplines, it is not surprising that recent authors are often unaware of it. What is surprising is that in the broad sense, relatively little progress seems to have been made in nearly 50 years. Even though much recent work cites results at the 90% level or better, these studies typically involve very few words, most often only nouns, and frequently concern only broad sense distinctions. In a sense, WSD work has come full circle, returning most recently to empirical methods and corpus-based analyses that characterize some of the earliest attempts to solve the problem. With sufficiently greater resources and enhanced statistical methods at their disposal, researchers in the 1990s have obviously improved on earlier results, but it appears that we may nearly have reached the limit of what can be achieved in the current framework. For this reason, it is especially timely to assess the state of WSD and consider, in the context of its entire history, the next directions of research. This paper is an attempt to provide that context, at least in part, by bringing WSD into the perspective of the past 50 years of work on the topic. While we are aware that much more could be added to what is presented here, we have made an attempt to cover at least the major areas of work and sketch the broad lines of development in the field.&quot; Of course, WSD is problematic in part because of the inherent difficulty of determining or even defining word sense, and this is not an issue that is likely to be solved in the near future. Nonetheless, it seems clear that current WSD research could benefit from a more comprehensive consideration of theories of meaning and work in the area of lexical semantics. One of the obvious stumbling blocks in much recent WSD work is the rather narrow view of sense that comes hand-in-hand with the attempt to use sense distinctions in everyday dictionaries, which cannot, and are not intended to, represent meaning in context. A different sort of view, one more consistent with current linguistic theory, is required; here, we see the recent work using generative lexicons as providing at least a point of departure. Another goal of this paper is to provide a starting point for the growing number of researchers working in various areas of computational linguistics who want to learn about WSD. There is renewed interest in WSD as it contributes to various applications, such as machine translation and document retrieval. WSD as &quot;intermediate task,&quot; while interesting in its own right, is difficult and perhaps ultimately impossible to assess in the abstract; incorporation of WSD methods into larger applications will therefore hopefully inform and enhance future work. Finally, if a lesson is to be learned from a review of the history of WSD, it is that research can be very myopic and, as a result, tends to revisit many of the same issues over time. This is especially true when work on a problem has been cross-disciplinary. There is some movement toward more merging of research from various areas, at least as far as language processing is con cerned, spurred by the practical problems of information access that we are facing as a result of rapid technological development. Hopefully, this will contribute to further progress on WSD.","Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art
We present a very concise survey of the history of ideas used in word sense disambiguation.
In general, the various WSD approaches of the past can be divided into two types, i.e., data and knowledge-based approaches.
We argue that word sense ambiguity is a central problem for many established HLT applications (for example Machine Translation, Information Extraction and Information Retrieval).
","The need for sense disambiguation in such analysis, in order to include only those instances of a word in its proper sense, has long been recognized (see, for instance, Stone et al. [1966], Stone [1969], Kelly and Stone [1975]; for a more recent discussion see Litowski [1997]).In general terms, word sense disammiguation involves the association of a given word in a text or discourse with a definition or meaning (sense) which is distinguishable from other meanings (like much other similar activity in the field of computational linguistics, to the availability of large amounts of machine-readable text and the corresponding development of statistical methods to identify and apply information about regularities in",0.8228838443756104,0.8372782468795776,0.8300186395645142
"Providing A Unified Account Of Definite Noun Phrases In Discourse discourse utterances that combine into of the discourse, namely, units discourse that are typically larger than a single sentence, but smaller than the complete discourse. However, the constituent structure is not determined solely by the linear sequence of utterances. It is common for two contiguous utterances to be members of different subconstituents of the discourse (as with breaks between phrases in the syntactic analysis of a sentence); likewise, it is common for two utterances that are not contiguous to be members of the same subconstituent. An individual subconstituent of a discourse exhibits both internal coherence and coherence with the other subconstituents. That is, discourses have been shown to of coherence. coherence to the ways in which the larger segments of discourse relate to one another. It depends on such things as the function of a discourse, its subject matter, and rhetorical schema 1977, 1981; Reichman, 19811. coherence refers to the ways in which individual sentences bind together to form larger discourse segments. It depends on such things as the syntactic structure of an utterance, and the use of referring expressions 1Sidner, 19811. The two levels of discourse coherence correspond to two of focusing—global centering. Participants are said to be globally focused on a set of entities relevant to the overall discourse. These entities may either have been explicitly introduced into the discourse or be sufficiently closely related to such entities to be considered implicitly in focus [Grosz, 19811. In contrast. centering refers to a more local focusing process, one relates to identifying the single entity that an individual utterance most centrally concerns (Sidner, 1979; Joshi and Weinstein, 19811. 44 The two levels of focusing/coherence have different effects on the processing of pronominal and nonpronominal definite noun phrases. Global coherence and focusing are major factors in the generation and interpretation of nonpronominal definite referring expressions.- Local coherence and centering have greater effect on the processing of pronominal expressions. In Section 5 we shall describe the rules governing the use of these kinds of expressions and shall explain why additional processing by the hearer (needed for drawing additional inferences) is involved when pronominal expressions are used to refer to globally focused entities or nonpronominal expressions are used to refer to centered entities. Many approaches to language interpretation have ignored these differences, depending instead on powerful inference mechanisms to identify the referents of referring expressions. Although such approaches may suffice, especially for well-formed texts, they are insufficient in general. In particular, such approaches will not work for generation. Here the relationships among focusing, coherence, and referring expressions are essential and must be explicitly provided for. Theories—and systems based on them--will generate unacceptable uses of referring expressions if they do not take these into 3. Centering and Anaphora theory, the centers of a sentence in a discourse serve to integrate that sentence into the discourse. Each S, has a single center, a set of centers, Cb(S) serves to link S to the preceding discourse, while Cf(S) provides a set of entities to which the succeeding discourse may be linked. To avoid confusion, the phrase 'the center' will he used to refer only to Cb(S). To clarify the notion of center, we will consider a number of discourses illustrating the various factors that combined in (abstractly) and in its identification in a discourse. In Section 5 we define center more precisely, show how it relates to Sidner's [19811 immediate focus and potential foci, and discuss how the linkages established by the centers of a sentence help to determine the degree of intelligibility of a discourse. We begin by showing that the center cannot be defined in syntactic terms alone. The interaction of semantics and centering is more complex and is discussed in Section 4. The following examples, drawn from Reinhart [19821, illustrate the point that the notion of center is not i.e., the syntax of a sentence S not determine which of its NPs (The differ in other respects also. Reichman [19811 and Grosz 110811 discuss some of these. attempts to incorporate focusing mechanisms in generation systems are described in [Appelt, 1981 and McKeown, 19821. can obviously affect the interpretation; for of this paper, it may be regarded as part of a for the use of this terminology discussed (la) Who did Max see yesterday? (lb) Max saw Rosa. (2a) Did anyone see Rosa yesterday? (2b) Max saw Rosa. (lb) and (2b) are identical, Cb(lb) Max and Cb(2b) is Rosa. This can be seen in part by noticing that saw Rosa' seems more natural than (Ib) *Max saw her' than (2b) (a fact consistent with the centering rule introduced in Section 5.) The subject NP is the center in one context, the object NP in the other. when the NP used to realize Cb(S) syntactically determined, the Cb(S) itself is not yet fully determined, for Cb(S) is typically not a linguistic entity (i.e., it is not a particular linguistic expression). Rosa, not 'Rosa' is the Cb(2b). Consider. the discourse: (3a) How is Rosa? (3b) Did anyone see her yesterday? saw her. Here, Cb(3c) is Rosa, but clearly would not be in other contexts where the expression 'her' still realized the backward-looking center of 'Max saw her.' This is seen most simply by considering the discourse that would result if &quot;How is Joan?' replaced (3a). In the discourse that resulted, Joan, not Rosa, would be the center of (3c). 4. Centering and Realization The interactions of semantic and pragmatic factors with centering and their effects on referring expressions are more complex than the preceding discussion suggests. In the examples given above, the NPs that realize Cb(S) also denote it, but this is not always the case: we used the term 'realize' in the above discussion advisedly. In this section, we consider two kinds of examples in which the center of a sentence is not simply the denotation of some noun phrase occurring in the sentence. First, we will examine several examples in which the choice of and interaction among different kinds of interpretations of definite noun phrases are affected by the local discourse context (i.e., centering). Second, the role of pragmatic factors in some problematic cases of referential uses of definite descriptions [Donnellan 19661 is discussed. 4.1. Realization and Value-Free and Value-Loaded Interpretations distinction between semantic denotation is necessary to treat the interaction between value-free and value-loaded interpretations [Barwise and Perry, 19821 of definite descriptions, as they occur in extended discourse. Consider, for example, the following sequence: (4a) The vice president of the United States is also president of the Senate. (4b) Historically, he is the president's key man in negotiations with Congress. to China, he handled tricky negotiations, so he prepared for this Cb(4b) and Cb(4b') are each realized by the anaphoric element 'he.' But (4b) expresses the same thing as 'Historically, the vice president of the United States is the president's key man in negotiations with Congress' (in which it is clear that no single individual vice president is being referred to) whereas (4b1 expresses the same thing as, 'As ambassador to China, the [person who is now] vice president of the United States handled many tricky negotiations,...' This can be accounted for by observing that 'the vice president of the United States' contributes both its value-free interpretation and its value-loading at the world type to Cf(4a). Cb(4b) is then the value-free interpretation and Cb(4b') is the valueloading, i.e., George Bush. In this example, both value-free and value-loaded interpretations are shown to stern from the same full definite noun phrase. It is also possible for the movement of the center from a value-free interpretation (for Cb(S)) to a value-loaded interpretation (for Cb of the next sentence)--or vice versa—to be accomplished solely with pronouns. That is, although (4b)-(4b1 is (at least for some readers) not a natural dialogue, similar sequences are possible. There appear to be strong constraints on the kinds of transitions that are allowed. In particular, if a given sentence forces either the value-free or value-loaded interpretation, then only that interpretation becomes possible in a subsequent sentence. However, if some sentence in a given context merely prefers one interpretation while allowing the other, then either one is possible in a subsequent sentence. For example, the sequence (6a) The vice president of the United States also president Senate. the president's key aan in negotiations vith Congress. in which 'he' may be interpreted as either value-free or (VL), may be followed by either of following As to China, he zany tricky negotiations. is required to be at least old. However, if we change (5b) to force the value-loaded interpretation, as in (5b&quot;), then only (5c) is possible. (Sb') Right now he is the president's key man in negotiations with Congress. Similarly, if (5b) is changed to force the value-free interpretation, as in (4b), then only (5c') is possible. If an intermediate sentence allows both interpretations but prefers one in a given context, then either is possible in the third sentence. A use with preference for a valueloaded interpretation followed by a use indicating the value-free interpretation is illustrated in the sequence: John thinks that the telephone is a toy. it every day. (11 preferred; ok) He doesn't realize that it is an invention changed the world. preference a value-free interpretation that is by value-loaded one is easiest to see in a dialogue situation; vice president of the United States is also president of the Senate. I he played some role in the House. (VF preferred; VL did, but that was before he was Realization and Use these examples, might appear that the concepts of value-free and value-loaded interpretation are identical to DonneIlan's 119661 attributive and referential uses of noun phrases. However, there is an important difference between these two distinctions. The importance to our theory is that the referential use of definite noun phrases introduces the need to take pragmatic factors (in particular speaker intention) into account, not just semantic factors. Donnellan [1966] describes the referential and uses of descriptions in the following way: 'A speaker who uses a definite description attributively in an assertion states something whoever or whatever is the so-and-so. speaker who uses a definite description referentially in an assertion, on the other hand, uses the description to enable his audience to pick out whom or what he is talking about and states something about that person or thing. In the first case the definite description might be said to occur essentially, for the speaker wishes to assert something about whatever or whoever fits that description; but in the referential use the definite description is merely one tool for doing a certain job--calling attention to a person or thing--and in general any other device for doing the same job, another description or a name, would do as well. In the attributive use, the attribute of being the so-and-so is all important, while it is not in the referential use.' The distinction Donnellan suggests can be formulated in terms of the different propositions a sentence S containing a definite description D may be used to express on different occasions of use. When D is used referentially, it contributes its denotation to the proposition expressed by 46 S; when it is used attributively, it contributes to the proposition expressed by S a semantic interpretation related to the descriptive content of D. The identity of this semantic interpretation is not something about which Donnellan is explicit. Distinct formal treatments of the semantics of definite descriptions in natural language would construe the appropriate interpretation differently. In semantic treatments based on possible worlds, the appropriate interpretation would be a (partial) function from possible worlds to objects; in the situation semantics expounded by Barwise and Perry, the appropriate interpretation is a (partial) function from resource to objects. As just described, the referential-attributive distinction appears to be exactly the distinction that Barwise and Perry formulate in terms of the value-loaded and valuefree interpretations of definite noun phrases. But this gloss omits an essential aspect of the referentialattributive distinction as elaborated by Donnellan. In view, a speaker may use referentially to refer to an object distinct from the semantic denotation of the description, and, moreover, to refer to an object even when the description has no semantic denotation. In one sense, this phenomenon arises within the framework of Barwise and Perry's treatment of descriptions. If we understand the semantic denotation of a description to be the unique object that satisfies the content of the description, if there is one, then Barwise and Perry would allow that there are referential uses of a description D that contribute objects other than the semantic denotation of D to the propositions expressed by uses of sentences in which D occurs. But this is only because Barwise and Perry allow that a description may be evaluated at a resource situation other than the complete situation in order to arrive at its denotation on a given occasion of use. Still, the denotation of the description relative to a given resource situation is the unique object in the situation that satisfies the description relative to that situation. The referential uses of descriptions that Donnellan gives of do seem to arise by evaluation of descriptions at alternative resource situations, but rather through the *referential intentions' of the speaker in his of the description. aspect of referential use is a rather a semantic phenomenon and is best analyzed in terms of the distinction between semantic reference and speaker's reference elaborated in Kripke [10771. the following discourses from Kripke [10771: 'any situation on which the speaker can focus attention is a potential candidate for a resource situation with to which the speaker may value load uses of descriptions. Such resource situations must contain a unique object satisfies description. husband is kind to her. NO. isn't. The can you're referring to isn't her husband. Her husband to He her isn't her husband. With (6a) and (7a), Kripke has in mind a case like the one discussed in Donnellan [19661, in which a speaker uses a description to refer to something other than the referent of that description, the unique thing that satisfies the description (if there is one). Kripke analyzes this case as an instance of the general phenomenon of a clash of intentions in language use. In the case at hand, the speaker has a general intention to use the description to refer to its semantic referent; his intention, distinct from general semantic intention, is to use it to refer to a particular individual. He incorrectly believes that these two intentions coincide this gives rise to a use of referring expression in which the speaker's reference reference are (The speaker's referent is presumably the woman's lover). From our point of view, the importance of the case resides in its showing that Cf(S) may include more than one entity, that is realized by a single NP in S. In this case, 'her husband' contributes both the husband and the lover to Cf(6a) and Cf(7a). This can be seen by observing that both discourses seem equally appropriate and that the backward-looking centers of (6b) and (7b) are the husband and the lover, respectively, realized by their anaphoric elements. Hence, the forward-looking centers of a sentence may be related not semantically but to the that realize Hence, the importance of the referential/attributive distinction from our point of view is that it leads to cases in which the centers of a sentence may be pragmatically rather than semantically related to the noun phrases that realize them. 5. Center Movement and Center Realization-- Constraints En the foregoing sections we have discussed a number of examples to illustrate two essential points. First, the noun phrase that realizes the backward-looking center of an utterance in a discourse cannot be determined from the of the utterance alone. Second, the relation c noun phrases centers solely a semantic a pragmatic relation. This discussion has proceeded at a rather intuitive level, without explicit elaboration of the framework we regard as appropriate for dealing with centering and its role in explaining discourse phenomena. Before going on to describe constraints on the realization relation that are, of course, several alternative explanations; e.g., the may believe that the description is more likely than to be interpreted correctly by the hearer. Ferreting out the in a given situation requires of belief and the like. A discussion of issues is beyond the this paper. 67 explain certain phenomena in discourse, we should be somewhat more explicit about the notions of center and realization. We have said that each utterance S in a discourse has associated with it a backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S). What manner of objects are these centers? They are the sort of objects that can serve as the semantic interpretations of singular That is, either they are objects in the world (e.g., planets, people, numbers) or they are functions from possible worlds (situations, etc.) to objects in the world that can be used to interpret definite descriptions. That is, whatever serves to interpret a definite noun phrase can be a center. For the sake of concreteness in many of the examples in the preceding discussion, we have relied on the situation semantics of Barwise and Perry. The theory we are developing does not depend on this particular semantical treatment of definite noun phrases, but it does require several of the distinctions that treatment provides. In particular, our theory requires a semantical treatment that accommodates the distinction between interpretations of definite noun phrases that contribute their content to the propositions expressed by sentences in which they occur and interpretations that contribute only their denotation—in other words, the distinction between value-free and value-loaded interpretations. As noted, a distinction of this sort can be effected within the framework of 'possible-worlds' approaches to the semantics of natural language. In addition, we see the need for interpretations of definite noun phrases to be dependent on their discourse context. Once again, this is a feature of interpretations that is accommodated in the relational approach to semantics advocated by Barwise and Perry, but it might be accommodated within other as Given that Cb(S), the center of sentence S in a discourse, is the interpretation of a definite noun phrase, how does it become related to S? In a typical example, S will contain a full definite noun phrase or pronoun that realizes the center. The realization relation is neither nor pragmatic. For example, realizes c in cases where a definite description and is interpretation, or an object related to it by a 'speaker's reference.' More when is pronoun, the principles that which c are such that realizes c from neither semantics nor pragmatics exclusively. They are principles that must be elicited from the study of itself. A tentative formulation of some principles is given below. it is typical that, when a center of S, S an that realizes c, is by no means necessary. In particular, for sentences containing noun treatment of our theory we will consider centers that are realized by constituents in other syntactic categories. 119831 discusses some of these issues and compares several of with Montague semantics. phrases that express functional relations (e.g., 'the door,' 'the owner') whose arguments are not exhibited explicitly (e.g., a house is the current center, but so far its door nor its owner has been it is the case that such argument can be backward-looking center of the sentence. We are studying such and expect to integrate that into our theory of discourse The basic rule that constrains the realization of the backward-looking center of an utterance is a constraint on the speaker, namely: the Cb the current utterance is the same as the of the previous utterance, a pronoun should be are two things to about this rule. First, it not preclude using for other entities as long as one is used for the center. Second, it is not a hard but rather principle, like a Gricean maxim, that violated. However, such violations lead at best to in which the is forced to draw additional inferences. simple example, consider the following sequence, assuming at the outset that John is the center of the discourse: (8a) He called up Mike yesterday. (he=John) (8b) He vas annoyed by John's call. Linguistic theories typically assign various linguistic phenomena to one of the categories, syntactic, semantic, or pragmatic, as if the phenomena in each category were relatively independent of those in the others. However, various phenomena in discourse do not seem to yield comfortably to any account that is strictly a syntactic or semantic or pragmatic one. This paper focuses on particular phenomena of this sort—the use of various referring expressions such as definite noun phrases and pronouns—and examines their interaction with mechanisms used to maintain discourse coherence. Even a casual survey of the literature on definite descriptions and referring expressions reveals not only defects in the individual accounts provided by theorists (from several different disciplines), but also deep confusions about the roles that syntactic, semantic, and pragmatic factors play in accounting for these phenomena. The research we have undertaken is an attempt to sort out some of these confusions and to create the basis for a theoretical framework that can account for a variety of discourse phenomena in which all three factors of language use interact. The major premise on which our research depends is that the concepts necessary for an adequate understanding of the phenomena in question are not exclusively either syntactic or semantic or pragmatic. The next section of this paper defines two levels of discourse coherence and describes their roles in accounting for the use of singular definite noun phrases. To illustrate the integration of factors in explaining the uses of referring expressions, their use on one of these levels, i.e., the local one, is discussed in Sections 3 and 4. This account requires introducing the notion of the centers of a sentence in a discourse, a notion that cannot be defined in terms of factors that are exclusively syntactic or semantic or pragmatic. In Section 5, the interactions of the two levels with these factors and their effects on the uses of referring expressions in discourse are discussed. A discourse comprises utterances that combine into subconstituents of the discourse, namely, units of discourse that are typically larger than a single sentence, but smaller than the complete discourse. However, the constituent structure is not determined solely by the linear sequence of utterances. It is common for two contiguous utterances to be members of different subconstituents of the discourse (as with breaks between phrases in the syntactic analysis of a sentence); likewise, it is common for two utterances that are not contiguous to be members of the same subconstituent. An individual subconstituent of a discourse exhibits both internal coherence and coherence with the other subconstituents. That is, discourses have been shown to have two levels of coherence. Global coherence refers to the ways in which the larger segments of discourse relate to one another. It depends on such things as the function of a discourse, its subject matter, and rhetorical schema (Grosz, 1977, 1981; Reichman, 19811. Local coherence refers to the ways in which individual sentences bind together to form larger discourse segments. It depends on such things as the syntactic structure of an utterance, ellipsis, and the use of pronominal referring expressions 1Sidner, 19811. The two levels of discourse coherence correspond to two levels of focusing—global focusing and centering. Participants are said to be globally focused on a set of entities relevant to the overall discourse. These entities may either have been explicitly introduced into the discourse or be sufficiently closely related to such entities to be considered implicitly in focus [Grosz, 19811. In contrast. centering refers to a more local focusing process, one relates to identifying the single entity that an individual utterance most centrally concerns (Sidner, 1979; Joshi and Weinstein, 19811. The two levels of focusing/coherence have different effects on the processing of pronominal and nonpronominal definite noun phrases. Global coherence and focusing are major factors in the generation and interpretation of nonpronominal definite referring expressions.- Local coherence and centering have greater effect on the processing of pronominal expressions. In Section 5 we shall describe the rules governing the use of these kinds of expressions and shall explain why additional processing by the hearer (needed for drawing additional inferences) is involved when pronominal expressions are used to refer to globally focused entities or nonpronominal expressions are used to refer to centered entities. Many approaches to language interpretation have ignored these differences, depending instead on powerful inference mechanisms to identify the referents of referring expressions. Although such approaches may suffice, especially for well-formed texts, they are insufficient in general. In particular, such approaches will not work for generation. Here the relationships among focusing, coherence, and referring expressions are essential and must be explicitly provided for. Theories—and systems based on them--will generate unacceptable uses of referring expressions if they do not take these relationships into account.3 In our theory, the centers of a sentence in a discourse serve to integrate that sentence into the discourse. Each sentence, S, has a single backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S). Cb(S) serves to link S to the preceding discourse, while Cf(S) provides a set of entities to which the succeeding discourse may be linked. To avoid confusion, the phrase 'the center' will he used to refer only to Cb(S). To clarify the notion of center, we will consider a number of discourses illustrating the various factors that are combined in its definition (abstractly) and in its identification in a discourse. In Section 5 we define center more precisely, show how it relates to Sidner's [19811 immediate focus and potential foci, and discuss how the linkages established by the centers of a sentence help to determine the degree of intelligibility of a discourse. We begin by showing that the center cannot be defined in syntactic terms alone. The interaction of semantics and centering is more complex and is discussed in Section 4. The following examples, drawn from Reinhart [19821, illustrate the point that the notion of center is not syntactically definable,4 i.e., the syntax of a sentence S does not determine which of its NPs realizes Cb(S). (The reasons for the use of this terminology are discussed in Section 4.) Although (lb) and (2b) are identical, Cb(lb) is Max and Cb(2b) is Rosa. This can be seen in part by noticing that 'He saw Rosa' seems more natural than (Ib) and *Max saw her' than (2b) (a fact consistent with the centering rule introduced in Section 5.) The subject NP is the center in one context, the object NP in the other. Even when the NP used to realize Cb(S) can be syntactically determined, the Cb(S) itself is not yet fully determined, for Cb(S) is typically not a linguistic entity (i.e., it is not a particular linguistic expression). Rosa, not 'Rosa' is the Cb(2b). Consider. the discourse: Here, Cb(3c) is Rosa, but clearly would not be in other contexts where the expression 'her' still realized the backward-looking center of 'Max saw her.' This is seen most simply by considering the discourse that would result if &quot;How is Joan?' replaced (3a). In the discourse that resulted, Joan, not Rosa, would be the center of (3c). The interactions of semantic and pragmatic factors with centering and their effects on referring expressions are more complex than the preceding discussion suggests. In the examples given above, the NPs that realize Cb(S) also denote it, but this is not always the case: we used the term 'realize' in the above discussion advisedly. In this section, we consider two kinds of examples in which the center of a sentence is not simply the denotation of some noun phrase occurring in the sentence. First, we will examine several examples in which the choice of and interaction among different kinds of interpretations of definite noun phrases are affected by the local discourse context (i.e., centering). Second, the role of pragmatic factors in some problematic cases of referential uses of definite descriptions [Donnellan 19661 is discussed. The distinction between realization and semantic denotation is necessary to treat the interaction between value-free and value-loaded interpretations [Barwise and Perry, 19821 of definite descriptions, as they occur in extended discourse. Consider, for example, the following sequence: (4a) The vice president of the United States is also president of the Senate. (4b) Historically, he is the president's key man in negotiations with Congress. (4b') As Ambassador to China, he handled aany tricky negotiations, so he is well prepared for this job. Cb(4b) and Cb(4b') are each realized by the anaphoric element 'he.' But (4b) expresses the same thing as 'Historically, the vice president of the United States is the president's key man in negotiations with Congress' (in which it is clear that no single individual vice president is being referred to) whereas (4b1 expresses the same thing as, 'As ambassador to China, the [person who is now] vice president of the United States handled many tricky negotiations,...' This can be accounted for by observing that 'the vice president of the United States' contributes both its value-free interpretation and its value-loading at the world type to Cf(4a). Cb(4b) is then the value-free interpretation and Cb(4b') is the valueloading, i.e., George Bush. In this example, both value-free and value-loaded interpretations are shown to stern from the same full definite noun phrase. It is also possible for the movement of the center from a value-free interpretation (for Cb(S)) to a value-loaded interpretation (for Cb of the next sentence)--or vice versa—to be accomplished solely with pronouns. That is, although (4b)-(4b1 is (at least for some readers) not a natural dialogue, similar sequences are possible. There appear to be strong constraints on the kinds of transitions that are allowed. In particular, if a given sentence forces either the value-free or value-loaded interpretation, then only that interpretation becomes possible in a subsequent sentence. However, if some sentence in a given context merely prefers one interpretation while allowing the other, then either one is possible in a subsequent sentence. (6a) The vice president of the United States is also president of the Senate. (5b) He's the president's key aan in negotiations vith Congress. in which 'he' may be interpreted as either value-free (IT) or value-loaded (VL), may be followed by either of the following two sentences: (Sc) As ambassador to China, he handled zany tricky negotiations. (VL) (Sc') He is required to be at least 36 years old. (VF) However, if we change (5b) to force the value-loaded interpretation, as in (5b&quot;), then only (5c) is possible. (Sb') Right now he is the president's key man in negotiations with Congress. Similarly, if (5b) is changed to force the value-free interpretation, as in (4b), then only (5c') is possible. If an intermediate sentence allows both interpretations but prefers one in a given context, then either is possible in the third sentence. A use with preference for a valueloaded interpretation followed by a use indicating the value-free interpretation is illustrated in the sequence: John thinks that the telephone is a toy. He plays with it every day. (11 preferred; VT. ok) He doesn't realize that it is an invention that changed the world. (VF) The preference for a value-free interpretation that is followed by a value-loaded one is easiest to see in a dialogue situation; sl. The vice president of the United States is also president of the Senate. s2: I thought he played some important role in the House. (VF preferred; VL ok) SI: He did, but that was before he was VP. (W.) From these examples, it might appear that the concepts of value-free and value-loaded interpretation are identical to DonneIlan's 119661 attributive and referential uses of noun phrases. However, there is an important difference between these two distinctions. The importance to our theory is that the referential use of definite noun phrases introduces the need to take pragmatic factors (in particular speaker intention) into account, not just semantic factors. Donnellan [1966] describes the referential and attributive uses of definite descriptions in the following way: 'A speaker who uses a definite description attributively in an assertion states something about whoever or whatever is the so-and-so. A speaker who uses a definite description referentially in an assertion, on the other hand, uses the description to enable his audience to pick out whom or what he is talking about and states something about that person or thing. In the first case the definite description might be said to occur essentially, for the speaker wishes to assert something about whatever or whoever fits that description; but in the referential use the definite description is merely one tool for doing a certain job--calling attention to a person or thing--and in general any other device for doing the same job, another description or a name, would do as well. In the attributive use, the attribute of being the so-and-so is all important, while it is not in the referential use.' The distinction Donnellan suggests can be formulated in terms of the different propositions a sentence S containing a definite description D may be used to express on different occasions of use. When D is used referentially, it contributes its denotation to the proposition expressed by S; when it is used attributively, it contributes to the proposition expressed by S a semantic interpretation related to the descriptive content of D. The identity of this semantic interpretation is not something about which Donnellan is explicit. Distinct formal treatments of the semantics of definite descriptions in natural language would construe the appropriate interpretation differently. In semantic treatments based on possible worlds, the appropriate interpretation would be a (partial) function from possible worlds to objects; in the situation semantics expounded by Barwise and Perry, the appropriate interpretation is a (partial) function from resource situations5 to objects. As just described, the referential-attributive distinction appears to be exactly the distinction that Barwise and Perry formulate in terms of the value-loaded and valuefree interpretations of definite noun phrases. But this gloss omits an essential aspect of the referentialattributive distinction as elaborated by Donnellan. In DonneIlan's view, a speaker may use a description referentially to refer to an object distinct from the semantic denotation of the description, and, moreover, to refer to an object even when the description has no semantic denotation. In one sense, this phenomenon arises within the framework of Barwise and Perry's treatment of descriptions. If we understand the semantic denotation of a description to be the unique object that satisfies the content of the description, if there is one, then Barwise and Perry would allow that there are referential uses of a description D that contribute objects other than the semantic denotation of D to the propositions expressed by uses of sentences in which D occurs. But this is only because Barwise and Perry allow that a description may be evaluated at a resource situation other than the complete situation in order to arrive at its denotation on a given occasion of use. Still, the denotation of the description relative to a given resource situation is the unique object in the situation that satisfies the description relative to that situation. The referential uses of descriptions that Donnellan gives examples of do not seem to arise by evaluation of descriptions at alternative resource situations, but rather through the *referential intentions' of the speaker in his use of the description. This aspect of referential use is a pragmatic rather than a semantic phenomenon and is best analyzed in terms of the distinction between semantic reference and speaker's reference elaborated in Kripke [10771. Consider the following discourses drawn from Kripke [10771: 5Roughly, 'any situation on which the speaker can focus attention is a potential candidate for a resource situation with respect to which the speaker may value load his uses of definite descriptions. Such resource situations must contain a unique object which satisfies the description. (6a) Her husband is kind to her. (61)) NO. he isn't. The can you're referring to isn't her husband. (7a) Her husband is kind to her. (7b) He is kind to her but he isn't her husband. With (6a) and (7a), Kripke has in mind a case like the one discussed in Donnellan [19661, in which a speaker uses a description to refer to something other than the semantic referent of that description, i.e., the unique thing that satisfies the description (if there is one). Kripke analyzes this case as an instance of the general phenomenon of a clash of intentions in language use. In the case at hand, the speaker has a general intention to use the description to refer to its semantic referent; his specific intention, distinct from his general semantic intention, is to use it to refer to a particular individual. He incorrectly believes that these two intentions coincide and this gives rise to a use of the referring expression 'her husband&quot; in which the speaker's reference and the semantic reference are distinct.•6 (The speaker's referent is presumably the woman's lover). From our point of view, the importance of the case resides in its showing that Cf(S) may include more than one entity, that is realized by a single NP in S. In this case, 'her husband' contributes both the husband and the lover to Cf(6a) and Cf(7a). This can be seen by observing that both discourses seem equally appropriate and that the backward-looking centers of (6b) and (7b) are the husband and the lover, respectively, realized by their anaphoric elements. Hence, the forward-looking centers of a sentence may be related not semantically but pragmatically to the NPs that realize them. Hence, the importance of the referential/attributive distinction from our point of view is that it leads to cases in which the centers of a sentence may be pragmatically rather than semantically related to the noun phrases that realize them. En the foregoing sections we have discussed a number of examples to illustrate two essential points. First, the noun phrase that realizes the backward-looking center of an utterance in a discourse cannot be determined from the syntax of the utterance alone. Second, the relation N realizes c between noun phrases N and centers c is neither solely a semantic nor solely a pragmatic relation. This discussion has proceeded at a rather intuitive level, without explicit elaboration of the framework we regard as appropriate for dealing with centering and its role in explaining discourse phenomena. Before going on to describe constraints on the realization relation that explain certain phenomena in discourse, we should be somewhat more explicit about the notions of center and realization. We have said that each utterance S in a discourse has associated with it a backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S). What manner of objects are these centers? They are the sort of objects that can serve as the semantic interpretations of singular noun phrases.7 That is, either they are objects in the world (e.g., planets, people, numbers) or they are functions from possible worlds (situations, etc.) to objects in the world that can be used to interpret definite descriptions. That is, whatever serves to interpret a definite noun phrase can be a center. For the sake of concreteness in many of the examples in the preceding discussion, we have relied on the situation semantics of Barwise and Perry. The theory we are developing does not depend on this particular semantical treatment of definite noun phrases, but it does require several of the distinctions that treatment provides. In particular, our theory requires a semantical treatment that accommodates the distinction between interpretations of definite noun phrases that contribute their content to the propositions expressed by sentences in which they occur and interpretations that contribute only their denotation—in other words, the distinction between value-free and value-loaded interpretations. As noted, a distinction of this sort can be effected within the framework of 'possible-worlds' approaches to the semantics of natural language. In addition, we see the need for interpretations of definite noun phrases to be dependent on their discourse context. Once again, this is a feature of interpretations that is accommodated in the relational approach to semantics advocated by Barwise and Perry, but it might be accommodated within other approaches as wel1.8 Given that Cb(S), the center of sentence S in a discourse, is the interpretation of a definite noun phrase, how does it become related to S? In a typical example, S will contain a full definite noun phrase or pronoun that realizes the center. The realization relation is neither semantic nor pragmatic. For example, N realizes c may hold in cases where N is a definite description and e is its denotation, its value-free interpretation, or an object related to it by a 'speaker's reference.' More importantly, when N is a pronoun, the principles that govern which c are such that N realizes c derive from neither semantics nor pragmatics exclusively. They are principles that must be elicited from the study of discourse itself. A tentative formulation of some such principles is given below. Though it is typical that, when c is a center of S, S contains an N such that N realizes c, it is by no means necessary. In particular, for sentences containing noun In 3 hiller treatment of our theory we will consider centers that are realized by constituents in other syntactic categories. 8Israel 119831 discusses some of these issues and compares several properties of situation semantics with Montague semantics. phrases that express functional relations (e.g., 'the door,' 'the owner') whose arguments are not exhibited explicitly (e.g., a house is the current center, but so far neither its door nor its owner has been mentioned),9 it is sometimes the case that such an argument can be the backward-looking center of the sentence. We are currently studying such cases and expect to integrate that study into our theory of discourse phenomena. The basic rule that constrains the realization of the backward-looking center of an utterance is a constraint on the speaker, namely: If the Cb of the current utterance is the same as the Cb of the previous utterance, a pronoun should be used. There are two things to note about this rule. First, it does not preclude using pronouns for other entities as long as one is used for the center. Second, it is not a hard rule, but rather a principle, like a Gricean maxim, that can be violated. However, such violations lead at best to conditions in which the hearer is forced to draw additional inferences. As a simple example, consider the following sequence, assuming at the outset that John is the center of the discourse: (8a) He called up Mike yesterday. (he=John) (8b) He vas annoyed by John's call. (8b) is unacceptable, unless it is possible to consider the introduction of a second person named 'John.' However, intervening sentences that provide for a shift in center from John to Mike (e.g., 'He was studying for his driver's test') suffice to make (8b) completely acceptable. Sidner's discourse focus corresponds roughly to Cb(S), while her potential foci correspond approximately to Cf(S). However, she also introduces an actor focus to handle multiple pronouns in a single utterance. The basic centering rule not only allows us to handle the same examples more simply, but also appears to avoid one of the complications in Sidner's account. Example D4 from Sidner [1981) illustrates this problem: (9-1)I haven't seen Jeff for several days. (9-2)Carl thinks he's studying for his exams. (9-3)But I think he went to the Cape with Linda. On Sidner's account, Carl is the actor focus after (9-2) and Jeff is the discourse focus (Cb(9-2)). Because the actor focus is preferred as the referrent of pronominal expressions, Carl is the leading candidate for the entity referred to by he in (9-3). It is difficult to rule this case out without invoking fairly special rules. On our account, Jeff is Cb(9-2) and there is no problem. The addition of actor focus was made to handle multiple pronouns--for example, if (9-3) were replaced by The center rule allows such uses, without introducing a gGrosz 119771 refers to this as 'implicit focusing'; other examples are presented in Joshi and Weinstein 119811. second kind of focus (or center), by permitting entities other than Cb(S) to be pronominalized as long as Cb(S) is.io Two aspects of centering affect the kinds of inferences a hearer must draw in interpreting a definite description. First, the shifting of center from one entity to another requires recognition of this change. Most often such changes are affected by the use of full definite noun phrases, but in some instances a pronoun may be used. For example, Grosz [19771 presents several examples of pronouns being used to refer to objects mentioned many utterances back. Second, the hearer must process (interpret) the particular linguistic expression that realizes the center. Most previous attempts to account for the interaction of different kinds of referring expressions with centering and focusing (or 'topic') have conflated these two. For example, Joshi and Weinstein [19811 present a preliminary report on their research regarding the connection between the computational complexity of the inferences required to process a discourse and the coherence of that discourse as assessed by measures that invoke the centering phenomenon. However, several of the examples combine changes of expression and shifts in centering. Violations of the basic centering rule require the hearer to draw two different kinds of inferences. The kind required depends on whether a full definite noun phrase is used to express the center or whether a pronoun is used for a noncentered entity. We will consider each case separately. Several different functions may be served by the use of a full definite noun phrase to realize the currently centered entity. For instance, the full noun phrase may include some new and unshared information about the entity. In such cases, additional inferences arise from the need to determine that the center has not shifted and that the properties expressed hold for the centered entity. For example, in the following sequences the full definite noun phrases that are in boldface do more than merely refer. When the current center is not pronominalized (it may not be present in the sentence), the use of a pronoun to express an entity other than the current center, is strongly constrained. The particular cases that have been identified involve instances in which attention is being shifted back to a previously centered entity (e.g., Grosz, 1977; Reichman, 1978) or to one element of a set that is currently centered. In such cases, additional inferences 10Obviously, if Cb(S) is not expressed in the next sentence then this issue does not arise. are required to determine that the pronoun does not refer to the current center, as well as to identify the context back to which attention is shifting. These shifts, though indicated by linguistic expressions typically used for centering (pronouns), correspond to a shift in global focus. The main purpose of the paper was to sort out the confusion about the roles of syntactic, semantic, and pragmatic factors in the interpretation and generation of definite noun phrases in discourse. Specific mechanisms that account for the interactions among these factors were presented. Discourses were shown to be coherent at two different levels, i.e., with referring expressions used to identify entities that are centered locally and those focused upon more globally. The differences between references at the global and local levels were discussed, and the interaction of the syntactic role of a given noun phrase and its semantic interpretation with centering was described.","Providing A Unified Account Of Definite Noun Phrases In Discourse
Linguistic theories typically assign various linguistic phenomena to one of the categories, syntactic, semantic, or pragmatic, as if the phenomena in each category were relatively independent of those in the others.
However, various phenomena in discourse do not seem to yield comfortably to any account that is strictly a syntactic or semantic or pragmatic one.
This paper focuses on particular phenomena of this sort - the use of various referring expressions such as definite noun phrases and pronouns - and examines their interaction with mechanisms used to maintain discourse coherence.
Even a casual survey of the literature on definite descriptions and referring expressions reveals not only defects in the individual accounts provided by theorists (from several different disciplines), but also deep confusions about the roles that syntactic, semantic, and pragmatic factors play in accounting for these phenomena.
The research we have undertaken is an attempt to sort out some of these confusions and to create the basis for a theoretical framework that can account for a variety of discourse phenomena in which all three factors of language use interact.
The major premise on which our research depends is that the concepts necessary for an adequate understanding of the phenomena in question are not exclusively either syntactic or semantic or pragmatic.
The next section of this paper defines two levels of discourse coherence and describes their roles in accounting for the use of singular definite noun phrases.
To illustrate the integration of factors in explaining the uses of referring expressions, their use on one of these levels, i.e., the local one, is discussed in Sections 3 and 4.
This account requires introducing the notion of the centers of a sentence in a discourse, a notion that cannot be defined in terms of factors that are exclusively syntactic or semantic or pragmatic.
In Section 5, the interactions of the two levels with these factors and their effects on the uses of referring expressions in discourse are discussed.
To resolve referring expression, we develop centering theory.
","In Section 5 we define center more precisely, show how it relates to Sidner's [19811 immediate focus and potential foci, and discuss how the linkages established by the centers of a sentence help to determine the degree of intelligibility of a discourse.The two levels of focusing/coherence correspond to two of focusing—global centering.Each S, has a single center, a set of centers, Cb(S) serves to link S to the preceding discourse, while Cf(",0.8292841911315918,0.7999817728996277,0.8143694400787354
"Unsupervised Learning of Narrative Schemas and their Participants We describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury, Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles. This paper describes a new approach to event semantics that jointly learns event relations and their participants from unlabeled corpora. The early years of natural language processing (NLP) took a “top-down” approach to language understanding, using representations like scripts (Schank and Abelson, 1977) (structured representations of events, their causal relationships, and their participants) and frames to drive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to be learned. Even unsupervised attempts to learn semantic roles have required a pre-defined set of roles (Grenager and Manning, 2006) and often a hand-labeled seed corpus (Swier and Stevenson, 2004; He and Gildea, 2006). In this paper, we describe our attempts to learn script-like information about the world, including both event structures and the roles of their participants, but without pre-defined frames, roles, or tagged corpora. Consider the following Narrative Schema, to be defined more formally later. The events on the left follow a set of participants through a series of connected events that constitute a narrative: Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles. This paper addresses two areas of work in event semantics, narrative event chains and semantic role labeling. We begin by highlighting areas in both that can mutually inform each other through a narrative schema model. Narrative Event Chains are partially ordered sets of events that all involve the same shared participant, the protagonist (Chambers and Jurafsky, 2008). A chain contains a set of verbs representing events, and for each verb, the grammatical role filled by the shared protagonist. An event is a verb together with its constellation of arguments. An event slot is a tuple of an event and a particular argument slot (grammatical relation), represented as a pair (v, d) where v is a verb and d E {subject, object, prep}. A chain is a tuple (L, O) where L is a set of event slots and O is a partial (temporal) ordering. We will write event slots in shorthand as (X pleads) or (pleads X) for (pleads, subject) and (pleads, object). Below is an example chain modeling criminal prosecution. In this example, the protagonist of the chain is the person being prosecuted and the other unspecified event slots remain unfilled and unconstrained. Chains in the Chambers and Jurafsky (2008) model are ordered; in this paper rather than address the ordering task we focus on event and argument induction, leaving ordering as future work. The Chambers and Jurafsky (2008) model learns chains completely unsupervised, (albeit after parsing and resolving coreference in the text) by counting pairs of verbs that share coreferring arguments within documents and computing the pointwise mutual information (PMI) between these verb-argument pairs. The algorithm creates chains by clustering event slots using their PMI scores, and we showed this use of co-referring arguments improves event relatedness. Our previous work, however, has two major limitations. First, the model did not express any information about the protagonist, such as its type or role. Role information (such as knowing whether a filler is a location, a person, a particular class of people, or even an inanimate object) could crucially inform learning and inference. Second, the model only represents one participant (the protagonist). Representing the other entities involved in all event slots in the narrative could potentially provide valuable information. We discuss both of these extensions next. The Chambers and Jurafsky (2008) narrative chains do not specify what type of argument fills the role of protagonist. Chain learning and clustering is based only on the frequency with which two verbs share arguments, ignoring any features of the arguments themselves. Take this example of an actual chain from an article in our training data. Given this chain of five events, we want to choose other events most likely to occur in this scenario. One of the top scoring event slots is (fly X). Narrative chains incorrectly favor (fly X) because it is observed during training with all five event slots, although not frequently with any one of them. An event slot like (charge X) is much more plausible, but is unfortunately scored lower by the model. Representing the types of the arguments can help solve this problem. Few types of arguments are shared between the chain and (fly X). However, (charge X) shares many arguments with (accuse X), (search X) and (suspect X) (e.g., criminal and suspect). Even more telling is that these arguments are jointly shared (the same or coreferent) across all three events. Chains represent coherent scenarios, not just a set of independent pairs, so we want to model argument overlap across all pairs. The second problem with narrative chains is that they make judgments only between protagonist arguments, one slot per event. All entities and slots in the space of events should be jointly considered when making event relatedness decisions. As an illustration, consider the verb arrest. Which verb is more related, convict or capture? A narrative chain might only look at the objects of these verbs and choose the one with the highest score, usually choosing convict. But in this case the subjects offer additional information; the subject of arrest (police) is different from that of convict (judge). A more informed decision prefers capture because both the objects (suspect) and subjects (police) are identical. This joint reasoning is absent from the narrative chain model. The task of semantic role learning and labeling is to identify classes of entities that fill predicate slots; semantic roles seem like they’d be a good model for the kind of argument types we’d like to learn for narratives. Most work on semantic role labeling, however, is supervised, using Propbank (Palmer et al., 2005), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untagged newspaper text. Most similar to us, Alishahi and Stevenson (2007) learn verb specific semantic profiles of arguments using WordNet classes to define the roles. We learn situation-specific classes of roles shared by multiple verbs. Thus, two open goals in role learning include (1) unsupervised learning and (2) learning the roles themselves rather than relying on pre-defined role classes. As just described, Chambers and Jurafsky (2008) offers an unsupervised approach to event learning (goal 1), but lacks semantic role knowledge (goal 2). The following sections describe a model that addresses both goals. The next sections introduce typed narrative chains and chain merging, extensions that allow us to jointly learn argument roles with event structure. The first step in describing a narrative schema is to extend the definition of a narrative chain to include argument types. We now constrain the protagonist to be of a certain type or role. A Typed Narrative Chain is a partially ordered set of event slots that share an argument, but now the shared argument is a role defined by being a member of a set of types R. These types can be lexical units (such as observed head words), noun clusters, or other semantic representations. We use head words in the examples below, but we also evaluate with argument clustering by mapping head words to member clusters created with the CBC clustering algorithm (Pantel and Lin, 2002). We define a typed narrative chain as a tuple (L, P, O) with L and O the set of event slots and partial ordering as before. Let P be a set of argument types (head words) representing a single role. An example is given here: As mentioned above, narrative chains are learned by parsing the text, resolving coreference, and extracting chains of events that share participants. In our new model, argument types are learned simultaneously with narrative chains by finding salient words that represent coreferential arguments. We record counts of arguments that are observed with each pair of event slots, build the referential set for each word from its coreference chain, and then represent each observed argument by the most frequent head word in its referential set (ignoring pronouns and mapping entity mentions with person pronouns to a constant PERSON identifier). As an example, the following contains four worker mentions: But for a growing proportion of U.S. workers, the troubles really set in when they apply for unemployment benefits. Many workers find their benefits challenged. The four bolded terms are coreferential and (hopefully) identified by coreference. Our algorithm chooses the head word of each phrase and ignores the pronouns. It then chooses the most frequent head word as the most salient mention. In this example, the most salient term is workers. If any pair of event slots share arguments from this set, we count workers. In this example, the pair (X find) and (X apply) shares an argument (they and workers). The pair ((X find),(X apply)) is counted once for narrative chain induction, and ((X find), (X apply), workers) once for argument induction. Figure 1 shows the top occurring words across all event slot pairs in a criminal scenario chain. This chain will be part of a larger narrative schema, described in section 3.4. We now formalize event slot similarity with arguments. Narrative chains as defined in (Chambers and Jurafsky, 2008) score a new event slot (f, g) against a chain of size n by summing over the scores between all pairs: where C is a narrative chain, f is a verb with grammatical argument g, and sim(e, e') is the pointwise mutual information pmi(e, e'). Growing a chain by one adds the highest scoring event. We extend this function to include argument types by defining similarity in the context of a specific argument a: where A is a constant weighting factor and freq(b, b', a) is the corpus count of a filling the arguments of events b and b'. We then score the entire chain for a particular argument: sim((ei, di) , (ej, dj) , a) (3) Using this chain score, we finally extend chainsim to score a new event slot based on the argument that maximizes the entire chain’s score: The argument is now directly influencing event slot similarity scores. We will use this definition in the next section to build Narrative Schemas. Whereas a narrative chain is a set of event slots, a Narrative Schema is a set of typed narrative chains. A schema thus models all actors in a set of events. If (push X) is in one chain, (Y push) is in another. This allows us to model a document’s entire narrative, not just one main actor. A narrative schema is defined as a 2-tuple N = (E, C) with E a set of events (here defined as verbs) and C a set of typed chains over the event slots. We represent an event as a verb v and its grammatical argument positions D„ C_ {subject, object, prep}. Thus, each event slot (v, d) for all d E D„ belongs to a chain c E C in the schema. Further, each c must be unique for each slot of a single verb. Using the criminal prosecution domain as an example, a narrative schema in this domain is built as in figure 2. The three dotted boxes are graphical representations of the typed chains that are combined in this schema. The first represents the event slots in which the criminal is involved, the second the police, and the third is a court or judge. Although our representation uses a set of chains, it is equivalent to represent a schema as a constraint satisfaction problem between (e, d) event slots. The next section describes how to learn these schemas. Previous work on narrative chains focused on relatedness scores between pairs of verb arguments (event slots). The clustering step which built chains depended on these pairwise scores. Narrative schemas use a generalization of the entire verb with all of its arguments. A joint decision can be made such that a verb is added to a schema if both its subject and object are assigned to chains in the schema with high confidence. For instance, it may be the case that (Y pull over) scores well with the ‘police’ chain in figure 3. However, the object of (pull over A) is not present in any of the other chains. Police pull over cars, but this schema does not have a chain involving cars. In contrast, (Y search) scores well with the ‘police’ chain and (search X) scores well in the ‘defendant’ chain too. Thus, we want to favor search instead of pull over because the schema is already modeling both arguments. This intuition leads us to our event relatedness function for the entire narrative schema N, not just one chain. Instead of asking which event slot (v, d) is a best fit, we ask if v is best by considering all slots at once: where CN is the set of chains in our narrative N. If (v, d) does not have strong enough similarity with any chain, it creates a new one with base score Q. The Q parameter balances this decision of adding to an existing chain in N or creating a new one. We use equation 5 to build schemas from the set of events as opposed to the set of event slots that previous work on narrative chains used. In Chambers and Jurafsky (2008), narrative chains add the best (e, d) based on the following: where m is the number of seen event slots in the corpus and (vj, gj) is the jth such possible event slot. Schemas are now learned by adding events that maximize equation 5: where |v |is the number of observed verbs and vj is the jth such verb. Verbs are incrementally added to a narrative schema by strength of similarity. Figures 3 and 4 show two criminal schemas learned completely automatically from the NYT portion of the Gigaword Corpus (Graff, 2002). We parse the text into dependency graphs and resolve coreferences. The figures result from learning over the event slot counts. In addition, figure 5 shows six of the top 20 scoring narrative schemas learned by our system. We artificially required the clustering procedure to stop (and sometimes continue) at six events per schema. Six was chosen as the size to enable us to compare to FrameNet in the next section; the mean number of verbs in FrameNet frames is between five and six. A low Q was chosen to limit chain splitting. We built a new schema starting from each verb that occurs in more than 3000 and less than 50,000 documents in the NYT section. This amounted to approximately 1800 verbs from which we show the top 20. Not surprisingly, most of the top schemas concern business, politics, crime, or food. Most previous work on unsupervised semantic role labeling assumes that the set of possible automatically built from the verb ‘convict’. Each node shape is a chain in the schema. classes is very small (i.e, PropBank roles ARG0 and ARG1) and is known in advance. By contrast, our approach induces sets of entities that appear in the argument positions of verbs in a narrative schema. Our model thus does not assume the set of roles is known in advance, and it learns the roles at the same time as clustering verbs into frame-like schemas. The resulting sets of entities (such as {police, agent, authorities, government} or {court, judge, justice}) can be viewed as a kind of schema-specific semantic role. How can this unsupervised method of learning roles be evaluated? In Section 6 we evaluate the schemas together with their arguments in a cloze task. In this section we perform a more qualitative evalation by comparing our schema to FrameNet. FrameNet (Baker et al., 1998) is a database of frames, structures that characterize particular situations. A frame consists of a set of events (the verbs and nouns that describe them) and a set of frame-specific semantic roles called frame elements that can be arguments of the lexical units in the frame. FrameNet frames share commonalities with narrative schemas; both represent aspects of situations in the world, and both link semantically related words into frame-like sets in which each predicate draws its argument roles from a frame-specific set. They differ in that schemas focus on events in a narrative, while frames focus on events that share core participants. Nonetheless, the fact that FrameNet defines frame-specific argument roles suggests that comparing our schemas and roles to FrameNet would be elucidating. We took the 20 learned narrative schemas described in the previous section and used FrameNet to perform qualitative evaluations on three aspects of schema: verb groupings, linking structure (the mapping of each argument role to syntactic subject or object), and the roles themselves (the set of entities that constitutes the schema roles). Verb groupings To compare a schema’s event selection to a frame’s lexical units, we first map the top 20 schemas to the FrameNet frames that have the largest overlap with each schema’s six verbs. We were able to map 13 of our 20 narratives to FrameNet (for the remaining 7, no frame contained more than one of the six verbs). The remaining 13 schemas contained 6 verbs each for a total of 78 verbs. 26 of these verbs, however, did not occur in FrameNet, either at all, or with the correct sense. Of the remaining 52 verb mappings, 35 (67%) occurred in the closest FrameNet frame or in a frame one link away. 17 verbs (33%) deliberate deadlocked found convict acquit occurred in a different frame than the one chosen. We examined the 33% of verbs that occurred in a different frame. Most occurred in related frames, but did not have FrameNet links between them. For instance, one schema includes the causal verb trade with unaccusative verbs of change like rise and fall. FrameNet separates these classes of verbs into distinct frames, distinguishing motion frames from caused-motion frames. Even though trade and rise are in different FrameNet frames, they do in fact have the narrative relation that our system discovered. Of the 17 misaligned events, we judged all but one to be correct in a narrative sense. Thus although not exactly aligned with FrameNet’s notion of event clusters, our induction algorithm seems to do very well. Linking structure Next, we compare a schema’s linking structure, the grammatical relation chosen for each verb event. We thus decide, e.g., if the object of the verb arrest (arrest B) plays the same role as the object of detain (detain B), or if the subject of detain (B detain) would have been more appropriate. We evaluated the clustering decisions of the 13 schemas (78 verbs) that mapped to frames. For each chain in a schema, we identified the frame element that could correctly fill the most verb arguments in the chain. The remaining arguments were considered incorrect. Because we assumed all verbs to be transitive, there were 156 arguments (subjects and objects) in the 13 schema. Of these 156 arguments, 151 were correctly clustered together, achieving 96.8% accuracy. The schema in figure 5 with events detain, seize, arrest, etc. shows some of these errors. The object of all of these verbs is an animate theme, but confiscate B and raid B are incorrect; people cannot be confiscated/raided. They should have been split into their own chain within the schema. Argument Roles Finally, we evaluate the learned sets of entities that fill the argument slots. As with the above linking evaluation, we first identify the best frame element for each argument. For example, the events in the top left schema of figure 5 map to the Manufacturing frame. Argument B was identified as the Product frame element. We then evaluate the top 10 arguments in the argument set, judging whether each is a reasonable filler of the role. In our example, drug and product are correct Product arguments. An incorrect argument is test, as it was judged that a test is not a product. We evaluated all 20 schemas. The 13 mapped schemas used their assigned frames, and we created frame element definitions for the remaining 7 that were consistent with the syntactic positions. There were 400 possible arguments (20 schemas, 2 chains each), and 289 were judged correct for a precision of 72%. This number includes Person and Organization names as correct fillers. A more conservative metric removing these classes results in 259 (65%) correct. Most of the errors appear to be from parsing mistakes. Several resulted from confusing objects with adjuncts. Others misattached modifiers, such as including most as an argument. The cooking schema appears to have attached verbal arguments learned from instruction lists (wash, heat, boil). Two schemas require situations as arguments, but the dependency graphs chose as arguments the subjects of the embedded clauses, resulting in 20 incorrect arguments in these schema. The previous section compared our learned knowledge to current work in event and role semantics. We now provide a more formal evaluation against untyped narrative chains. The two main contributions of schema are (1) adding typed arguments and (2) considering joint chains in one model. We evaluate each using the narrative cloze test as in (Chambers and Jurafsky, 2008). The cloze task (Taylor, 1953) evaluates human understanding of lexical units by removing a random word from a sentence and asking the subject to guess what is missing. The narrative cloze is a variation on this idea that removes an event slot from a known narrative chain.Performance is measured by the position of the missing event slot in a system’s ranked guess list. This task is particularly attractive for narrative schemas (and chains) because it aligns with one of the original ideas behind Schankian scripts, namely that scripts help humans ‘fill in the blanks’ when language is underspecified. We count verb pairs and shared arguments over the NYT portion of the Gigaword Corpus (years 1994-2004), approximately one million articles. We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006), recording all verbs with subject, object, or prepositional typed dependencies. Unlike in (Chambers and Jurafsky, 2008), we lemmatize verbs and argument head words. We use the OpenNLP1 coreference engine to resolve entity mentions. The test set is the same as in (Chambers and Jurafsky, 2008). 100 random news articles were selected from the 2001 NYT section of the Gigaword Corpus. Articles that did not contain a protagonist with five or more events were ignored, leaving a test set of 69 articles. We used a smaller development set of size 17 to tune parameters. The first evaluation compares untyped against typed narrative event chains. The typed model uses equation 4 for chain clustering. The dotted line ‘Chain’ and solid ‘Typed Chain’ in figure 6 shows the average ranked position over the test set. The untyped chains plateau and begin to worsen as the amount of training data increases, but the typed model is able to improve for some time after. We see a 6.9% gain at 2004 when both lines trend upwards. The second evaluation compares the performance of the narrative schema model against single narrative chains. We ignore argument types and use untyped chains in both (using equation 1 instead of 4). The dotted line ‘Chain’ and solid ‘Schema’ show performance results in figure 6. Narrative Schemas have better ranked scores in all data sizes and follow the previous experiment in improving results as more data is added even though untyped chains trend upward. We see a 3.3% gain at 2004. The final evaluation combines schemas with argument types to measure overall gain. We evaluated with both head words and CBC clusters as argument representations. Not only do typed chains and schemas outperform untyped chains, combining the two gives a further performance boost. Clustered arguments improve the results further, helping with sparse argument counts (‘Typed Schema’ in figure 6 uses CBC arguments). Overall, using all the data (by year 2004) shows a 10.1% improvement over untyped narrative chains. Our significant improvement in the cloze evaluation shows that even though narrative cloze does not evaluate argument types, jointly modeling the arguments with events improves event clustering. Likewise, the FrameNet comparison suggests that modeling related events helps argument learning. The tasks mutually inform each other. Our argument learning algorithm not only performs unsupervised induction of situation-specific role classes, but the resulting roles and linking structures may also offer the possibility of (unsupervised) FrameNet-style semantic role labeling. Finding the best argument representation is an important future direction. The performance of our noun clusters in figure 6 showed that while the other approaches leveled off, clusters continually improved with more data. The exact balance between lexical units, clusters, or more general (traditional) semantic roles remains to be solved, and may be application specific. We hope in the future to show that a range of NLU applications can benefit from the rich inferential structures that narrative schemas provide. This work is funded in part by NSF (IIS-0811974). We thank the reviewers and the Stanford NLP Group for helpful suggestions.","Unsupervised Learning of Narrative Schemas and their Participants
We describe an unsupervised system for learning narrative schemas, coherent sequences or sets of events (arrested(POLICE,SUSPECT), convicted(JUDGE, SUSPECT)) whose arguments are filled with participant semantic roles defined over words (JUDGE = {judge, jury, court}, POLICE = {police, agent, authorities}).
Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles.
Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles.
By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles.
We describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing co referring arguments, followed by temporal classification to induce partial order.
","We describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury, Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles.The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist.Narrative Event Chains are partially ordered sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation.",0.8864753246307373,0.8561854362487793,0.8710671067237854
"Name Tagging With Word Clusters And Discriminative Training We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model. Active learning is used to select training examples. We evaluate the technique for named-entity tagging. Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance. Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material. At a recent meeting, we presented name-tagging technology to a potential user. The technology had performed well in formal evaluations, had been applied successfully by several research groups, and required only annotated training examples to configure for new name classes. Nevertheless, it did not meet the user's needs. To achieve reasonable performance, the HMM-based technology we presented required roughly 150,000 words of annotated examples, and over a million words to achieve peak accuracy. Given a typical annotation rate of 5,000 words per hour, we estimated that setting up a name finder for a new problem would take four person days of annotation work – a period we considered reasonable. However, this user's problems were too dynamic for that much setup time. To be useful, the system would have to be trainable in minutes or hours, not days or weeks. We left the meeting thinking about ways to reduce training requirements to no more than a few hours. It seemed that three existing ideas could be combined in a way that might reduce training requirements sufficiently to achieve the objective. First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999). The resulting clusters appeared to contain a great deal of implicit semantic information. This implicit information, we believed, could serve to augment a small amount of annotated data. Particularly promising were techniques for producing hierarchical clusters at various scales, from small and highly specific to large and more general. To benefit from such information, however, we would need an automatic learning mechanism that could effectively exploit it. Fortunately, a second line of recent research provided a potential solution. Recent work in discriminative methods (Lafferty et al., 2001; Sha and Pereira, 2003, Collins 2002) suggested a framework for exploiting large numbers of arbitrary input features. These methods seemed to have exactly the right characteristics for incorporating the statistically-correlated hierarchical word clusters we wished to exploit. Combining these two methods, we suspected, would be sufficient to drastically reduce the number of annotated examples required. However, we also hoped that a third technique, active learning (Cohn et al., 1996; McCallum and Nigam, 1998), would be particularly effective when used in conjunction with hierarchical word clusters. Specifically, active learning attempts to select examples for annotation by estimating the system's certainty about the answer, requesting a human judgment only for those cases where it is most uncertain. Unfortunately, the issue often comes down to whether a specific word has previously been observed in training: if the system has seen the word, it is certain, if not, it is uncertain. Word clusters at various scales, we hoped, would permit more subtle distinctions to influence the system's certainty, increasing the method’s effectiveness earlier in the process when fewer training examples have been annotated. We view clustering here as a method for estimating the probabilities of low frequency events, particularly events that are likely to go unobserved in a small annotated training corpus. For example, a clustering mechanism may choose to place AT&T in the same cluster as other company names based on contextual similarity. Then, even if the word AT&T was not previously annotated as a company, it may nonetheless be possible to infer that AT&T indeed is a company because it occupies a cluster that is populated mostly by other company names. Likewise, cluster membership can be used to exploit information from neighboring words. For example, if the word reported has previously been observed to follow person names, but the word announced has not yet been seen, it may be possible to guess that the word preceding announced is a person based on the fact that reported and announced occupy the same cluster. A practical obstacle to using clusters for this purpose is selecting an appropriate level of granularity: too small, and the clusters provide insufficient generalization; too large, and they inappropriately overgeneralize. Hierarchical clusters provide one way around the problem by avoiding commitment to any particular granularity in advance. However, the dominant trend during the past decade toward generative models has made integration of such hierarchical clusters difficult. Because the nested clusters surrounding each word are highly correlated, it is unreasonable to treat them as independent. Unfortunately, any treatment in a generative framework other than independent requires considerable ingenuity. Interestingly, before generative models began to dominate parsing, the Spatter parser (Magerman, 1995) achieved extremely promising results using a nongenerative statistical model. Of particular interest is the fact that Spatter used hierarchical word clusters for estimating its lexical attachment probabilities. However, the statistical decision trees underlying Spatter’s probability model never gained widespread acceptance, and indeed, our own limited experience with them yielded mixed results. In the past few years, researchers have begun to view generative models as instances of a broader class of linear (or log-linear) models, and have introduced discriminative methods (e.g. conditional random fields) to estimate the model parameters. These estimation methods do not impose the same strict independence conditions as generative models. Armed with modern discriminative training methods, it seemed reasonable to us to revisit hierarchical clustering. Specifically, we picked up where Spatter left off, with the clustering algorithm of (Brown et al., 1990). We implemented this algorithm twice as part of our work. The first implementation derived directly from the description given in the Brown paper. Then, in the hope of achieving greater efficiency, we reverseengineered the clustering software in Spatter. While the mathematical details differ slightly between the two algorithms, both aim to cluster together words so as to minimize the bigram language-model perplexity of the unsupervised corpus. In practice, we observed no significant differences in accuracy when using one or the other in our experiments. All experimental results given in this paper are with the Spatter clustering algorithm. The result of running the clustering algorithm is a binary tree, where each word occupies a single leaf node, and where each leaf node contains a single word. The root node defines a cluster containing the entire vocabulary. Interior nodes represent intermediate size clusters containing all of the words that they dominate. Thus, nodes higher in the tree correspond to larger word clusters, while lower nodes correspond to smaller clusters. A particular word can be assigned a binary string by following the traversal path from the root to its leaf, assigning a 0 for each left branch, and a 1 for each right branch. The following are example bit strings from the Spatter clustering algorithm: To implement discriminative training, we followed the averaged perceptron approach of (Collins, 2002). Our decision was based on three criteria. First, the method performed nearly as well as the currently best global discriminative model (Sha and Pereira, 2003), as evaluated on one of the few tasks for which there are any published results (noun phrase chunking). Second, convergence rates appeared favorable, which would facilitate multiple experiments. Finally, and most important, the method appeared far simpler to implement than any of the alternatives. algorithm exactly as described by Collins. However, we did not implement cross-validation to determine when to stop training. Instead, we simply iterated for 5 epochs in all cases, regardless of the training set size or number of features used. Furthermore, we did not implement features that occurred in no training instances, as was done in (Sha and Pereira, 2003). We suspect that these simplifications may have cost several tenths of a point in performance. A set of 16 tags was used to tag 8 name classes (the seven MUC classes plus the additional null class). Two tags were required per class to account for adjacent elements of the same type. For example, the string Betty Mary and Bobby Lou would be tagged as PERSON-START PERSON-START NULL-START PERSON-START PERSON-CONTINUE. Our model uses a total of 19 classes of features. The first seven of these correspond closely to features used in a typical HMM name tagger. The remaining twelve encode cluster membership. Clusters of various granularity are specified by prefixes of the bit strings. Short prefixes specify short paths from the root node and therefore large clusters. Long prefixes specify long paths and small clusters. We used 4 different prefix lengths: 8 bit, 12 bit, 16 bit, and 20 bit. Thus, the clusters decrease in size by about a factor of 16 at each level. The complete set of features is given in Table 2. We implemented the averaged perceptron training We used only a rudimentary confidence measure to perform active learning, introducing no additional features beyond those used in training and decoding. The confidence score we assign to a sentence is just the un-normalized difference in perceptron scores between the highest scoring theory and the second highest scoring alternative. To apply active learning, we simply To compute the confidence scores efficiently, we use a combination of the forward Viterbi and backward Viterbi scores at each word. We define the confidence at a word to be the difference between the summed forward and backward scores of the best and second best tags for that word. The confidence for the entire sentence is then just the minimum of the scores at each word position. We performed our experiments using the seven MUC-6 name categories: person, organization, location, date, time, percent, and monetary amount. For annotated data, we used text from Sections 02-23 of the Wall Street Journal Treebank corpus that had previously been annotated with the MUC name classes. Sections 02-21 were used as training material, and Section 23 was used as test (note that the syntactic trees were not used in any way). Scoring was performed using the MUC scorer. For unsupervised clustering data, we used the Wall Street Journal subset of the Continuous Speech Recognition (CSR-III) collection (LDC catalog # LDC95T6). This portion of the collection contains approximately 100 million words. Active learning experiments were performed by permitting the system to choose examples from among the pool of annotated data, rather than presenting the examples in their natural chronological order. This approach, previously used in [Boschee et al, 2002], permits simulation of human-in-the-loop experiments that are inexpensive to run and repeatable because they don’t actually involve a human annotator. However, because the pool of pre-annotated examples is limited, the results are most meaningful for small training sets. Once the system has selected the most useful examples from the pool, it is forced to choose among the remainder that it previously rejected as less useful. At the extreme where all available examples are used, our experimental framework prevents active learning from exhibiting any benefit whatsoever since the system is left no choice in selecting examples. Before considering the impact of word clustering on system performance, we first evaluate the discriminative tagger relative to the baseline HMM. For this experiment, we used all of the features described in Section 3 except word cluster features. The remaining features encode essentially the same information used in the HMM, although in a slightly different form. Results are shown in Figure 1. For very small and very large training sets, the systems perform about the same. Between these extremes, the discriminative tagger exhibits somewhat, though not distressingly, worse performance. We conjecture that lack of smoothing in the discriminative tagger may account for the difference. Second, we consider the impact of word clusters. Figure 2 compares performance of the discriminative tagger, now with cluster features included, to the baseline HMM. Immediately, with only 5,000 words of training, the discriminative model significantly outperforms the HMM. With 50,000 words of training, performance for the discriminative model exceeds 90F, a level not reached by the HMM until it has observed 150,000 words of training. Somewhat surprisingly, the clusters continue to provide some benefit even with 1,000,000 words of training. At this operating point, the discriminative tagger achieves an F-score of 96.08 compared to 94.72 for the HMM, a 25% reduction in error. Third, we consider the impact of active learning. Figure 3 shows (a) discriminative tagger performance without cluster features, (b) the same tagger using active learning, (c) the discriminative tagger with cluster features, and (d) the discriminative tagger with cluster features using active learning. Both with and without clusters, active learning exhibits a noticeable increase in learning rates. However, the increase in learning rate is significantly more pronounced when cluster features are introduced. We attribute this increase to better confidence measures provided by word clusters – the system is no longer restricted to whether or not it knows a word; it now can know something about the clusters to which a word belongs, even if it does not know the word. Finally, Figure 4 shows the impact of consolidating the gains from both cluster features and active learning compared to the baseline HMM. This final combination achieves an F-score of 90 with less than 20,000 words of training – a quantity that can be annotated in about 4 person hours – compared to 150,000 words for the HMM – a quantity requiring nearly 4 person days to annotate. At 1,000,000 word of training, the final combination continues to exhibit a 25% reduction in error over the baseline system (because of limitations in the experimental framework discussed earlier, active learning can provide no additional gain at this operating point). The work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Lin et al., 2003; Boschee et al, 2002; Collins and Singer, 1999; Yarowsky, 1995) that all focuses on reducing annotation requirements through a combination of (a) seed examples, (b) large unannotated corpora, and (c) training example selection. Moreover, our work is based largely on existing techniques for word clustering (Brown et al., 1990), discriminative training (Collins 2002), and active learning. The synthesis of these techniques, nevertheless, proved highly effective in achieving our primary objective of reducing the need for annotated data. Much work remains to be done. In an effort to move rapidly toward our primary objective, we investigated only one type of discriminative training (averaged perceptron), only one type of clustering (bigram mutual information), and only one simple confidence measure for active learning. It seems likely that some additional gains could be realized by alternative discriminative methods (e.g. conditional random fields estimated with conjugate-gradient training). Similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance. On the application side, it would be interesting to apply the technique to other language problems. Applying it to parsing would yield a rare sense of closure, knitting together the word clustering of Magerman’s (1995) Spatter parser – arguably the first successful broadcoverage statistical parser – with structural elements of the now-dominant Collins (1997) style parsers. Because our combined method promises to require substantially less training data, it may also prove useful for so-called low-density languages, where limited resources – and even more limited numbers of native speakers – are available. For the moment, we find the initial results encouraging. We achieved a 25% reduction in error on a standard named-entity problem, compared to a state-of-the-art HMM. Our main objective, though, was not reducing error rates but rather reducing the amount of annotation required. At least for the named-entity task we studied, using the method described, a single annotator could begin work after breakfast and, by lunchtime, have enough data annotated to achieve an F-score of 90.","Name Tagging With Word Clusters And Discriminative Training
We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus.
Cluster membership is encoded in features that are incorporated in a discriminatively trained tagging model.
Active learning is used to select training examples.
We evaluate the technique for named-entity tagging.
Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance.
Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material.
We use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity.
We use the Brown algorithm for clustering (Brown et al 1992).
","We evaluate the technique for named-entity tagging.Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material.The resulting clusters appeared to contain a great deal of implicit semantic information.We view clustering here as a method for estimating the probabilities of low frequency events, particularly events that are likely to go unobserved in a small annotated learning corpus.The technique is used to select training examples.It seemed that three existing ideas could be combined in a way that might reduce training requirements sufficiently to achieve the objective.First were techniques for producing word clusters from large unannotated corpora (Brown",0.8929222226142883,0.8800817728042603,0.8864554762840271
"Effects of Adjective Orientation and Gradability on Sentence Subjectivity Vas i le ios  Hatz ivass i log lou Depar tment  o1 Computer  Sc ience Co lumbia  Un ivers i l y New York,  NY  10027 vh@cs ,  co lumbia ,  edu Janyce  M.  Wiebe Depar tment  o f  Computer  Sc ience New Mex ico  State Un ivers i ty Las  Cruces ,  NM 88003 w iebe@cs ,  nmsu. edu Abstract Subjectivity is a pragmatic, sentence-level feature that has important implications for texl processing applica- lions such as information exlractiou and information ic- lricwd. We study tile elfeels of dymunic adjectives, se- mantically oriented adjectives, and gradable ad.ieclivcs on a simple subjectivity classiiicr, and establish lhat lhcy arc strong predictors of subjectivity. A novel train- able mclhod thai statistically combines two indicators of gradability is presented and ewlhlalcd, complementing exisling automatic Icchniques for assigning orientation labels. 1 I n t roduct ion In recent years, computalional tcchniqt,es for the deter- mination of &:deal semantic features have been proposed and ewdualed. Such features include sense, register, do- main spccilicity, pragmatic restrictions on usage, scnlan- lic markcdncss, and orientation, as well as automatically ictcnlifiecl links between words (e.g., semantic rclalcd- hess, syllollynly, antonylny, and tneronymy). Aulomal- ically learning features of this type from hugc corpora allows the construction or augmentation of lexicons, and the assignment of scmanlic htbcls lo words and phrases in running text. This information in turn can bc used to help dcterlninc addilional features at the It?teal, clause, sentence, or document level. Tiffs paper explores lira benelits that some lexical fea- tures of adjectives offer lor the prediction of a contexlual sentence-level feature, suOjectivity. Subjectivity in nat- ural language re[crs to aspects of language used to ex- press opinions and ewfluations. The computatiomtl task addressed here is to distinguish sentences used to present opinions and other tbrms of subjectivity (suOjective sen- tences, e.g., ""At several different layers, its a fascinating title"") from sentences used to objectively present factual information (objective sentences, e.g., ""Bell industries Inc. increased its quarterly to 10 cents from 7 cents a share""). Much research in discourse processing has focused on task-oriented and insmmtional dialogs. The task ad- dressed here comes to the fore in other genres, especially news reporting and lnternet lorums, in which opinions of various agents are expressed and where subjectivity judgements couht help in recognizing inllammatory rues- sages (""llanles) and mining online sources for product reviews. ()thor (asks for whicll subjectivity recognition is potentially very useful include infornmtion extraction and information retrieval. Assigning sub.icctivity labels to documents or portions of documents is an example of non-topical characteri?ation f information. Current in- formation extraction and rolricval lechnology focuses al- most exclusively on lhe subject matter of the documcnls. Yet, additiomtl components of a document inllucncc its relevance to imrlicuhu ? users or tasks, including, for ex- alnple, the evidential slatus el: lhc material presented, and attitudes adopted in fawn"" or against a lmrticular person, event, or posilion (e.g., articles on a presidenlial cam- paign wrillen to promote a specific candidate). In sum- marization, subjectivity judgmcnls could be included in documcllt proiilcs to augment aulomatically produced docunacnt summaries, and to hel l) the user make rele- vance judgments when using a search engine. ()thor work on sub.iectivity (Wicbc et al., 1999; Bruce and Wicbc, 2000) has established a positive and statisti- cally signilicant correlation with the presence of adiec- lives. ?incc the mere presence of one or iDoic adjectives is useful for prcdicling (hat a scntcrtce is subjective we investigate ill this paper (lie cflccts of additional cxical scmanlic lcalurcs of adjectives that can be automatically learned from corpora. We consider two such l%atures: se- mantic orientation, which represents an ewdualivc har- acterization of a words deviation from the norm for its semantic group (e.g., beauti/ul is positively oriented, as opposed to ugly); and gradability, which characterizes a words ability to express a property in wlrying degrees. In lira remainder of this paper, we [irst address adjec- tive orientation in Section 2, summarizing a previously published method for automatically separating oriented adjectives into positive and negative classes. Then, Sec- tion 3 presents a novel method for learning gradablc ad- jectives using a largo corpus and a statistical feature com- bination naodel. In Section 4, we review earlier exper- iments on testing subjectivity using wuious fcatt, res as predictors, and then present comparative analyses of the effects that orientation and gradability have on our abil- ity to Inedict sentence subjectivity from adjectives. Wc show that both give us higher-quality features for recog- nizing st@icctive sentences, and conclude by discussing future extensions to Ibis work. 299 Ct Number of Number of Average nnmber Ratio o1 average adjectives in links in of links for Accuracy test set (IAc~l) test set (IL~I) each adjective group frequencies 730 2,568 7.04 78.08% 1.8699 516 2,159 8.37 82.56% 1.9235 369 1,742 9.44 87.26% 1.3486 236 1,238 10.49 92.37% 1.4040 Table 1: Evaluation o1 the adjective orientation classification and labeling methods (from (Hatzivassiloglou and McK- eown, 1997)). 2 Semantic Orientation The semantic orientation or polarity of a word indicates the direction the word deviates flom the norm for its se- mantic group or lexicalfield (Lehrer, 1974). It is an eval- uative characteristic (Battistella, 1990) of the meaning of the word which restricts its usage to appropriate prag- matic contexts. Words that encode a desirable state (e.g., beautiful, unbiased) have a positive orientation, while words that represent undesirable states have a negative orientation. Within tile particular syntactic lass o1 ad- jectives, orientation can be expressed as the ability of an adjective to ascribe in general a positive or negative qual- ity to the modified item, making it better or worse than a similar unmodilied item. Most antonymous adjectives can be contrasted on the basis of orientation (e.g., beautil)d-ugly); similarly, nearly synonymous terms are often distinguished by dill fcrent orientations (e.g., simple-siml)listic). While ori- entation applies to many adjectives, there are also those that have no orientation, typically as members of groups of complementary, qualitative terms (Lyons, 1977) (e.g., domestic, medical, or red). Since orientation is inher- ently connected with cwduative judgements, it appears to be a promising feature for predicting subjectivity. Hatzivassiloglou and McKeown (1997) presented a method for autonmtically assigning a + or - orientation label to adjectives known to have some semantic orien- tation. Their method is based on information extracted fiom conjunctions between adjectives in a large corpus I because orientation constrains the use of the words in specific contexts (e.g., compare corrupt and brutal with *corrupt but brutal), observed conjunctions of adjectives can be exploited to inler whether the conjoined words are of the same or different orientation. Using a shallow parser on a 21 million word corpus of Wall Street Jour- nal articles, Hatzivassiloglou and McKeown developed and trained a log-linear statistical model that predicts whether any two adjectives have the same orientation with 82% accuracy. The predicted links o1 same or dil L ferent orientation are automatically assigned a strength value (essentially, a confidence stimate) by tile model, and induce a graph that can be partitioned with a clus- tering algorithm into components so that all words in the same component belong to the same orientation class. Once the classes have been determined, flequency infor- mation is used to assign positive or negative labels to each class (there are slightly fewer positive terms, but with a significantly higher rate of occurrence than nega- tive terms). Hatzivassiloglou and McKeown applied their method to 1,336 (657 positive and 679 negative) adjectives which were all the oriented adjectives appearing in the corpus 20 times or more. Orientation labels were assigned to these adjectives by hand. I Subsequent validation of the initial selection and label assignment steps with indepen- dent human judges showed an agreement of 89% tor tile first step and 97% for the second step, establishing that orientation is a fairly objective semantic property. Be- cause the accuracy ol the method depends on the den- sity of conjunctions per adjective, Hatzivassiloglou and MeKeown tested separately their algorithm for adjectives appearing in at least 2, 3, 4, or 5 conjunctions in the co l pus; their results are shown in Table I. In this paper, we use the model labels assigned by hand by Hatzivassiloglou and McKeown, and tile labels automatically obtained by their method and reported in (Hatzivassiloglou and McKeown, 1997) with the follow- ing extension: An adjective that appears in k conjunc- tions will receive (possibly different) labels when ana- lyzed together with all adjectives appearing in at least 2, 3 . k conjunctions; since performance generally in- creases with the number of conjunctions per adjective, we select as the orientation label the one assigned by the experi,nent t,sing the highest applicable conjunctions threshold. Overall, we have labels for 730 adjectives 2, with a prediction accuracy of 81.51%. 3 Gradability Gradability (or grading) (Sapir, 1944; Lyons, 1977, p. 27 I) is the semantic property that enables a word to par- ticipate in comparative constructs and to accept mod- ifying expressions that act as intensitiers or diminish- ers. Gradable adjectives express properties in varying degrees ot strength, relative to a norm either explicitly ISome adjectives with unclem; mnbiguous, or conlexl,-dependenl orientation were excluded. 2Those appearing in the corpus in two conjunctions or inore, since some conjunction data nlust be left out to hain the link prediction algo- rithm. 300 cold Unmodilied by grading words Moditied by grading words civil Unmodilied by grading words Modified by grading words Uninllected 392 20 1,296 1 Inllected for degree 18 0 0 0 litble 2: Extracted wdues of gradability indicators, i.e., frequencies of the word with or without he specitied intlection or moditication, for two adjectives, one gradable (cold) and one primarily non-gradable (civil). The frequencies were compt, ted liom the 1987 Wall Street Journal corpus. mentioned or implicitly supplied by the modilied noun (for example, asmall planet is usually much larger thart a large house; cf. the distinction between absolute and tel- alive adjectives made by Katz (1972, p. 254)). This rel- ativism in the interpetation of gradable words indicates that gradability is likely to be a good predictor ?71 subjec- tivity. 3.1 Indicators ofgradability Most gradable words appear at least several times in a large corpt, s either in forms inflected for degree (i.e., comparative and superlative), or in tile context of grading modilicrs such as veo,. However, non-gradable words may also occasionally appear in such contexts or forms under exceptional circumstances. For example, ve O, dead can be used tk)r emphasis, and re&let am~ re&let (as in ""her lhce became redder and redder"") can be used to indicate a progression of coloring, qb distinguish be- tween truly gradablc adjectives and non-gradable adjec- tives in these exceptional contexts, we have developed a trainable log-linear statistical model that lakes into ac- count tile number of times an ad.iective has been observed in a form or context indicating gradability relative to the number of limes it has been seen in non-gradable con- texts. We use a shallow parser to retrieve from a large corpus tagged for part-of-speech with Churchs PARTS tagger (Church, 1988) all adjectives and their modifiers. Al- though the most common use of an adverb modifying an adjective is to function as an intensilier or diminisher (Quirk et al., 1985, p. 445), adverbs can also add to tile semantic ontent of the adjectival phrase instead of pro- viding a grading effect (e.g., immediately available, po- litically vuhmrable), or function as cmphasizers, adding to the force o1 tile base adjective and not lo its degree (e.g., virtually impossible; compare *re O, impossible). Therefore, we compiled by hand a list of 73 adverbs and noun phrases (such as a little, exceedingly, somewhat, and veo) that are fiequently used as grading moditicrs. The number of times each adjective appears mod ilied by a term form this list becomes a first indicator of gradabil- ity. To detect inflected forms o1 adjectives (which, in 15> glish, always indicate gradability st, bject to the excel> tions discussed earlier), we have implemented an auto- matic lnorphology analysis component. This program recognizes several irregular forms (e.g., good-better- best) and strips tile grading suffixes -er and -est Dora regularly inllected adjectives, producing a list of candi- date base forms that if inflected would yield tilt origi- nal adjective (e.g., bigger produces three potential forms, big, bigg, and bigge). The frequency of these candi- date base words is checked against ile corpus, and tile form with signilicantly higher frequency is selected. To guard against cases of base adjective forms that end in -er or-est (e.g., sih,er), the original word is also included alllong tile candidates. The total number of times this procedure is successfully applied for each adjective be- comes a second indicator of gradability. 3.2 l )etermlnlng radabil l ty The presence or absence of each of the above two indica- tors results in a 2 x 2 frequency table IBr each adjective; examples for one gradable and one non-gradable adjec- tive are given in ""lhble 2. ""lb convert lhese four numbers to a single decision on tile gradability of tile ad.iective, we use a log-linear model. Ix)g-linear models (Nantnef and l)ufly, 1989) construct a linear combination (weighted sum) of the predictor wlriables 1~, i=1 and relate it to the actual response H. (in this case, 0 for non-gmdable and 1 for gradable) via the so-called logis- tic trcm,sJbrmation, 1~- I -t- eJ Maximum likelihood estimates for the coefficients fli are obtained from training samples for which the correct response H, is known, using the iterative reweighted non- linear least squares algorithm (Bates and Watts, 1988). This statistical model is particularly suited for model- ing variables with a ""yes""-""no"" (binary) value, because, unlike linear models, it captures the dependency of IFs variance on its mean (Santner and Dully, 1989). We normalize the counts for the two indicators of g,adability, and the cot, at otjoint occurrences of both in- tleetion and modilication by grading moditiers, by divid- ing with the total frequency of the adjective in the corpus. In this manner, we obtain three real-valued predictors 301 Classitied as gradable: acceptable accurate afraid aware busy careful cautious el~eap creative critical dangerous different disappointing equal fair fanfiliar far favorable formal free frequent good grand inadequate intense interesting legitimate likely positive professional reasonable rich short-term significant slow solid sophisticated sound speculative thin tight tough uucertain widespread worth Classilied as non-gradable: additional alleged alternative annual antitrust automatic ertain criminal cumulative daily deputy domestic ldcrly false linaneial first-quarter full hefty illegal institutional internal egislative long-distance military min imum monthly moral national official one-time other outstanding present prior prospective punitive regional scientific secondary sexual subsidiary taxable three-nmnth three-year total tremendous two-year unfifir unsolicited upper vohmtary white wholesale world-wide wrong Figure 1: Automatically obtained classification of a sample of 100 adjectives as gradable or not. Correct decisions (according to the COBU1LD-based reference model) are indicated in bold. ,  3 for the log-linear model. We also con- sider a modilied model, where any adjective for which any occurrence of simultaneous inflection and modilica- tion has been detected is automatically labeled gradable; the remaining two predictors are used to classify the ad- jectives that do not fullill this condition. This modilica- tion is motivated by the fact that observing an adjective in such a context offers a very high likelihood o1 grad- ability. 3.3 Experimental results We extracted from the 1987 Wall Street Journal corpus (21 million words) all adjectives with a frequency o1 300 or more; this produced a collection of 496 words. Grad- ability labels specifying whether each word is gradable or not were manually assigned, using tim designations of the Collins COBUILD (Collins Birmingham Univer- sity International Language Database) dictionary (Sin- clair, 1987). COBUILD marks each sense of each adjec- tive with one of the labels QUALIT, CLASSIF, or COLOR, corresponding to gradable, non-gradable, and color ad- jectives. In cases where COBUILD supplies conflicting labels for different senses of a word, we either omitted that word or, if a sense were predominant, gave it the label of that sense. In some cases, the word did not appear in COBUILD; these typically were descriptive compounds peci[ic to the domain (e.g., anti-takeover, over-the-coullter) and were in most cases marked as non- gradable adjectives. Overall, 453 of tile 496 adjeclives (91.33%) were assigned gradability labels by hand, while the remaining 53 words were discarded because they were misclassitied as adjectives by the part-ol:speech tagger (e.g., such) or because they coukt not be assigned a unique gradability label in accordance with COBUILD. Out of these words, 235 (51.88%) were manually classi- lied as gradable adjectives, and 218 (48.12%) were clas- silied as non-gradablc adjectives. Following the methodology of the preceding subsec- tion, we recovered the inflection and modilication indica- tors for these 453 adjectives, and trained both the unmod- ified and modilied log-linear models rcpcatedly, using a randomly selected subset ol 300 adjectives for training and 100 adjectives for testing. The entire cycle of se- lecting random test and training sets, fitting the models coefficients, making predictions, and evaluating the pre- dicted gradability labels is repeated 100 times, to ensure that the ewtluation is not affected by a lucky (or unlucky) partition of the data between training and test sets. This procedure yields over the 453 adjectives gradability clas- sifications with an average precision o1 93.55% and av- erage recall o1 82.24% (in terms of the gradable words reported or recovered, respectively). The overall accu- racy of the predicted gradability labels is 87.97%. These results were obtained with the modified log-linear model, which slightly ot, tperformed the model that uses all three predictors (in that case, we obtained an average precision of 93.86%, average recall ol 81.70%, and average over- all accuracy o1 87.70%). Figure I lists the gradability labels that were automatically assigned to one of the 100 random test sets ttsing the moditied prediction algorithm. We also assigned automatically labels to the entire set of 453 adjectives, using 4-fold cross-validation (repeatedly training on three-fourths of tim 453 adjectives and test- ing on the rest). This resulted in precision of 94.15%, recall of 82.13%, and accuracy of 88.08% for the entire adjective set. 4 Subjectivity The main motivation for the present paper is to examine the effect that information about an adjectives semantic orientation and gradability has on its probability of oc- curring in a subjective sentence (and hence on its quality as a subjectivity predictor). We tirst review related work on subjectivity recognition and then present our results. 4.1 Previous work on subjectivity recognition In work by Wiebc, Bruce, and OHara (Wiebe ct al., 1999; Bruce and Wicbe, 2000), a corpus of 1,001 sen- tences 3 of the Wall Street Journal TreeBank Corpus 3Conlpoutld sentences were manually segmented into their con- juncts, and each conjtmct treated as a scparale sentence. 302 (Marcus et al., 1993) was nlanually annotated with sub- jeciivity chlssifications. Specifically, each sentence was assigned a subjective or objective classitication, accord- ing to concensus lags derived by a slalistical analysis of lhe chisses assigned by three human judges (see (Wiebe et al., 1999) for further infornmtion). The total nulnber of subjective sentences in lhe data is 486, and the total number of objeclive sentences i 515. Bruce and Wiebe (2000) performed a statistical anal- ysis of the assigned classitications, linding lhat ac(iec- tivcs are statistically signilicantly and positively corre- lated with subjective sentences in the corpus on the basis (, . The proba- of the log-likelihood ratio test statistic -,2 bility of a sentence being subjective, simply given din! there is at least one adjective in lhe sentellee, is 56%, even though there are more objective than subjective sen- lences in the corpus. In addition, Bruce and Wicbe iden- tiffed a type of adjective that is indicative of subjective sentences: those Quirk et al. (1985) term dynamic, which ""denote qualities that a,e thoughl to be subjecl to con- trol by the possessor"" (p. 434). IZxamples are ""kind"" and ""careful"". Bruce and Wiebe nianually applied synlactic tests to identify dynamic adjectives in hall of the corpus nlentioned above. We inclutle such adjectives in the anal- ysis below, to assess whether additional lexical seinantic features associated with subjectivity hel I ) improve pro- dictability. (1999) developed an automatic system to perform st, bjectivily lagging. In 10-fold cross valida- lion experiments applied to the corpus described above, a probabilislic lassilier oblaincd an average accuracy on subjectivity lagging of 72.17%, nlorc Ihan 20 perccnlage poinls higher than the baseline accuracy obtained by al- ways choosing tile nlore frcquent class. A binary feature is included for each of lhe lbllowing: lhe presence in lhe sentence of a plollotln, an adjective, a cardinal number, a modal other fllan will, and an adverb other than #lot. They also inchlded a binary feature representing whether or not the sentence begins a new lxuagraph, l:inally, a feature was included representing co-occurrence of word tokens and punciuation marks with tile sul~jective and ob- jective classilicfition. An analysis of the system showed that the adjective [cature was imporlant to realizing the inlprovolncnts over lllO baseline accuracy, in this ])apci, we use lhe performance of the simple adjcclive fealtue as a baseline, and identify higher quality adjeclive features based on gradability and orienlalion. 4.2 Or ientat ion and gradabi l i ty  as subjectivity predictors: Results We measure the precision of a simple prediction method for subjectivity: a sonlence is classilicd as subjcclivc il at least one nlonlbor of a set of adjectives N occurs in 1he sontonco, alld objeclive otherwise. By wirying 1tlo sot (e.g., all adjeclives, only gradable adjectives, only nega- tively orienied adjectives, etc.) we call assess the t, seful- heSS of ihe additional knowledge for predicting subjec- livity. For the present study, we use tile set of all adjectives automatically identified in tile corpt, s by Wiebc et al. (1999) (Section 4.1 ); the set of dynamic adjectives Ill,{Inu- ally identified by Bruce and Wiebe (2000) (Section 4.1); tile set of scnmntic orientation labels assigned by Hatzi- vassiloglou and McKeown (1997), both manually and automatically with our extension described in Section 2; and the set of gradability labels, both manually and att- tomatically assigned according to the revised log-linear model of Section 3. We calculate restllts (shown in hi- ble 3) for each of lhese sets of all adjectives, dynamic, oriented and gradable adjectives, as well as for unions and intersections of lhose sets. Nole fliat these four sets have been extracted lrom comparable but different cor- pora (different years of the Wall Street Journal), therefore sometimes adjectives in one corpus may not be present in the other corpus, reducing the size of intersection sets. Also, for gradability, we worked with a sample set of 100 adjectives rather than all possible adjectives we could automatically calcuhtte gladabiliiy vahles for, since our goal in the present work is to measure correlations be- tween these sets and sul~jeciivity, rather than building a system for predicling subjectivity for as many ac[iectives as possible. In Table 3, the second cohmm identifies 8, the set of ac[iective types in question. The third cohimn gives the number of subjective sentences that contain one or more instances of members of S, and the fourth colunul gives lhe same ligure for ol~jective sentences. Therefore these two cohinuls together specify lhe coverage of tlm subjectivity indicator examined. The lifth cohimn gives 111c onditional probability that a sentence is subjective. givell that (tile of iilorc illstatices of ti/enlbcl+S of +5; ap- pears. This is a precishm inetrie that assesses feature quality: if inslances of <""7 appear, how likely is the son- tence to be subjective? The last two colunuls contrast the observed conditional probability with the a priori prob- ability of subjective sentellees (i.e., chalice; sixth col- ulnn) and with the probability assigned by the baseline all-adjectives model (i.e., the lirst row in the table; sev- enth colunm). The nlost striking aspect of these results is lhat all sets involviug dynamic adiectives positive or negative po- larity, or gradability are better predictors of sul~jective sentenccs than the class of adjectives as a whole, lqve of the sets are at least 25 points better (LI4, LI6, L21, L23, and L24); four others are at least 20 points better (L2, L9, L13, and 1,15); and live others are at least 15 points better (L4, LI I, 1,18, L20, and 1,22). In most of these cases, the difference between these predictors and all adjectives i  statistically signiticant 4 fit the 5% level or less; ahnost all of these predictors offer statistically sig- nificantly better than even odds in predicting subjectivity correctly. In nlany cases where statistical signilicance Iwe applied achi-square l st Oll the 2 x 2 cross-classificalion able (Fleiss, 1981). Adjeclive Set S # Subj Sents with (s G ,5) + Dyn Adjs fq S of L5. # Obj Sents l(Subj Sent I Significance with (s G ,5) + (~ e S) +) Against maiority Against all adjs All Adjectives 403 321 0.56 0.0041 N/A Dynamic Adjectives 92 32 0.74 1.1989 ? 1.0 - r  1..6369 - 10 -4 Pol+, man 138 87 0.61 0.0007 0.1546 Pol- ,  man 79 37 0.67 0.0001 0.0158 Pol+ U Pol- ,  man 197 114 0.63 6.91.91 ? 10 -~ 0.0260 Grad, man 193 115 0.63 1.9633 ? 10 -~ 0.0440 Not Grad, man 172 147 0.54 0.1084 0.6496 to1+, auto 121 79 0.60 0.0026 0.2537 Pol- ,  auto 61 21 0.74 1.1635 ? 10 -~ 0.0017 PoI+ U Io1--, auto 170 95 0.64 8.5888 - 10 -~ 0.0202 Grad, auto 30 14 0.68 0.0166 0.1418 Not Grad, auto 63 51 0.55 0.2079 0.9363 51 19 0.73 0.0001 0.0081 8.0397.10 -~ Dyn Adjs 71 S of L6. 39 8 0.83 l)yn Adjs 71 S of L I0. 50 19 0.72 0.0002 0.0103 Dyn Adjs 71 S ofLl  I 7 2 0.78 0.1582 0.3220 Grad 71 Pol+, man 90 58 0.61 0.0070 0.2891 Grad 71 Pol-,  man 35 I6 0.69 0.0080 0.09711 Grad 71 (Pol+ U Pol-), man 119 71 0.63 0.0005 0.1000 Grad fl Pol+, auto 13 6 0.68 0.1376 0.3833 Grad n Pol-,  auto 2 0 1.00 0.4556 0.5838 Grad 71 (Pol+ U Pol-), auto 15 6 0.71 0.0636 0.2255 l)yn Adjs N S o1 L22. 4 0 1.00 0.1203 0.2019 I)yn Adjs (1 ,_""; of L19. 24 5 0.83 0.0006 0.0070 Key: (s G ,5)+: one or more instances of members ofS. Ib/+: positive polarity, l b l - :  negalive polarity. GtzM: gradable. Matt: manually identilied. Auto: automalically identified. Table 3: Subjectivity prediction results. 4.3671.. 10 -4 could not established this is due to small counts, caused by the small size of the set of adjectives automatically labeled for gradability. It is also important to note that, in most cases, tile automatically-classified adjectives are comparable or better predictors of subjective sentences than the manually-assigned ones. Comparing tile automatically generated classes with the manually identilied ones, the positive polarity set decreases by 1 percentage point (L3 and L8), while the negative polarity set increases by 7 points (L4 and L9), and the gradable sot increases by 5 percentage points (L6 and LI 1). Among the intersection sets, in two cases the results are lower for tile computer- generated sets (Ll 3/LI 5 and L 14/L 16), but in tile other 4 eases, the results are higher (LI 7/L20, L 18/L21, L19/L2, L24/L23). Finally, the table shows that, in most cases, pro- dictability improves or at worst remains essentially tile same as additional lexical features are considered. For tile set of dynamic adjectives, the predictability is 74% (L2), and improves in 4 of the 6 cases in which it is in- tersected with other sets (LI4, L l6, L23, and L24). For the other two (L 13 and LI 5), predictability is only 1 or 2 points lower (not statistically significant). For the man- ually assigned polarity and gradability sets, in one case predictability is lower (L17 < L6), but in the other cases it remains the same or improves. The results are even better for the automatically assigned polarity and grad- ability sets: predictability improves when both features are considered in all but one case, when predictability remains the same (L20 > L8; L21 > L9; L22 > LI0; and LI 1 _< L20, L21, and L22). 5 Conclusion and Future Work This paper presents an analysis of different adjective fea- tures for predicting subjectivity, showing that tlmy are more precise than those previously used for this task. Wc establish that lexical semantic features uch as seman- tic orientation and gradability determine in large part the subjectivity status of sentences in which they appear. We also present an automatic meflmd for extracting radabil- ity values reliably, complementing earlier work on se- mantic orientation and dynamic adjectives. In addition to finding more precise features for auto- marie subjectivity recognition, this kind of analysis could help efforts to encode subjective features in ontologies such as those described in (Knight and Luk, 1994; Ma- hesh and Nirenburg, 1995; Hovy, 1998). These on- tologies are useful for many NLP tasks, such as ma- chine translation, word-sense disambiguation, and gen- eration. Some subjective features are included in exist- ing ontologies (for example, Mikrokosmos (Mahesh and 304 Nirenburg, 1995) includes atlitude slots). Our corpus- based methods could help in idenlifying more or exlend- ing their coverage. To be able to use automatic subjectivily recognition in texl-processing applications, good ch,cs o1 sub.iccliv- ity mttst be found. The features developed in lhis paper are not only good clues of subjectivity, lhey can be Men- tilied automatically from corpora (see (Hatzivassiloglou and McKeown, 1997), and Section 3 in the present pa- per). In fact, the results in ""Iable 3 show that the pre- dictability of the automatically determined gradability and polarity sets is better than or at least comparable to the predictability of the manually determined sets. Thus, tile oriented and gradable adjectives in the particular ap- plication genre can be idenlified fo,"" use in subjectivity recognition. Ou, efforts in this paper are largely exploratory, aim- ing to establish correlations among tim wlrious features examined. In related work, we have begun to incorporale the features developed herc into systems for recognizing flames and mining reviews in lnternel forums, extend- ing subjectivity judgments froth the sentence to the doc- ument level. In addition, we are seeking ways lo extend the orientation and gradability methods o that individual word occurrences, rather than word lypes, are character- ized as oriented or gradable. We also pla n l{7 incorpo- rate the new features presented here in machine learning models for tile prediction of subjectivity (e.g., (Wiebe ct al., 1999)) and lest lheir interaclions wilh olhcr proposed features. Acknowledglnents This research was SUl~ported in part by the National Sci- ence Foundation under grant number IIS-9817434, and by |he Of lice of Nawtl Research under grant number N00014-95-1-0776. Any opinions, tindings, or recom- mendations a,e those of tile authors, and do not neces- sarily rellect the views of the above agencies. References I)ouglas M. Bates and 1)onald G. Watts. NoMi~> ear Regression Analysis and its Applicatiolls. Wiley, New York. Edwin L. Battistella. Markedness: 7he Evahiative Siq~etwtructure qfLanguage. State University of New York Press, Albany, New York. Rebecca Bruce and ,lanyce Wiebe. Recognizing subjectivity: A case study of rllanual tagging. Natural Language E, gineering, 6(2). Kenneth W. Church. A stochastic paris p,ogranl and noun phrase parser for unrestricted text. In Pro- ceedings of the Second Co,ference o, Applied Natu- ral Language Processing (ANLP-88), pages 136-143, Austin, Texas, February. Association for Computa- tional Linguistics. Statistical Methods for Rates and lmportions. Wiley, New York, 2rid edition. Vasileios Hatzivassiloglou and Kathlcen R. McKeown. Predicting tile semantic orientation of adjec- tives. In Pmeeedi,gs o[ the 35th Annual Meeting q/ the ACL and the 8th Col!/erence o/ rite Europeall Ch(q)ter of the ACL, pages 174-181, Madrid, Spain, July. Association re,"" Computational Linguistics. Combining and slandardizing large-scale practical ontologies for machine lransla- lion and other uses. In Proceedings of the 1st Interna- tional Conference on Language Resources and Evaht- alien (LREC), Granada, Spain. Jerrold J. Kalz. Sema,tic Theory. Harper and Row, New York. Kevin Knight and Steve K. Luk. Building a large- scale knowledge base liw machine hanslation. Ill Pro- ceedi,gs o["" the 12th Natio,al Co,ference o, Artifi- cial l,telli,q,e, ce (AAAI-94), w)lume 1, pages 773-778, Sealtlc, Washinglon, July-Augt, st. American Associ- ation for Artificial Intelligence. Adrienne Lehrer. Sema, tic l,}elds and Lexical Structztre. North Holland, Amster&tm and New York. Sema,tics, volume 1. Cambridge University Press, Cambridge, England. K. Mahesh and S. Nirenburg. A siluated ontol- ogy for practical NLP. In Pmceedi,gs of the Work- shol~ oil Basic Ontological Issues in Knowledge Shar- ing, 14th lntenmtio,al.loi, t Co,ference oil Artificial Intelligence (LICAI-95), MontrEal, Canada, Augusl. Milchell E /Vlarcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large aunotaled cor- pus of Fmglish: the Penn Treebank. Coml;tttatioltal Lin,~?uistics, 19(2):313-330, June. I~tandolph Quirk, Sidney Grecnbaum, Geoffrey l,eech, alld Jall Svartvik. A Complvhe,sive Grammar elthe English l.cmguage. Longman, London and New York. Sanlner and Diane E. l)uffy. The Statis- tical Analysis of Discrete Data. Springer-Verlag, New York. ()n grading: A study ill semantics. l~hilosol;hy qfScie,ce, 2:93-116. Reprinted in (Sapir, 1949). Selected Wiqtings i, Language, Culture and Personality. University of California Press, Be,keley, California. Edited by David G. Mat> delbat, m. John M. Sinclair (editor in chiet). Collins COBU1LD English Language Dictionary. Collins, London. J. Wiebe, R. Bruce, and T. OHara. Develop- ment and use of a gold standard ata set for subjec- tivity classilieations. In Proceedings of tile 37th An- total Meeting of the Association for Computational Li,guistics (ACL-99), pages 246-253, Universily of Maryhmd, June.","Effects Of Adjective Orientation And Gradability On Sentence Subjectivity
Subjectivity is a pragmatic, sentence-level feature that has important implications for text processing applications such as information extraction and information retrieval.
We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classifier, and establish that they are strong predictors of subjectivity.
A novel trainable method that statistically combines two indicators of gradability is presented and evaluated, complementing existing automatic techniques for assigning orientation labels.
Unlike nouns, many adjectives are inherently subjective, and the number of adjectives in texts correlates with human judgements of their subjectivity.
We report a statistical correlation between the number of adjectives in a text and human judgments of subjectivity.
We show that automatically detected gradable adjectives are a useful feature for subjectivity classification.
","The task ad- dressed here comes to the fore in other genres, especially news reporting and lnternet lorums, in which opinions of various agents are expressed and where subjectivity judgements couht help in recognizing inllammatory rues- sages (""llanles) and mining online sources for product reviews.Aulomal- ically learning features of this type from hugc corpora allows the construction or augmentation of lexicons, and the assignment of scmanlic htbcls lo words and phrases in running text.The computatiomtl task addressed here is to distinguish sentences used to present opinions and other tbrms of subjectivity (suOjective sen",0.7917137742042542,0.8233988285064697,0.8072454929351807
"Minimized Models for Unsupervised Part-of-Speech Tagging We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings. In recent years, we have seen increased interest in using unsupervised methods for attacking different NLP tasks like part-of-speech (POS) tagging. The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods. In this paper, we develop new methods for unsupervised part-of-speech tagging. We adopt the problem formulation of Merialdo (1994), in which we are given a raw word sequence and a dictionary of legal tags for each word type. The goal is to tag each word token so as to maximize accuracy against a gold tag sequence. Whether this is a realistic problem set-up is arguable, but an interesting collection of methods and results has accumulated around it, and these can be clearly compared with one another. We use the standard test set for this task, a 24,115-word subset of the Penn Treebank, for which a gold tag sequence is available. There are 5,878 word types in this test set. We use the standard tag dictionary, consisting of 57,388 word/tag pairs derived from the entire Penn Treebank.1 8,910 dictionary entries are relevant to the 5,878 word types in the test set. Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data. There are 45 distinct grammatical tags. In this set-up, there are no unknown words. Figure 1 shows prior results for this problem. While the methods are quite different, they all make use of two common model elements. One is a probabilistic n-gram tag model P(ti|ti−n+1...ti−1), which we call the grammar. The other is a probabilistic word-given-tag model P(wi|ti), which we call the dictionary. The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: Goldwater and Griffiths (2007) report 74.5% accuracy for EM with a 3-gram tag model, which we confirm by replication. They improve this to 83.9% by employing a fully Bayesian approach which integrates over all possible parameter values, rather than estimating a single distribution. They further improve this to 86.8% by using priors that favor sparse distributions. Smith and Eisner (2005) employ a contrastive estimation technique, in which they automatically generate negative examples and use CRF training. In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes. They show considerable improvements in tagging accuracy when using a coarser-grained version (with 17-tags) of the tag set from the Penn Treebank. Goldberg et al. (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. They use language specific information (like word contexts, syntax and morphology) for learning initial P(t|w) distributions and also use linguistic knowledge to apply constraints on the tag sequences allowed by their models (e.g., the tag sequence “V V” is disallowed). Also, they make other manual adjustments to reduce noise from the word/tag dictionary (e.g., reducing the number of tags for “the” from six to just one). In contrast, we keep all the original dictionary entries derived from the Penn Treebank data for our experiments. The literature omits one other baseline, which is EM with a 2-gram tag model. Here we obtain 81.7% accuracy, which is better than the 3-gram model. It seems that EM with a 3-gram tag model runs amok with its freedom. For the rest of this paper, we will limit ourselves to a 2-gram tag model. We analyze the tag sequence output produced by EM and try to see where EM goes wrong. The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. The Bayesian methods overcome this effect by using priors which favor sparser distributions. But it is not easy to model such priors into EM learning. As a result, EM exploits a lot of rare tags (like FW = foreign word, or SYM = symbol) and assigns them to common word types (in, of, etc.). We can compare the tag assignments from the gold tagging and the EM tagging (Viterbi tag sequence). The table below shows tag assignments (and their counts in parentheses) for a few word types which occur frequently in the test corpus. We see how the rare tag labels (like FW, SYM, etc.) are abused by EM. As a result, many word tokens which occur very frequently in the corpus are incorrectly tagged with rare tags in the EM tagging output. We also look at things more globally. We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence. We call this the observed grammar size, and it is 915. That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 x 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we sequence. Here, we show a sample word sequence and the corresponding IP network generated for that sequence. can compare it with the gold tagging’s observed grammar size, which is 760. So we can safely say that EM is learning a grammar that is too big, still abusing its freedom. Bayesian sparse priors aim to create small models. We take a different tack in the paper and directly ask: What is the smallest model that explains the text? Our approach is related to minimum description length (MDL). We formulate our question precisely by asking which tag sequence (of the 106425 available) has the smallest observed grammar size. The answer is 459. That is, there exists a tag sequence that contains 459 distinct tag bigrams, and no other tag sequence contains fewer. We obtain this answer by formulating the problem in an integer programming (IP) framework. Figure 2 illustrates this with a small sample word sequence. We create a network of possible taggings, and we assign a binary variable to each link in the network. We create constraints to ensure that those link variables receiving a value of 1 form a left-to-right path through the tagging network, and that all other link variables receive a value of 0. We accomplish this by requiring the sum of the links entering each node to equal to the sum of the links leaving each node. We also create variables for every possible tag bigram and word/tag dictionary entry. We constrain link variable assignments to respect those grammar and dictionary variables. For example, we do not allow a link variable to “activate” unless the corresponding grammar variable is also “activated”. Finally, we add an objective function that minimizes the number of grammar variables that are assigned a value of 1. Figure 3 shows the IP solution for the example word sequence from Figure 2. Of course, a small grammar size does not necessarily correlate with higher tagging accuracy. For the small toy example shown in Figure 3, the correct tagging is “PRO AUX V . PRO V” (with 5 tag pairs), whereas the IP tries to minimize the grammar size and picks another solution instead. For solving the integer program, we use CPLEX software (a commercial IP solver package). Alternatively, there are other programs such as lp solve, which are free and publicly available for use. Once we create an integer program for the full test corpus, and pass it to CPLEX, the solver returns an sponding grammar sizes for the sample word sequence from Figure 2 using the given dictionary and grammar. The IP solver finds the smallest grammar set that can explain the given word sequence. In this example, there exist two solutions that each contain only 4 tag pair entries, and IP returns one of them. objective function value of 459.3 CPLEX also returns a tag sequence via assignments to the link variables. However, there are actually 104378 tag sequences compatible with the 459-sized grammar, and our IP solver just selects one at random. We find that of all those tag sequences, the worst gives an accuracy of 50.8%, and the best gives an accuracy of 90.3%. We also note that CPLEX takes 320 seconds to return the optimal solution for the integer program corresponding to this particular test data (24,115 tokens with the 45-tag set). It might be interesting to see how the performance of the IP method (in terms of time complexity) is affected when scaling up to larger data and bigger tagsets. We leave this as part of future work. But we do note that it is possible to obtain less than optimal solutions faster by interrupting the CPLEX solver. Our IP formulation can find us a small model, but it does not attempt to fit the model to the data. Fortunately, we can use EM for that. We still give EM the full word/tag dictionary, but now we constrain its initial grammar model to the 459 tag bigrams identified by IP. Starting with uniform probabilities, EM finds a tagging that is 84.5% accurate, substantially better than the 81.7% originally obtained with the fully-connected grammar. So we see a benefit to our explicit small-model approach. While EM does not find the most accurate 3Note that the grammar identified by IP is not uniquely minimal. For the same word sequence, there exist other minimal grammars having the same size (459 entries). In our experiments, we choose the first solution returned by CPLEX. sequence consistent with the IP grammar (90.3%), it finds a relatively good one. The IP+EM tagging (with 84.5% accuracy) has some interesting properties. First, the dictionary we observe from the tagging is of higher quality (with fewer spurious tagging assignments) than the one we observe from the original EM tagging. Figure 4 shows some examples. We also measure the quality of the two observed grammars/dictionaries by computing their precision and recall against the grammar/dictionary we observe in the gold tagging.4 We find that precision of the observed grammar increases from 0.73 (EM) to 0.94 (IP+EM). In addition to removing many bad tag bigrams from the grammar, IP minimization also removes some of the good ones, leading to lower recall (EM = 0.87, IP+EM = 0.57). In the case of the observed dictionary, using a smaller grammar model does not affect the precision (EM = 0.91, IP+EM = 0.89) or recall (EM = 0.89, IP+EM = 0.89). During EM training, the smaller grammar with fewer bad tag bigrams helps to restrict the dictionary model from making too many bad choices that EM made earlier. Here are a few examples of bad dictionary entries that get removed when we use the minimized grammar for EM training: in FW a SYM of RP In RBR During EM training, the minimized grammar helps to eliminate many incorrect entries (i.e., zero out model parameters) from the dictionary, thereby yielding an improved dictionary model. So using the minimized grammar (which has higher precision) helps to improve the quality of the chosen dictionary (examples shown in Figure 4). This in turn helps improve the tagging accuracy from 81.7% to 84.5%. It is clear that the IP-constrained grammar is a better choice to run EM on than the full grammar. Note that we used a very small IP-grammar (containing only 459 tag bigrams) during EM training. In the process of minimizing the grammar size, IP ends up removing many good tag bigrams from our grammar set (as seen from the low measured recall of 0.57 for the observed grammar). Next, we proceed to recover some good tag bigrams and expand the grammar in a restricted fashion by making use of the higher-quality dictionary produced by the IP+EM method. We now run EM again on the full grammar (all possible tag bigrams) in combination with this good dictionary (containing fewer entries than the full dictionary). Unlike the original training with full grammar, where EM could choose any tag bigram, now the choice of grammar entries is constrained by the good dictionary model that we provide EM with. This allows EM to recover some of the good tag pairs, and results in a good grammardictionary combination that yields better tagging performance. With these improvements in mind, we embark on an alternating scheme to find better models and taggings. We run EM for multiple passes, and in each pass we alternately constrain either the grammar model or the dictionary model. The procedure is simple and proceeds as follows: We notice significant gains in tagging performance when applying this technique. The tagging accuracy increases at each step and finally settles at a high of 91.6%, which outperforms the existing state-of-the-art systems for the 45-tag set. The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al. (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. Figure 5 shows the tagging performance achieved at each step. We found that it is the elimination of incorrect entries from the dictionary (and grammar) and not necessarily the initialization weights from previous EM training, that results in the tagging improvements. Initializing the last trained dictionary or grammar at each step with uniform weights also yields the same tagging improvements as shown in Figure 5. We find that the observed grammar also improves, growing from 459 entries to 603 entries, with precision increasing from 0.94 to 0.96, and recall increasing from 0.57 to 0.76. The figure also shows the model’s internal grammar and dictionary sizes. Figure 6 and 7 show how the precision/recall of the observed grammar and dictionary varies for different models from Figure 5. In the case of the observed grammar (Figure 6), precision increases at each step, whereas recall drops initially (owing to the grammar minimization) but then picks up again. The precision/recall of the observed dictionary on the other hand, is not affected by much. Multiple random restarts for EM, while not often emphasized in the literature, are key in this domain. Recall that our original EM tagging with a fully-connected 2-gram tag model was 81.7% accurate. When we execute 100 random restarts and select the model with the highest data likelihood, we get 83.8% accuracy. Likewise, when we extend our alternating EM scheme to 100 random restarts at each step, we improve our tagging accuracy from 91.6% to 91.8% (Figure 8). As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models. Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al. (2008). But previous results from Smith and Eisner (2005) and Goldwater and Griffiths (2007) show that their models do not benefit from using more unlabeled training data. Because EM is efficient, we can extend our word-sequence trainModel 1 Model 2 Model 3 Model 4 Model 5 ing data from the 24,115-token set to the entire Penn Treebank (973k tokens). We run EM training again for Model 5 (the best model from Figure 5) but this time using 973k word tokens, and further increase our accuracy to 92.3%. This is our final result on the 45-tagset, and we note that it is higher than previously reported results. Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008). Their systems were shown to obtain considerable improvements in accuracy when using a 17-tagset (a coarsergrained version of the tag labels from the Penn Treebank) instead of the 45-tagset. When tagging the same standard test corpus with the smaller 17-tagset, our method is able to achieve a substantially high accuracy of 96.8%, which is the best result reported so far on this task. The table in Figure 9 shows a comparison of different systems for which tagging accuracies have been reported previously for the 17-tagset case (Goldberg et al., 2008). The first row in the table compares tagging results when using a full dictionary (i.e., a lexicon containing entries for 49,206 word types). The InitEM-HMM system from Goldberg et al. (2008) reports an accuracy of 93.8%, followed by the LDA+AC model (Latent Dirichlet Allocation model with a strong Ambiguity Class component) from Toutanova and Johnson (2008). In comparison, the Bayesian HMM (BHMM) model from Goldwater et al. (2007) and the CE+spl model (Contrastive Estimation with a spelling model) from Smith and Eisner (2005) report lower accuracies (87.3% and 88.7%, respectively). Our system (IP+EM) which uses integer programming and EM, gets the highest accuracy (96.8%). The accuracy numbers reported for Init-HMM and LDA+AC are for models that are trained on all the available unlabeled data from the Penn Treebank. The IP+EM models used in the 17-tagset experiments reported here were not trained on the entire Penn Treebank, but instead used a smaller section containing 77,963 tokens for estimating model parameters. We also include the accuracies for our IP+EM model when using only the 24,115 token test corpus for EM estimation (shown within parenthesis in second column of the table in Figure 9). We find that our performance does not degrade when the parameter estimation is done using less data, and our model still achieves a high accuracy of 96.8%. The literature also includes results reported in a different setting for the tagging problem. In some scenarios, a complete dictionary with entries for all word types may not be readily available to us and instead, we might be provided with an incomplete dictionary that contains entries for only frequent word types. In such cases, any word not appearing in the dictionary will be treated as an unknown word, and can be labeled with any of the tags from given tagset (i.e., for every unknown word, there are 17 tag possibilities). Some previous approaches (Toutanova and Johnson, 2008; Goldberg et al., 2008) handle unknown words explicitly using ambiguity class components conditioned on various morphological features, and this has shown to produce good tagging results, especially when dealing with incomplete dictionaries. We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word. We first identify the top-100 suffixes (up to 3 characters) for words in the dictionary. Using the word/tag pairs from the dictionary, we train a simple probabilistic model that predicts the tag given a particular suffix (e.g., P(VBG I ing) = 0.97, P(N I ing) = 0.0001, ...). Next, for every unknown word “w”, the trained P(tag I suffix) model is used to predict the top 3 tag possibilities for “w” (using only its suffix information), and subsequently this word along with its 3 tags are added as a new entry to the lexicon. We do this for every unknown word, and eventually we have a dictionary containing entries for all the words. Once the completed lexicon (containing both correct entries for words in the lexicon and the predicted entries for unknown words) is available, we follow the same methodology from Sections 3 and 4 using integer programming to minimize the size of the grammar and then applying EM to estimate parameter values. Figure 9 shows comparative results for the 17tagset case when the dictionary is incomplete. The second and third rows in the table shows tagging accuracies for different systems when a cutoff of 2 (i.e., all word types that occur with frequency counts < 2 in the test corpus are removed) and a cutoff of 3 (i.e., all word types occurring with frequency counts < 3 in the test corpus are removed) is applied to the dictionary. This yields lexicons containing 2,141 and 1,249 words respectively, which are much smaller compared to the original 49,206 word dictionary. As the results in Figure 9 illustrate, the IP+EM method clearly does better than all the other systems except for the LDA+AC model. The LDA+AC model from Toutanova and Johnson (2008) has a strong ambiguity class component and uses more features to handle the unknown words better, and this contributes to the slightly higher performance in the incomplete dictionary cases, when compared to the IP+EM model. The method proposed in this paper is simple— once an integer program is produced, there are solvers available which directly give us the solution. In addition, we do not require any complex parameter estimation techniques; we train our models using simple EM, which proves to be efficient for this task. While some previous methods introduced for the same task have achieved big tagging improvements using additional linguistic knowledge or manual supervision, our models are not provided with any additional information. Figure 10 illustrates for the 45-tag set some of the common mistakes that our best tagging model (92.3%) makes. In some cases, the model actually gets a reasonable tagging but is penalized perhaps unfairly. For example, “to” is tagged as IN by our model sometimes when it occurs in the context of a preposition, whereas in the gold tagging it is always tagged as TO. The model also gets penalized for tagging the word “U.S.” as an adjective (JJ), which might be considered valid in some cases such as “the U.S. State Department”. In other cases, the model clearly produces incorrect tags (e.g., “New” gets tagged incorrectly as NNPS). Our method resembles the classic Minimum Description Length (MDL) approach for model selection (Barron et al., 1998). In MDL, there is a single objective function to (1) maximize the likelihood of observing the data, and at the same time (2) minimize the length of the model description (which depends on the model size). However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results. In the past, only a few approaches utilizing MDL have been shown to work for natural language applications. These approaches employ heuristic search methods with MDL for the task of unsupervised learning of morphology of natural languages (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2005). The method proposed in this paper is the first application of the MDL idea to POS tagging, and the first to use an integer programming formulation rather than heuristic search techniques. We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007). We presented a novel method for attacking dictionary-based unsupervised part-of-speech tagging. Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset. The method works by explicitly minimizing the grammar size using integer programming, and then using EM to estimate parameter values. The entire process is fully automated and yields better performance than any existing state-of-the-art system, even though our models were not provided with any additional linguistic knowledge (for example, explicit syntactic constraints to avoid certain tag combinations such as “V V”, etc.). However, it is easy to model some of these linguistic constraints (both at the local and global levels) directly using integer programming, and this may result in further improvements and lead to new possibilities for future research. For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task.","Minimized Models for Unsupervised Part-of-Speech Tagging
We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.
We achieve the best results (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data.
We propose a rigid mechanism for modeling sparsity that minimizes the size of tagging grammar as measured by the number of transition types.
To avoid the need for manually pruning the tag dictionary, we propose that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary.
","In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes.The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence:Goldwater and Griffiths (2007) report 74.5% accuracy for EM with a 3-gram tag model, which we confirm by replication.We use the standard test set for this task, a 24,115-word subset of the Penn Treebank, for which",0.8312948942184448,0.8340266942977905,0.8326585292816162
"A Polynomial-Time Algorithm For Statistical Machine Translation Hong Kong's stabilize boom is us life styles's pillar. Our prosperity and stability underpin our way of life. 44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.) Hong Kong's economic foreground with China, particular Guangdong province's economic foreground vitally interrelated. Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g. (WO win quin zhi chi ta de yi jian.) I absolutely uphold his views. I fully support his views. Mt (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.) These arrangements can enforce us future kept financial stabilization's competency. These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. wa zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.) However, I now can certainty's say, will provide for us attain various dominant goal necessary's current expenditure. The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets. Figure 4: Example translation outputs. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3. We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices. For comparison, the accuracies from the A*-based systems are also shown. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. On the other hand, the new algorithm has indeed proven to be much faster. At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems. However, the order-of-magnitude improvements are immediately apparent. In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations). In contrast the new algorithm generally takes less than one minute—usually substantially less—with no special optimization of the code. 6 Conclusion We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. 157 We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used. But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions. The generalized version permits such strategies. Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted. Acknowledgements Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan. Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance. The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English. The underlying generative model, shown in Figure 1, contains a stochastic English sentence generator whose output is &quot;corrupted&quot; by the translation channel to produce Chinese sentences. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under generative models. For each Chinese sentence e that is to be translated, the system must attempt to find the English sentence e* such that: In the IBM model, the search for the optimal e* is performed using a best-first heuristic &quot;stack search&quot; similar to A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* fashion. This price is paid for the robustness that is obtained by using very flexible language and translation models. The language model allows sentences of arbitrary order and the translation model allows arbitrary word-order permutation. The models employ no structural constraints, relying instead on probability parameters to assign low probabilities to implausible sentences. This exhaustive space, together with massive number of parameters, permits greater modeling accuracy. But while accuracy is enhanced, translation efficiency suffers due to the lack of structure in the hypothesis space. The translation channel is characterized by two sets of parameters: translation and alignment probabilities.' The translation probabilities describe lexical substitution, while alignment probabilities describe word-order permutation. The key problem is that the formulation of alignment probabilities a(il j, V, T) permits the Chinese word in position j of a length-T sentence to map to any position i of a length-V English sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. Note there are no explicit linguistic grammars in the IBM channel model. Useful methods do exist for incorporating constraints fed in from other preprocessing modules, and some of these modules do employ linguistic grammars. For instance, we previously reported a method for improving search times in channel translation models that exploits bracketing information (Wu and Ng, 1995). If any brackets for the Chinese sentence can be supplied as additional input information, produced for example by a preprocessing stage, a modified version of the A*based algorithm can follow the brackets to guide the search heuristically. This strategy appears to produces moderate improvements in search speed and slightly better translations. Such linguistic-preprocessing techniques could 'Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &quot;Model 2&quot;; search costs for the more complex models are correspondingly higher. also be used with the new model described below, but the issue is independent of our focus here. In this paper we address the underlying assumptions of core channel model itself which does not directly use linguistic structure. A slightly different model is employed for a word alignment application by Dagan et al. (Dagan, Church, and Gale, 1993). Instead of alignment probabilities, offset probabilities o(k) are employed, where k is essentially the positional distance between the English words aligned to two adjacent Chinese words: wherepr e v is the position of the immediately preceding Chinese word and N is a constant that normalizes for average sentence lengths in different languages. The motivation is that words that are close to each other in the Chinese sentence should tend to be close in the English sentence as well. The size of the parameter set is greatly reduced from the x x (TI x I VI parameters of the alignment probabilities, down to a small set of jkl parameters. However, the search space remains the same. The A*-style stack-decoding approach is in some ways a carryover from the speech recognition architectures that inspired the channel translation model. It has proven highly effective for speech recognition in both accuracy and speed, where the search space contains no order variation since the acoustic and text streams can be assumed to be linearly aligned. But in contrast, for translation models the stack search alone does not adequately compensate for the combinatorially more complex space that results from permitting arbitrary order variations. Indeed, the stack-decoding approach remains impractically slow for translation, and has not achieved the same kind of speed as for speech recognition. The model we describe in this paper, like Dagan et al. 's model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x I y, where x is a Chinese word and y is an English translation of x.2 Any parse tree thus generates two strings, one on the Chinese stream and one on the English stream. Thus, the tree: An additional mechanism accommodates a conservative degree of word-order variation between the two languages. With each production of the grammar is associated either a straight orientation or an inverted orientation, respectively denoted as follows: In the case of a production with straight orientation, the right-hand-side symbols are visited leftto-right for both the Chinese and English streams. But for a production with inverted orientation, the 2 Readers of the papers cited above should note that we have switched the roles of English and Chinese here, which helps simplify the presentation of the new translation algorithm. right-hand-side symbols are visited left-to-right for Chinese and right-to-left for English. Thus, the tree: In the special case of BTGs which are employed in the model presented below, there is only one undifferentiated nonterminal category (aside from the start symbol). Designating this category A, this means all non-lexical productions are of one of these two forms: The degree of word-order flexibility is the critical point. BTGs make a favorable trade-off between efficiency and expressiveness: constraints are strong enough to allow algorithms to operate efficiently, but without so much loss of expressiveness as to hinder useful translation. We summarize here; details are given elsewhere (Wu, 1995b). With regard to efficiency, Figure 2 demonstrates the kind of reduction that BTGs obtain in the space of possible alignments. The number of possible alignments, compared against the unrestricted case where any English word may align to any Chinese position, drops off dramatically for strings longer than four words. (This table makes the simplification of counting only 1-1 matchings and is merely representative.) With regard to expressiveness, we believe that almost all variation in the order of arguments in a syntactic frame can be accommodated.3 Syntactic frames generally contain four or fewer subconstituents. Figure 2 shows that for the case of four subconstituents, BTGs permit 22 out of the 24 possible alignments. The only prohibited arrangements are &quot;inside-out&quot; transformations (Wu, 1995b), which we have been unable to find any examples of in our corpus. Moreover, extremely distorted alignments can be handled by BTGs (Wu, 1995c), without resorting to the unrestricted-alignment model. The translation expressiveness of BTGs is by no means perfect. They are nonetheless proving very useful in applications and are substantially more feasible than previous models. In our previous corpus analysis applications, any expressiveness limitations were easily tolerable since degradation was graceful. In the present translation application, any expressiveness limitation simply means that certain translations are not considered. For the remainder of the paper, we take advantage of a convenient normal-form theorem (Wu, 1995a) that allows us to assume without loss of generality that the BTG only contains the binary-branching form for the non-lexical productions.4 A first approach to improving the translation search is to limit the allowed word alignment patterns to those permitted by a BTG. In this case, Equation (2) is kept as the objective function and the translation channel can be parameterized similarly to Dagan ei al. (Dagan, Church, and Gale, 1993). The effect of the BTG restriction is just to constrain the shapes of the word-order distortions. A BTG rather than ITG is used since, as we discussed earlier, pure channel translation models operate without explicit grammars, providing no constituent categories around which a more sophisticated ITG could be structured. But the structural constraints of the BTG can improve search efficiency, even without differentiated constituent categories. Just as in the baseline system, we rely on the language and translation models to take up the slack in place of an explicit grammar. In this approach, an 0(T7) algorithm similar to the one described later can be constructed to replace A* search. However we do not feel it is worth preserving offset (or alignment or distortion) parameters simply for the sake of preserving the original translation channel model. These parameterizations were only intended to crudely model word-order variation. Instead, the BTG itself can be used directly to probabilistically rank alternative alignments, as described next. The second possibility is to use a stochastic bracketing transduction grammar (SBTG) in the channel model, replacing the translation model altogether. In a SBTG, a probability is associated with each production. Thus for the normal-form BTG, we have: The translation lexicon is encoded in productions of the third kind. The latter two kinds of productions allow words of either Chinese or English to go unmatched. The SBTG assigns a probability Pr(c, e, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(e) and integrating out Pr(q) to give Pr(cle) in Equation (2). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation To complete the picture we add a bigram model get—let = g(e.i 1e3_ Pr(e). 1) for the English language model Offset, alignment, or distortion parameters are entirely eliminated. A large part of the implicit function of such parameters—to prevent alignments where too many frame arguments become separated—is rendered unnecessary by the BTG's structural constraints, which prohibit many such configurations altogether. Another part of the parameters' purpose is subsumed by the SBTG's probabilities all and a0, which can be set to prefer straight or inverted orientation depending on the language pair. As in the original models, the language model heavily influences the remaining ordering decisions. Matters are complicated by the presence of the bigram model in the objective function (which wordalignment models, as opposed to translation models, do not need to deal with). As in our word-alignment model, the translation algorithm optimizes Equation (4) via dynamic programming, similar to chart parsing (Earley, 1970) but with a probabilistic objective function as for HMMs (Viterbi, 1967). But unlike the word-alignment model, to accommodate the bigram model we introduce indexes in the recurrence not only on subtrees over the source Chinese string, but also on the delimiting words of the target English substrings. Another feature of the algorithm is that segmentation of the Chinese input sentence is performed in parallel with the translation search. Conventional architectures for Chinese NLP generally attempt to identify word boundaries as a preprocessing stage.5 Whenever the segmentation preprocessor prematurely commits to an inappropriate segmentation, difficulties are created for later stages. This problem is particularly acute for translation, since the decision as to whether to regard a sequence as a single unit depends on whether its components can be translated compositionally. This in turn often depends on what the target language is. In other words, the Chinese cannot be appropriately segmented except with respect to the target language of translation—a task-driven definition of correct segmentation. The algorithm is given below. A few remarks about the notation used: cs,.t denotes the subsequence of Chinese tokens cs+1, cs+2, , ct. We use E(s..t) to denote the set of English words that are translations the Chinese word created by taking all tokens in c, t together. E(s, t) denotes the set of English words that are translations of any of the Chinese words anywhere within e, ..t. Note also that we assume the explicit sentence-start and sentenceend tokens co = <s> and cT+1 = <Is>, which makes the algorithm description more parsimonious. Finally, the argmax operator is generalized to vector notation to accomodate multiple indices. = 0 if etyz > Olslyz and 4yz > 451)tyz where of the parse tree to qo =_- (-1,T— 1, <s>, </ s>). The remaining descendants in the optimal parse tree are then given recursively for any q = (s,t,y, z) by: a probabilistic optimization problem. But perhaps most importantly, our goal is to constrain as tightly as possible the space of possible transduction relationships between two languages with fixed wordorder, making no other language-specific assumptions; we are thus driven to seek a kind of languageuniversal property. In contrast, the ID/LP work was directed at parsing a single language with free word-order. As a consequence, it would be necessary to enumerate a specific set of linear-precedence (LP) relations for the language, and moreover the immediate-dominance (ID) productions would typically be more complex than binary-branching. This significantly increases time complexity, compared to our BTG model. Although it is not mentioned in their paper, the time complexity for ID/LP parsing rises exponentially with the length of production right-hand-sides, due to the number of permutations. ITGs avoid this with their restriction to inversions, rather than permutations, and BTGs further minimize the grammar size. We have also confirmed empirically that our models would not be feasible under general permutations. Assume the number of translations per word is bounded by some constant. Then the maximum size of E(s,t) is proportional to t — s. The asymptotic time complexity for the translation algorithm is thus bounded by 0(T7). Note that in practice, actual performance is improved by the sparseness of the translation matrix. An interesting connection has been suggested to direct parsing for ID/LP grammars (Shieber, 1984), in which word-order variations would be accommodated by the parser, and related ideas for generation of free word-order languages in the TAG framework (Joshi, 1987). Our work differs from the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). The resulting English vocabulary is approximately 6,500 words and the Chinese vocabulary is approximately 5,500 words, with a many-to-many translation mapping averaging 2.25 Chinese translations per English word. Due to the unsupervised training, the translation lexicon contains noise and is only at about 86% percent weighted precision. With regard to accuracy, we merely wish to demonstrate that for statistical MT, accuracy is not significantly compromised by substituting our efficient optimization algorithm. It is not our purpose here to argue that accuracy can be increased with our model. No morphological processing has been used to correct the output, and until now we have only been testing with a bigram model trained on extremely limited samples. A coarse evaluation of (Xiang gang de an ding fan rong shi WO men sheng hu6 fang shi de zhi Hong Kong's stabilize boom is us life styles's pillar. Our prosperity and stability underpin our way of life. 44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.) Hong Kong's economic foreground with China, particular Guangdong province's economic foreground vitally interrelated. Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g. (WO win quin zhi chi ta de yi jian.) I absolutely uphold his views. I fully support his views. (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.) These arrangements can enforce us future kept financial stabilization's competency. These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. gu'o, wa xian zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.) However, I now can certainty's say, will provide for us attain various dominant goal necessary's current expenditure. The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3. We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices. For comparison, the accuracies from the A*-based systems are also shown. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. On the other hand, the new algorithm has indeed proven to be much faster. At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems. However, the order-of-magnitude improvements are immediately apparent. In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations). In contrast the new algorithm generally takes less than one minute—usually substantially less—with no special optimization of the code. We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used. But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions. The generalized version permits such strategies. Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted. Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan. Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance.","A Polynomial-Time Algorithm For Statistical Machine Translation
We introduce a polynomial-time algorithm for statistical machine translation.
This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures.
The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model.
The new algorithm in our experience yields major speed improvement with no significant loss of accuracy.
We test our algorithm on Chinese-English translation.
","We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT.We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models.Our economic future is inextricably bound up with China, and with Guangdong Province in particular.TWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. wa zai ke yl ken ding",0.8072926998138428,0.8494945168495178,0.8278561234474182
"An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text. When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied. This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways. We present an unsupervised stochastic model – the only resource we use is a morphological analyzer – which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language. We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation. We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step. Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets. Our method is applicable to other languages with affix morphology. Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text, according to the word context. In this work, we investigate morphological disambiguation in Modern Hebrew. We explore unsupervised learning method, which is more challenging than the supervised case. The main motivation for this approach is that despite the development of annotated corpora in Hebrew', there is still not enough data available for supervised training. The other reason, is that unsupervised methods can handle the dynamic nature of Modern Hebrew, as it evolves over time. In the case of English, because morphology is simpler, morphological disambiguation is generally covered under the task of part-of-speech tagging. The main morphological variations are embedded in the tag name (for example, Ns and Np for noun singular or plural). The tagging accuracy of supervised stochastic taggers is around 96%97% (Manning and Schutze, 1999, 10.6.1). Merialdo (1994) reports an accuracy of 86.6% for an unsupervised word-based HMM, trained on a corpus of 42,186 sentences (about 1M words), over a tag set of 159 different tags. Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tagset of 134 tags. With good initial conditions, such as good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27% and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2000 sentences of the training corpus are manually tagged. Modern Hebrew is characterized by rich morphology, with a high level of ambiguity. On average, in our corpus, the number of possible analyses per word reached 2.4 (in contrast to 1.4 for English). In Hebrew, several morphemes combine into a single word in both agglutinative and fusional ways. This results in a potentially high number of tags for each word. In contrast to English tag sets whose sizes range from 48 to 195, the number of tags for Hebrew, based on all combinations of the morphological attributes (part-of-speech, gender, number, person, tense, status, and the affixes' properties2), can grow theoretically to about 300,000 tags. In practice, we found only 1,934 tags in a corpus of news stories we gathered, which contains about 6M words. The large size of such a tag set (about 10 times larger than the most comprehensive English tag set) is problematic in term of data sparseness. Each morphological combination appears rarely, and more samples are required in order to learn the probabilistic model. In this paper, we hypothesize that the large set of morphological features of Hebrew words, should be modeled by a compact morpheme model, based on the segmented words (into prefix, baseform, and suffix). Our main result is that best performance is obtained when learning segmentation and morpheme tagging in one step, which is made possible by an appropriate text representation. Several works have dealt with Hebrew tagging in the past decade. In Hebrew, morphological analysis requires complex processing according to the rules of Hebrew word formation. The task of a morphological analyzer is to produce all possible analyses for a given word. Recent analyzers provide good performance and documentation of this process (Yona and Wintner, 2005; Segal, 2000). Morphological analyzers rely on a dictionary, and their performance is, therefore, impacted by the occurrence of unknown words. The task of a morphological disambiguation system is to pick the most likely analysis produced by an analyzer in the context of a full sentence. Levinger et al. (1995) developed a context-free method in order to acquire the morpho-lexical probabilities, from an untagged corpus. Their method handles the data sparseness problem by using a set of similar words for each word, built according to a set of rules. The rules produce variations of the morphological properties of the word analyses. Their tests indicate an accuracy of about 88% for context-free analysis selection based on the approximated analysis distribution. In tests we reproduced on a larger data set (30K tagged words), the accuracy is only 78.2%. In order to improve the results, the authors recommend merging their method together with other morphological disambiguation methods – which is the approach we pursue in this work. Levinger's morphological disambiguation system (Levinger, 1992) combines the above approximated probabilities with an expert system, based on a manual set of 16 syntactic constraints . In the first phase, the expert system is applied, dis24–86). ambiguating 35% of the ambiguous words with an accuracy of 99.6%. In order to increase the applicability of the disambiguation, approximated probabilities are used for words that were not disambiguated in the first stage. Finally, the expert system is used again over the new probabilities that were set in the previous stage. Levinger reports an accuracy of about 94% for disambiguation of 85% of the words in the text (overall 80% disambiguation). The system was also applied to prune out the least likely analyses in a corpus but without, necessarily, selecting a single analysis for each word. For this task, an accuracy of 94% was reported while reducing 92% of the ambiguous analyses. Carmel and Maarek (1999) use the fact that on average 45% of the Hebrew words are unambiguous, to rank analyses, based on the number of disambiguated occurrences in the text, normalized by the total number of occurrences for each word. Their application – indexing for an information retrieval system – does not require all of the morphological attributes but only the lemma and the PoS of each word. As a result, for this case, 75% of the words remain with one analysis with 95% accuracy, 20% with two analyses and 5% with three analyses. Segal (2000) built a transformation-based tagger in the spirit of Brill (1995). In the first phase, the analyses of each word are ranked according to the frequencies of the possible lemmas and tags in a training corpus of about 5,000 words. Selection of the highest ranked analysis for each word gives an accuracy of 83% of the test text – which consists of about 1,000 words. In the second stage, a transformation learning algorithm is applied (in contrast to Brill, the observed transformations are not applied, but used for re-estimation of the word couples probabilities). After this stage, the accuracy is about 93%. The last stage uses a bottomup parser over a hand-crafted grammar with 150 rules, in order to select the analysis which causes the parsing to be more accurate. Segal reports an accuracy of 95%. Testing his system over a larger test corpus, gives poorer results: Lembersky (2001) reports an accuracy of about 85%. Bar-Haim et al. (2005) developed a word segmenter and PoS tagger for Hebrew. In their architecture, words are first segmented into morphemes, and then, as a second stage, these morphemes are tagged with PoS. The method proceeds in two sequential steps: segmentation into morphemes, then tagging over morphemes. The segmentation is based on an HMM and trained over a set of 30K annotated words. The segmentation step reaches an accuracy of 96.74%. PoS tagging, based on unsupervised estimation which combines a small annotated corpus with an untagged corpus of 340K words by using smoothing technique, gives an accuracy of 90.51%. As noted earlier, there is as yet no large scale Hebrew annotated corpus. We are in the process of developing such a corpus, and we have developed tagging guidelines (Elhadad et al., 2005) to define a comprehensive tag set, and assist human taggers achieve high agreement. The results discussed above should be taken as rough approximations of the real performance of the systems, until they can be re-evaluated on such a large scale corpus with a standard tag set. Arabic is a language with morphology quite similar to Hebrew. Theoretically, there might be 330,000 possible morphological tags, but in practice, Habash and Rambow (2005) extracted 2,200 different tags from their corpus, with an average number of 2 possible tags per word. As reported by Habash and Rambow, the first work on Arabic tagging which used a corpus for training and evaluation was the work of Diab et al. (2004). Habash and Rambow were the first to use a morphological analyzer as part of their tagger. They developed a supervised morphological disambiguator, based on training corpora of two sets of 120K words, which combines several classifiers of individual morphological features. The accuracy of their analyzer is 94.8% – 96.2% (depending on the test corpus). An unsupervised HMM model for dialectal Arabic (which is harder to be tagged than written Arabic), with accurracy of 69.83%, was presented by Duh and Kirchhoff (2005). Their supervised model, trained on a manually annotated corpus, reached an accuracy of 92.53%. Arabic morphology seems to be similar to Hebrew morphology, in term of complexity and data sparseness, but comparison of the performances of the baseline tagger used by Habash and Rambow – which selects the most frequent tag for a given word in the training corpus – for Hebrew and Arabic, shows some intriguing differences: 92.53% for Arabic and 71.85% for Hebrew. Furthermore, as mentioned above, even the use of a sophisticated context-free tagger, based on (Levinger et al., 1995), gives low accuracy of 78.2%. This might imply that, despite the similarities, morphological disambiguation in Hebrew might be harder than in Arabic. It could also mean that the tag set used for the Arabic corpora has not been adapted to the specific nature of Arabic morphology (a comment also made in (Habash and Rambow, 2005)). We propose an unsupervised morpheme-based HMM to address the data sparseness problem. In contrast to Bar-Haim et al., our model combines segmentation and morphological disambiguation, in parallel. The only resource we use in this work is a morphological analyzer. The analyzer itself can be generated from a word list and a morphological generation module, such as the HSpell wordlist (Har'el and Kenigsberg, 2004). The lexical items of word-based models are the words of the language. The implication of this decision is that both lexical and syntagmatic relations of the model, are based on a word-oriented tagset. With such a tagset, it must be possible to tag any word of the language with at least one tag. Let us consider, for instance, the Hebrew phrase bclm hn`im3, which contains two words. The word bclm has several possible morpheme segmentations and analyses as described in Table 1. In wordbased HMM, we consider such a phrase to be generated by a Markov process, based on the wordoriented tagset of N = 1934 tags/states and about M = 175K word types. Line W of Table 2 describes the size of a first-order word-based HMM, built over our corpus. In this model, we found 834 entries for the II vector (which models the distribution of tags in first position in sentences) out of possibly N = 1934, about 250K entries for the A matrix (which models the transition probabilities from tag to tag) out of possibly N2 ;::Li 3.7M, and about 300K entries for the B matrix (which models the emission probabilities from tag to word) out of possibly M · N ;::Li 350M. For the case of a secondorder HMM, the size of the A2 matrix (which models the transition probabilities from two tags to the third one), grows to about 7M entries, where the size of the B2 matrix (which models the emission probabilities from two tags to a word) is about 5M. Despite the sparseness of these matrices, the number of their entries is still high, since we model the whole set of features of the complex word forms. Let us assume, that the right segmentation for the sentence is provided to us – for example: b clm hn`im – as is the case for English text. In such a way, the observation is composed of morphemes, generated by a Markov process, based on a morpheme-based tagset. The size of such a tagset for Hebrew is about 200, where the size of the II,A,B,A2 and B2 matrices is reduced to 145, 16K, 140K, 700K, and 1.7M correspondingly, as described in line M of Table 2 – a reduction of 90% when compared with the size of a word-based model. The problem in this approach, is that ”someone” along the way, agglutinates the morphemes of each word leaving the observed morphemes uncertain. For example, the word bclm can be segmented in four different ways in Table 1, as indicated by the placement of the '-' in the Segmentation column, while the word hn`im can be segmented in two different ways. In the next section, we adapt the parameter estimation and the searching algorithms for such uncertain output observation. In contrast to standard HMM, the output observations of the above morpheme-based HMM are ambiguous. We adapted Baum-Welch (Baum, 1972) and Viterbi (Manning and Schutze, 1999, 9.3.2) algorithms for such uncertain observation. We first formalize the output representation and then describe the algorithms. Output Representation The learning and searching algorithms of HMM are based on the output sequence of the underlying Markov process. For the case of a morpheme-based model, the output sequence is uncertain – we don’t see the emitted morphemes but the words they form. If, for instance, the Markov process emitted the morphemes b clm h nim, we would see two words (bclm hn`im) instead. In order to handle the output ambiguity, we use static knowledge of how morphemes are combined into a word, such as the four known combinations of the word bclm, the two possible combinations of the word hn`im, and their possible tags within the original words. Based on this information, we encode the sentence into a structure that represents all the possible “readings” of the sentence, according to the possible morpheme combinations of the words, and their possible tags. The representation consists of a set of vectors, each vector containing the possible morphemes and their tags for each specific “time” (sequential position within the morpheme expansion of the words of the sentence). A morpheme is represented by a tuple (symbol, state, prev, next), where symbol denotes a morpheme, state is one possible tag for this morpheme, prev and next are sets of indexes, denoting the indexes of the morphemes (of the previous and the next vectors) that precede and follow the current morpheme in the overall lattice, representing the sentence. Fig. 2 describes the representation of the sentence bclm hn`im. An emission is denoted in this figure by its symbol, its state index, directed edges from its previous emissions, and directed edges to its next emissions. In order to meet the condition of Baum-Eagon inequality (Baum, 1972) that the polynomial P(O|µ) – which represents the probability of an observed sequence O given a model µ – be homogeneous, we must add a sequence of special EOS (end of sentence) symbols at the end of each path up to the last vector, so that all the paths reach the same length. The above text representation can be used to model multi-word expressions (MWEs). Consider the Hebrew sentence: hw' `wrk dyn gdwl, which can be interpreted as composed of 3 units (he lawyer great / he is a great lawyer) or as 4 units (he edits law big / he is editing an important legal decision). In order to select the correct interpretation, we must determine whether `wrk dyn is an MWE. This is another case of uncertain output observation, which can be represented by our text encoding, as done in Fig. 1. This representation seems to be expensive in term of the number of emissions per sentence. However, we observe in our data that most of the words have only one or two possible segmentations, and most of the segmentations consist of at most one affix. In practice, we found the average number of emissions per sentence in our corpus (where each symbol is counted as the number of its predecessor emissions) to be 455, where the average number of words per sentence is about 18. That is, the cost of operating over an ambiguous sentence representation increases the size of the sentence (from 18 to 455), but on the other hand, it reduces the probabilistic model by a factor of 10 (as discussed above). Morphological disambiguation over such a sequence of vectors of uncertain morphemes is similar to words extraction in automatic speech recognition (ASR)(Jurafsky and Martin, 2000, chp. 5,7). The states of the ASR model are phones, where each observation is a vector of spectral features. Given a sequence of observations for a sentence, the encoding – based on the lattice formed by the phones distribution of the observations, and the language model – searches for the set of words, made of phones, which maximizes the acoustic likelihood and the language model probabilities. In a similar manner, the supervised training of a speech recognizer combines a training corpus of speech wave files, together with word-transcription, and language model probabilities, in order to learn the phones model. There are two main differences between the typical ASR model and ours: (1) an ASR decoder deals with one aspect - segmentation of the observations into a set of words, where this segmentation can be modeled at several levels: subphones, phones and words. These levels can be trained individually (such as training a language model from a written corpus, and training the phones model for each word type, given transcripted wave file), and then combined together (in a hierarchical model). Morphological disambiguation over uncertain morphemes, on the other hand, deals with both morpheme segmentation and the tagging of each morpheme with its morphological features. Modeling morpheme segmentation, within a given word, without its morphology features would be insufficient. (2) The supervised resources of ASR are not available for morphological disambiguation: we don’t have a model of morphological features sequences (equivalent to the language model of ASR) nor a tagged corpus (equivalent to the transcripted wave files of ASR). These two differences require a design which combines the two dimensions of the problem, in order to support unsupervised learning (and searching) of morpheme sequences and their morphological features, simultaneously. Parameter Estimation We present a variation of the Baum-Welch algorithm (Baum, 1972) which operates over the lattice representation we have defined above. The algorithm starts with a probabilistic model µ (which can be chosen randomly or obtained from good initial conditions), and at each iteration, a new model µ� is derived in order to better explain the given output observations. For a given sentence, we define T as the number of words in the sentence, and T as the number of vectors of the output representation O = {ot},1 < t < T, where each item in the output is denoted by olt = (sym, state, prev, next),1 < t < T,1 < l < |ot|. We define a(t, l) as the probability to reach olt at time t, and 0(t, l) as the probability to end the sequence from olt. Fig. 3 describes the expectation and the maximization steps of the learning algorithm for a first-order HMM. The algorithm works in O(T) time complexity, where T is the total number of symbols in the output sequence encoding, where each symbol is counted as the size of its prev set. Searching for best state sequence The searching algorithm gets an observation sequence O and a probabilistic model µ, and looks for the best state sequence that generates the observation. We define S(t, l) as the probability of the best state sequence that leads to emission olt, and�(t, l) as the index of the emission at time t−1 that precedes olt in the best state sequence that leads to it. Fig. 4 describes the adaptation of the Viterbi (Manning and Schutze, 1999, 9.3.2) algorithm to our text representation for first-order HMM, which works in O(T) time. We ran a series of experiments on a Hebrew corpus to compare various approaches to the full morphological disambiguation and PoS tagging tasks. The training corpus is obtained from various newspaper sources and is characterized by the following statistics: 6M word occurrences, 178,580 distinct words, 64,541 distinct lemmas. Overall, the ambiguity level is 2.4 (average number of analyses per word). We tested the results on a test corpus, manually annotated by 2 taggers according to the guidelines we published and checked for agreement. The test corpus contains about 30K words. We compared two unsupervised models over this data set: Word model [W], and Morpheme model [M]. We also tested two different sets of initial conditions. Uniform distribution [Uniform]: For each word, each analysis provided by the analyzer is estimated with an equal likelihood. Context Free approximation [CF]: We applied the CF algorithm of Levinger et al. (1995) to estimate the likelihood of each analysis. Table 3 reports the results of full morphological disambiguation. For each morpheme and word models, three types of models were tested: [1] First-order HMM, [2-] Partial second-order HMM only state transitions were modeled (excluding B2 matrix), [2] Second-order HMM (including the B2 matrix). Analysis If we consider the tagger which selects the most probable morphological analysis for each word in the text, according to Levinger et al. (1995) approximations, with accuracy of 78.2%, as the baseline tagger, four steps of error reduction can be identified. (1) Contextual information: The simplest first-order word-based HMM with uniform initial conditions, achieves error reduction of 17.5% (78.2 – 82.01). (2) Initial conditions: Error reductions in the range: 11.5% – 37.8% (82.01 – 84.08 for word model 1, and 81.53 – 88.5 for morhpeme model 2-) were achieved by initializing the various models with context-free approximations. While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English - about 70% (79 – 94). The key difference (beside the unclear characteristic of Elworthy initial condition - since he made use of an annotated corpus) is the much higher quality of the uniform distribution for Hebrew. (3) Model order: The partial second-order HMM [2-] produced the best results for both word (85.75%) and morpheme (88.5%) models over the initial condition. The full second-order HMM [2] didn’t upgrade the accuracy of the partial second-order, but achieved the best results for the uniform distribution morpheme model. This is because the context-free approximation does not take into account the tag of the previous word, which is part of model 2. We believe that initializing the morpheme model over a small set of annotated corpus will set much stronger initial condition for this model. (4) Model type: The main result of this paper is the error reduction of the morpheme model with respect to the word model: about 19.3% (85.75 – 88.5). In addition, we apply the above models for the simpler task of segmentation and PoS tagging, as reported in Table 4. The task requires picking the correct morphemes of each word with their correct PoS (excluding all other morphological features). The best result for this task is obtained with the morpheme model 2: 92.32%. For this simpler task, the improvement brought by the morpheme model over the word model is less significant, but still consists of a 5% error reduction. Unknown words account for a significant chunk of the errors. Table 5 shows the distribution of errors contributed by unknown words (words that cannot be analyzed by the morphological analyzer). 7.5% of the words in the test corpus are unknown: 4% are not recognized at all by the morphological analyzer (marked as [None] in the table), and for 3.5%, the set of analyses proposed by the analyzer does not contain the correct analysis [Missing]. We extended the lexicon to include missing and none lexemes of the closed sets. In addition, we modified the analyzer to extract all possible segmentations of unknown words, with all the possible tags for the segmented affixes, where the remaining unknown baseforms are tagged as UK. The model was trained over this set. In the next phase, the corpus was automatically tagged, according to the trained model, in order to form a tag distribution for each unknown word, according to its context and its form. Finally, the tag for each unknown word were selected according to its tag distribution. This strategy accounts for about half of the 7.5% unknown words. Table 6 shows the confusion matrix for known words (5% and up). The key confusions can be attributed to linguistic properties of Modern Hebrew: most Hebrew proper names are also nouns (and they are not marked by capitalization) – which explains the PN/N confusion. The verb/noun and verb/adjective confusions are explained by the nature of the participle form in Hebrew (beinoni) – participles behave syntactically almost in an identical manner as nouns. In this work, we have introduced a new text encoding method that captures rules of word formation in a language with affixational morphology such as Hebrew. This text encoding method allows us to learn in parallel segmentation and tagging rules in an unsupervised manner, despite the high ambiguity level of the morphological data (average number of 2.4 analyses per word). Reported results on a large scale corpus (6M words) with fully unsupervised learning are 92.32% for PoS tagging and 88.5% for full morphological disambiguation. In this work, we used the backoff smoothing method, suggested by Thede and Harper (1999), with an extension of additive smoothing (Chen, 1996, 2.2.1) for the lexical probabilities (B and B2 matrices). To complete this study, we are currently investigating several smoothing techniques (Chen, 1996), in order to check whether the morpheme model is critical for the data sparseness problem, or whether it can be handled with smoothing over a word model. We are currently investigating two major methods to improve our results: first, we have started gathering a larger corpus of manually tagged text and plan to perform semi-supervised learning on a corpus of 100K manually tagged words. Second, we plan to improve the unknown word model, such as integrating it with named entity recognition system (Ben-Mordechai, 2005).","An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation
Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text.
When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied.
This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways.
We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language.
We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation.
We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step.
Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets.
Our method is applicable to other languages with affix morphology.
We provide for each word not only the PoS, but also full morphological features, such as Gender, Number, Person, Construct, Tense, and the affixes' properties.
We present a lattice-based modification of the BaumWelch algorithm to handle the segmentation ambiguity.
","Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text, according to the word context.In contrast to English tag sets whose sizes range from 48 to 195, the number of tags for Hebrew, based on all combinations of the morphological attributes (part-of-speech, gender, number, person, tense, status, and the affixes' properties2), can grow theoretically to about 300,000 tags.The large size of such a tag set (about 10 times larger than the most comprehensive English tag set) reports an improvement to 92.6% and 94.4% for the case where 100 and",0.8606302738189697,0.8324949741363525,0.846328854560852
"The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination. The result is an accurate wide-coverage CCG parserwhich is an order of magnitude faster than comparable systems for other linguistically moti vated formalisms. Lexicalised grammar formalisms such as Lexicalized Tree Adjoining Grammar (LTAG) and Com binatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentencewhich are then manipulated by the parser. Supertag ging was introduced for LTAG as a way of increasingparsing efficiency by reducing the number of struc tures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003).Supertagging accuracy is relatively high for man ually constructed LTAGs (Bangalore and Joshi,1999). However, for LTAGs extracted automati cally from the Penn Treebank, performance is much lower (Chen et al, 1999; Chen et al, 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al, 2000). In this paper we demonstratethat CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an auto matically extracted grammar, but also offers several practical advantages. Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse. Since an auto matically extracted CCG grammar can produce anextremely large number of parses, the use of a su pertagger is crucial in limiting the total number of parses for the training data to a computationally manageable number. The supertagger is also crucial for increasing thespeed of the parser. We show that spectacular in creases in speed can be obtained, without affectingaccuracy or coverage, by tightly integrating the su pertagger with the CCG grammar and parser. To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis. We also demonstrate how extra constraints on the category combinations, and the application of beam search using the parsing model, can further increase parsing speed.This is the first work we are aware of to succes fully integrate a supertagger with a full parser which uses a lexicalised grammar automatically extractedfrom the Penn Treebank. We also report signifi cantly higher parsing speeds on newspaper text than any previously reported for a full wide-coverage parser. Our results confirm that wide-coverage CCG parsing is feasible for many large-scale NLP tasks. Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the wordsin the sentence, and then combine the categories to gether using CCG?s combinatory rules.1 The first stage can be accomplished by simply assigning to each word all categories from the word?s entry in the lexicon (Hockenmaier, 2003). 1See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. The WSJ is a publication that I enjoy reading NP/N N (S[dcl]\NP)/NP NP/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP Figure 1: Example sentence with CCG lexical categories frequency # cat types # cat tokens in # sentences in 2-21 # cat tokens in # sentences in 00 cut-off 2-21 not in cat set with missing cat 00 not in cat set with missing cat 1 1 225 0 0 12 (0.03%) 12 (0.6%) 10 409 1 933 (0.2%) 1 712 (4.3%) 79 (0.2%) 69 (3.6%) Table 1: Statistics for the lexical category setAn alternative is to use a statistical tagging approach to assign one or more categories. A statisti cal model can be used to determine the most likelycategories given the word?s context. The advantage of this supertagging approach is that the number of categories assigned to each word can be re duced, with a correspondingly massive reduction in the number of derivations. Bangalore and Joshi (1999) use a standard Markov model tagger to assign LTAG elementarytrees to words. Here we use the Maximum En tropy models described in Curran and Clark (2003). An advantage of the Maximum Entropy approachis that it is easy to encode a wide range of poten tially useful information as features; for example,Clark (2002) has shown that POS tags provide use ful information for supertagging. The next section describes the set of lexical categories used by our supertagger and parser. 2.1 The Lexical Category Set. The set of lexical categories is obtained from CCGbank (Hockenmaier and Steedman, 2002; Hockenmaier, 2003), a corpus of CCG normal-form deriva tions derived semi-automatically from the PennTreebank. Following Clark (2002), we apply a fre quency cutoff to the training set, only using thosecategories which appear at least 10 times in sections 2-21. Figure 1 gives an example sentence su pertagged with the correct CCG lexical categories. Table 1 gives the number of different category types and shows the coverage on training (seen) anddevelopment (unseen) data (section 00 from CCGbank). The table also gives statistics for the com plete set containing every lexical category type inCCGbank.2 These figures show that using a fre quency cutoff can significantly reduce the size of the category set with only a small loss in coverage. 2The numbers differ slightly from those reported in Clark (2002) since a newer version of CCGbank is being used here. Clark (2002) compares the size of grammarsextracted from CCGbank with automatically extracted LTAGs. The grammars of Chen and Vijay Shanker (2000) contain between 2,000 and 9,000 tree frames, depending on the parameters used inthe extraction process, significantly more elemen tary structures than the number of lexical categories derived from CCGbank. We hypothesise this is a key factor in the higher accuracy for supertaggingusing a CCG grammar compared with an automati cally extracted LTAG. 2.2 The Tagging Model. The supertagger uses probabilities p(y|x) where y is a lexical category and x is a context. The conditional probabilities have the following log-linear form: p(y|x) = 1 Z(x)e ? i ?i fi(y,x) (1) where fi is a feature, ?i is the corresponding weight, and Z(x) is a normalisation constant. The context is a 5-word window surrounding the target word. Features are defined for each word in the window and for the POS tag of each word. Curran and Clark(2003) describes the model and explains how Gen eralised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories. Theper-word accuracy is between 91 and 92% on un seen data in CCGbank; however, Clark (2002) shows this is not high enough for integration into a parser since the large number of incorrect categories results in a significant loss in coverage. Clark (2002) shows how the models in (1) can be used to define a multi-tagger which can assign more than one category to a word. For each word inthe sentence, the multi-tagger assigns all those cat ? CATS/ ACC SENT ACC SENT WORD ACC (POS) ACC 0.1 1.4 97.0 62.6 96.4 57.4 0.075 1.5 97.4 65.9 96.8 60.6 0.05 1.7 97.8 70.2 97.3 64.4 0.01 2.9 98.5 78.4 98.2 74.2 0.01k=100 3.5 98.9 83.6 98.6 78.9 0 21.9 99.1 84.8 99.0 83.0 Table 2: Supertagger accuracy on section 00 egories whose probability according to (1) is within some factor, ?, of the highest probability category for the word. We follow Clark (2002) in ignoring the featuresbased on the previously assigned categories; there fore every tagging decision is local and the Viterbi algorithm is not required. This simple approach has the advantage of being very efficient, and we findthat it is accurate enough to enable highly accu rate parsing. However, a method which used theforward-backward algorithm to sum over all possi ble sequences, or some other method which took into account category sequence information, may well improve the results. For words seen at least k times in the trainingdata, the tagger can only assign categories appear ing in the word?s entry in the tag dictionary. Eachentry in the tag dictionary is a list of all the cate gories seen with that word in the training data. For words seen less than k times, we use an alternative dictionary based on the word?s POS tag: the tagger can only assign categories that have been seen with the POS tag in the training data. A value of k = 20was used in this work, and sections 2-21 of CCG bank were used as training data.Table 2 gives the per-word accuracy (acc) on sec tion 00 for various values of ?, together with the average number of categories per word. The sent acc column gives the precentage of sentences whose words are all supertagged correctly. The figures for ? = 0.01k=100 correspond to a value of 100 for thetag dictionary parameter k. The set of categories as signed to a word is considered correct if it contains the correct category. The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran andClark (2003) tagger. The drop in accuracy is ex pected given the importance of POS tags as features. The figures for ? = 0 are obtained by assigning all categories to a word from the word?s entry in the tag dictionary. For words which appear less than 20 times in the training data, the dictionary based on the word?s POS tag is used. The table demonstrates the significant reduction in the average number of categories that can be achieved through the use of a supertagger. To give one example, the number of categories in the tag dictionary?s entry for the wordis is 45 (only considering categories which have appeared at least 10 times in the training data). However, in the sentence Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group., the supertag ger correctly assigns 1 category to is for ? = 0.1, and 3 categories for ? = 0.01. The parser is described in detail in Clark and Curran (2004). It takes POS tagged sentences as input with each word assigned a set of lexical categories. A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG. In this paper weuse the normal-form model, which defines proba bilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence. Featuresare defined in terms of the local trees in the derivation, including lexical head information and word word dependencies. The normal-form derivations in CCGbank provide the gold standard training data. The feature set we use is from the best performing normal-form model in Clark and Curran (2004). For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using theViterbi algorithm. The dependency relations are de fined in terms of the argument slots of CCG lexical categories. Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. 3.1 Model Estimation. In Clark and Curran (2004) we describe a discrim inative method for estimating the parameters of a log-linear parsing model. The estimation method maximises the following objective function: L?(?) = L(?) ?G(?) (2) = log m ? j=1 P?(d j|S j) ? n ? i=1 ?2i 2?2The data consists of sentences S 1, . . . , S m, to gether with gold standard normal-form derivations, d1, . . . , dm. L(?) is the log-likelihood of model ?, and G(?) is a Gaussian prior term used to avoid overfitting (n is the number of features; ?i is the weight for feature fi; and ? is a parameter of theGaussian). The objective function is optimised using L-BFGS (Nocedal and Wright, 1999), an iterative algorithm from the numerical optimisation lit erature.The algorithm requires the gradient of the objective function, and the value of the objective function, at each iteration. Calculation of these val ues requires all derivations for each sentence in the training data. In Clark and Curran (2004) wedescribe efficient methods for performing the cal culations using packed charts. However, a very large amount of memory is still needed to store the packed charts for the complete training data even though the representation is very compact; in Clark and Curran (2003) we report a memory usage of 30 GB. To handle this we have developed a parallel implementation of the estimation algorithm which runs on a Beowulf cluster. The need for large high-performance computing resources is a disadvantage of our earlier approach.In the next section we show how use of the supertag ger, combined with normal-form constraints on thederivations, can significantly reduce the memory re quirements for the model estimation. Since the training data contains the correct lexicalcategories, we ensure the correct category is as signed to each word when generating the packed charts for model estimation. Whilst training theparser, the supertagger can be thought of as supply ing a number of plausible but incorrect categoriesfor each word; these, together with the correct cat egories, determine the parts of the parse space that are used in the estimation process. We would like to keep the packed charts as small as possible, but not lose accuracy in the resulting parser. Section 4.2discusses the use of various settings on the supertag ger. The next section describes how normal-form constraints can further reduce the derivation space. 4.1 Normal-Form Constraints. As well as the supertagger, we use two additional strategies for reducing the derivation space. Thefirst, following Hockenmaier (2003), is to only al low categories to combine if the combination hasbeen seen in sections 2-21 of CCGbank. For exam ple, NP/NP could combine with NP/NP accordingto CCG?s combinatory rules (by forward composi tion), but since this particular combination does not appear in CCGbank the parser does not allow it.The second strategy is to use Eisner?s normal form constraints (Eisner, 1996). The constraints SUPERTAGGING/PARSING USAGE CONSTRAINTS DISK MEMORY ? = 0.01 ? 0.05 ? 0.1 17 GB 31 GB CCGbank constraints 13 GB 23 GB Eisner constraints 9 GB 16 GB ? = 0.05 ? 0.1 2 GB 4 GB Table 3: Space requirements for model training dataprevent any constituent which is the result of a forward (backward) composition serving as the primary functor in another forward (backward) composition or a forward (backward) application. Eis ner only deals with a grammar without type-raising,and so the constraints do not guarantee a normal form parse when using a grammar extracted from CCGbank. However, the constraints are still useful in restricting the derivation space. As far as we are aware, this is the first demonstration of the utility of such constraints for a wide-coverage CCG parser. 4.2 Results (Space Requirements). Table 3 shows the effect of different supertagger set tings, and the normal-form constraints, on the size of the packed charts used for model estimation. The disk usage is the space taken on disk by the charts,and the memory usage is the space taken in memory during the estimation process. The training sen tences are parsed using a number of nodes from a 64-node Beowulf cluster.3 The time taken to parse the training sentences depends on the supertagging and parsing constraints, and the number of nodes used, but is typically around 30 minutes. The first row of the table corresponds to using the least restrictive ? value of 0.01, and reverting to ? = 0.05, and finally ? = 0.1, if the chart size exceeds some threshold. The threshold was set at300,000 nodes in the chart. Packed charts are created for approximately 94% of the sentences in sec tions 2-21 of CCGbank. The coverage is not 100%because, for some sentences, the parser cannot pro vide an analysis, and some charts exceed the node limit even at the ? = 0.1 level. This strategy was used in our earlier work (Clark and Curran, 2003) and, as the table shows, results in very large charts.Note that, even with this relaxed setting on the su pertagger, the number of categories assigned to each word is only around 3 on average. This suggests that it is only through use of the supertagger that we are able to estimate a log-linear parsing model on all of the training data at all, since without it the memory 3The figures in the table are estimates based on a sample of the nodes in the cluster. requirements would be far too great, even for the entire 64-node cluster.4 The second row shows the reduction in size if the parser is only allowed to combine categorieswhich have combined in the training data. This sig nificantly reduces the number of categories created using the composition rules, and also prevents thecreation of unlikely categories using rule combina tions not seen in CCGbank. The results show thatthe memory and disk usage are reduced by approx imately 25% using these constraints. The third row shows a further reduction in size when using the Eisner normal-form constraints. Even with the CCGbank rule constraints, theparser still builds many non-normal-form derivations, since CCGbank does contain cases of compo sition and type-raising. (These are used to analysesome coordination and extraction cases, for example.) The combination of the two types of normal form constraints reduces the memory requirements by 48% over the original approach. In Clark andCurran (2004) we show that the parsing model re sulting from training data generated in this way produces state-of-the-art CCG dependency recovery: 84.6 F-score over labelled dependencies. The final row corresponds to a more restrictive setting on the supertagger, in which a value of ? = 0.05 is used initially and ? = 0.1 is used if thenode limit is exceeded. The two types of normal form constraints are also used. In Clark and Curran(2004) we show that using this more restrictive set ting has a small negative impact on the accuracy of the resulting parser (about 0.6 F-score over labelled dependencies). However, the memory requirement for training the model is now only 4 GB, a reduction of 87% compared with the original approach. The previous section showed how to combine the supertagger and parser for the purpose of creating training data, assuming the correct category for each word is known. In this section we describe our approach to tightly integrating the supertagger and parser for parsing unseen data. Our previous approach to parsing unseen data (Clark et al, 2002; Clark and Curran, 2003) wasto use the least restrictive setting of the supertagger which still allows a reasonable compromise be tween speed and accuracy. Our philosophy was to give the parser the greatest possibility of finding thecorrect parse, by giving it as many categories as pos sible, while still retaining reasonable efficiency.4Another possible solution would be to use sampling meth ods, e.g. Osborne (2000). SUPERTAGGING/PARSING TIME SENTS WORDS CONSTRAINTS SEC /SEC /SEC ? = 0.01? 0.1 3 523 0.7 16 CCGbank constraints 1 181 2.0 46 Eisner constraints 995 2.4 55 ? = 0.1? 0.01k=100 608 3.9 90 CCGbank constraints 124 19.4 440 Eisner constraints 100 24.0 546 Parser beam 67 35.8 814 94% coverage 49 49.0 1 114 Parser beam 46 52.2 1 186 Oracle 18 133.4 3 031 Table 4: Parse times for section 23 The problem with this approach is that, for some sentences, the number of categories in the chart still gets extremely large and so parsing is unacceptably slow. Hence we applied a limit to the number of categories in the chart, as in the previous section,and reverted to a more restrictive setting of the su pertagger if the limit was exceeded. We first used a value of ? = 0.01, and then reverted to ? = 0.05, and finally ? = 0.1. In this paper we take the opposite approach: westart with a very restrictive setting of the supertag ger, and only assign more categories if the parser cannot find an analysis spanning the sentence. In this way the parser interacts much more closely with the supertagger. In effect, the parser is using the grammar to decide if the categories provided by thesupertagger are acceptable, and if not the parser re quests more categories. The parser uses the 5 levels given in Table 2, starting with ? = 0.1 and moving through the levels to ? = 0.01k=100 . The advantage of this approach is that parsing speeds are much higher. We also show that our new approach slightly increases parsing accuracy over the previous method. This suggests that, given our current parsing model, it is better to rely largely on the supertagger to provide the correct categoriesrather than use the parsing model to select the cor rect categories from a very large derivation space. 5.1 Results (Parse Times). The results in this section are all using the best per forming normal-form model in Clark and Curran (2004), which corresponds to row 3 in Table 3. All experiments were run on a 2.8 GHZ Intel Xeon P4 with 2 GB RAM. Table 4 gives parse times for the 2,401 sentences in section 23 of CCGbank. The final two columns give the number of sentences, and the number of ? CATS/ 0.1 FIRST 0.01 FIRST WORD PARSES % PARSES % 0.1 1.4 1689 88.4 0 0.0 0.075 1.5 43 2.3 7 0.4 0.05 1.7 51 2.7 39 2.0 0.01 2.9 79 4.1 1816 95.1 0.01k=100 3.5 33 1.7 33 1.7 NO SPAN 15 0.8 15 0.8 Table 5: Supertagger ? levels used on section 00words, parsed per second. For all of the figures re ported on section 23, unless stated otherwise, the parser is able to provide an analysis for 98.5% of the sentences. The parse times and speeds include the failed sentences, but do not include the time takenby the supertagger; however, the supertagger is ex tremely efficient, and takes less than 6 seconds to supertag section 23, most of which consists of load time for the Maximum Entropy model. The first three rows correspond to our strategy ofearlier work by starting with the least restrictive set ting of the supertagger. The first value of ? is 0.01; if the parser cannot find a spanning analysis, this ischanged to ? = 0.01k=100; if the node limit is ex ceeded (for these experiments set at 1,000,000), ? is changed to 0.05. If the node limit is still exceeded, ? is changed to 0.075, and finally 0.1. The second row has the CCGbank rule restriction applied, and the third row the Eisner normal-form restrictions.The next three rows correspond to our new strat egy of starting with the least restrictive setting of thesupertagger (? = 0.1), and moving through the set tings if the parser cannot find a spanning analysis. The table shows that the normal-form constraints have a significant impact on the speed, reducing theparse times for the old strategy by 72%, and reduc ing the times for the new strategy by 84%. The new strategy also has a spectacular impact on the speed compared with the old strategy, reducing the times by 83% without the normal-form constraints and 90% with the constraints. The 94% coverage row corresponds to using only the first two supertagging levels; the parser ignores the sentence if it cannot get an analysis at the ? = 0.05 level. The percentage of sentences without an analysis is now 6%, but the parser is extremely fast,processing almost 50 sentences a second. This configuration of the system would be useful for obtaining data for lexical knowledge acquisition, for ex ample, for which large amounts of data are required. The oracle row shows the parser speed when it is provided with only the correct lexical categories.The parser is extremely fast, and in Clark and Cur ran (2004) we show that the F-score for labelled dependencies is almost 98%. This demonstratesthe large amount of information in the lexical categories, and the potential for improving parser ac curacy and efficiency by improving the supertagger. Finally, the first parser beam row corresponds to the parser using a beam search to further reduce thederivation space. The beam search works by prun ing categories from the chart: a category can only be part of a derivation if its beam score is within some factor, ?, of the highest scoring category forthat cell in the chart. Here we simply use the ex ponential of the inside score of a category as the beam score; the inside score for a category c is the sum over all sub-derivations dominated by c of the weights of the features in those sub-derivations (see Clark and Curran (2004).5The value of ? that we use here reduces the accu racy of the parser on section 00 by a small amount (0.3% labelled F-score), but has a significant impacton parser speed, reducing the parse times by a fur ther 33%. The final parser beam row combines thebeam search with the fast, reduced coverage config uration of the parser, producing speeds of over 50 sentences per second. Table 5 gives the percentage of sentences which are parsed at each supertagger level, for both the new and old parsing strategies. The results show that, for the old approach, most of the sentences areparsed using the least restrictive setting of the supertagger (? = 0.01); conversely, for the new ap proach, most of the sentences are parsed using the most restrictive setting (? = 0.1). As well as investigating parser efficiency, we have also evaluated the accuracy of the parser onsection 00 of CCGbank, using both parsing strate gies together with the normal-form constraints. Thenew strategy increases the F-score over labelled de pendencies by approximately 0.5%, leading to the figures reported in Clark and Curran (2004). 5.2 Comparison with Other Work. The only other work we are aware of to investigate the impact of supertagging on parsing efficiency is the work of Sarkar et al (2000) for LTAG. Sarkar etal. did find that LTAG supertagging increased pars ing speed, but at a significant cost in coverage: only 1,324 sentences out of a test set of 2,250 received a parse. The parse times reported are also not as good as those reported here: the time taken to parse the 2,250 test sentences was over 5 hours.5Multiplying by an estimate of the outside score may im prove the efficacy of the beam. Kaplan et al (2004) report high parsing speedsfor a deep parsing system which uses an LFG gram mar: 1.9 sentences per second for 560 sentencesfrom section 23 of the Penn Treebank. They also re port speeds for the publicly available Collins parser (Collins, 1999): 2.8 sentences per second for the same set. The best speeds we have reported for the CCG parser are an order of magnitude faster. This paper has shown that by tightly integrating a supertagger with a CCG parser, very fast parse times can be achieved for Penn Treebank WSJ text. As far as we are aware, the times reported here are an orderof magnitude faster than any reported for compara ble systems using linguistically motivated grammar formalisms. The techniques we have presented inthis paper increase the speed of the parser by a factor of 77. This makes this parser suitable for large scale NLP tasks.The results also suggest that further improvements can be obtained by improving the supertagger, which should be possible given the simple tag ging approach currently being used.The novel parsing strategy of allowing the grammar to decide if the supertagging is likely to be cor rect suggests a number of interesting possibilities.In particular, we would like to investigate only re pairing those areas of the chart that are most likely to contain errors, rather than parsing the sentence from scratch using a new set of lexical categories. This could further increase parsing effficiency. Acknowledgements We would like to thank Julia Hockenmaier, whosework creating the CCGbank made this research possible, and Mark Steedman for his advice and guid ance. This research was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.","The Importance Of Supertagging For Wide-Coverage CCG Parsing
This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis.
The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training.
It also dramatically increases the speed of the parser.
We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser.
This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar.
We also further reduce the derivation space using constraints on category combination.
The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms.
Our scores give an indication of how supertagging accuracy corresponds to overall dependency recovery.
We describe two log-linear parsing models for CCG: a normal-form derivation model and a dependency model.
The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We propose a method for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.
","Supertag ging was introduced for LTAG as a way of increasingparsing efficiency by reducing the number of struc tures assigned to each word (Bangalore and Joshi, 1999).We show that spectacular in creases in speed can be obtained, without affectingaccuracy or coverage, by tightly integrating the su pertagger with the CCG grammar and parser.Our wide-coverage CCG parser uses a log-linear model to select an analysis.The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training.",0.873066246509552,0.8490204811096191,0.8608754873275757
"Overview of BioNLP Shared Task 2011 The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. The BioNLP Shared Task (BioNLP-ST, hereafter) series represents a community-wide move toward fine-grained information extraction (IE), in particular biomolecular event extraction (Kim et al., 2009; Ananiadou et al., 2010). The series is complementary to BioCreative (Hirschman et al., 2007); while BioCreative emphasizes the short-term applicability of introduced IE methods for tasks such as database curation, BioNLP-ST places more emphasis on the measurability of the state-of-the-art and traceability of challenges in extraction through an approach more closely tied to text. These goals were pursued in the first event, BioNLP-ST 2009 (Kim et al., 2009), through high quality benchmark data provided for system development and detailed evaluation performed to identify remaining problems hindering extraction performance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). As the second event of the series, BioNLP-ST 2011 preserves the general design and goals of the previous event, but adds a new focus on variability to address a limitation of BioNLP-ST 2009: the benchmark data sets were based on the Genia corpus (Kim et al., 2008), restricting the community-wide effort to resources developed by a single group for a small subdomain of molecular biology. BioNLPST 2011 is organized as a joint effort of several groups preparing various tasks and resources, in which variability is pursued in three primary directions: text types, event types, and subject domains. Consequently, generalization of fine grained bio-IE in these directions is emphasized as the main theme of the second event. This paper summarizes the entire BioNLP-ST 2011, covering the relationships between tasks and similar broad issues. Each task is presented in detail in separate overview papers and extraction systems in papers by participants. BioNLP-ST 2011 includes four main tracks (with five tasks) representing fine-grained bio-IE. The GE task (Kim et al., 2011) preserves the task definition of BioNLP-ST 2009, arranged based on the Genia corpus (Kim et al., 2008). The data represents a focused domain of molecular biology: transcription factors in human blood cells. The purpose of the GE task is two-fold: to measure the progress of the community since the last event, and to evaluate generalization of the technology to full papers. For the second purpose, the provided data is composed of two collections: the abstract collection, identical to the BioNLP-ST 2009 data, and the new full paper collection. Progress on the task is measured through the unchanged task definition and the abstract collection, while generalization to full papers is measured on the full paper collection. In this way, the GE task is intended to connect the entire event to the previous one. The EPI task (Ohta et al., 2011) focuses on IE for protein and DNA modifications, with particular emphasis on events of epigenetics interest. While the basic task setup and entity definitions follow those of the GE task, EPI extends on the extraction targets by defining 14 new event types relevant to task topics, including major protein modification types and their reverse reactions. For capturing the ways in which different entities participate in these events, the task extends the GE argument roles with two new roles specific to the domain, Sidechain and Contextgene. The task design and setup are oriented toward the needs of pathway extraction and curation for domain databases (Wu et al., 2003; Ongenaert et al., 2008) and are informed by previous studies on extraction of the target events (Ohta et al., 2010b; Ohta et al., 2010c). The ID task (Pyysalo et al., 2011a) concerns the extraction of events relevant to biomolecular mechanisms of infectious diseases from full-text publications. The task follows the basic design of BioNLPST 2009, and the ID entities and extraction targets are a superset of the GE ones. The task extends considerably on core entities, adding to PROTEIN four new entity types, including CHEMICAL and ORGANISM. The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high-level biological processes. The task was implemented in collaboration with domain experts and informed by prior studies on domain information extraction requirements (Pyysalo et al., 2010; Ananiadou et al., 2011), including the support of systems such as PATRIC (http://patricbrc.org). The bacteria track consists of two tasks, BB and BI. 2.4.1 Bacteria biotope task (BB) The aim of the BB task (Bossy et al., 2011) is to extract the habitats of bacteria mentioned in textbooklevel texts written for non-experts. The texts are Web pages about the state of the art knowledge about bacterial species. BB targets general relations, Localization and PartOf, and is challenging in that texts contain more coreferences than usual, habitat references are not necessarily named entities, and, unlike in other BioNLP-ST 2011 tasks, all entities need to be recognized by participants. BB is the first task to target phenotypic information and, as habitats are yet to be normalized by the field community, presents an opportunity for the BioNLP community to contribute to the standardization effort. The BI task (Jourde et al., 2011) is devoted to the extraction of bacterial molecular interactions and regulations from publication abstracts. Mainly focused on gene transcriptional regulation in Bacillus subtilis, the BI corpus is provided to participants with rich semantic annotation derived from a recently proposed ontology (Manine et al., 2009) defining ten entity types such as gene, protein and derivatives as well as DNA sites/motifs. Their interactions are described through ten relation types. The BI corpus consists of the sentences of the LLL corpus (N´edellec, 2005), provided with manually checked linguistic annotations. The main tasks are characterized in Table 1. From the text type perspective, BioNLP-ST 2011 generalizes from abstracts in 2009 to full papers (GE and ID) and web pages (BB). It also includes data collections for a variety of specific subject domains (GE, ID, BB an BI) and a task (EPI) whose scope is not defined through a domain but rather event types. In terms of the target event types, ID targets a superset of GE events and EPI extends on the representation for PHOSPHORYLATION events of GE. The two bacteria track tasks represent an independent perspective relatively far from other tasks in terms of their target information. BioNLP-ST 2011 includes three supporting tasks designed to assist in primary the extraction tasks. Other supporting resources made available to participants are presented in (Stenetorp et al., 2011). The CO task (Nguyen et al., 2011) concerns the recognition of coreferences to protein references. It is motivated from a finding from BioNLP-ST 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained IE systems. While finding connections between event triggers and protein references is a major part of event extraction, it becomes much harder if one is replaced with a coreferencing expression. The CO task seeks to address this problem. The data sets for the task were produced based on MedCO annotation (Su et al., 2008) and other Genia resources (Tateisi et al., 2005; Kim et al., 2008). The REL task (Pyysalo et al., 2011b) involves the recognition of two binary part-of relations between entities: PROTEIN-COMPONENT and SUBUNITCOMPLEX. The task is motivated by specific challenges: the identification of the components of proteins in text is relevant e.g. to the recognition of Site arguments (cf. GE, EPI and ID tasks), and relations between proteins and their complexes relevant to any task involving them. REL setup is informed by recent semantic relation tasks (Hendrickx et al., 2010). The task data, consisting of new annotations for GE data, extends a previously introduced resource (Pyysalo et al., 2009; Ohta et al., 2010a). The REN task (Jourde et al., 2011) objective is to extract renaming pairs of Bacillus subtilis gene/protein names from PubMed abstracts, motivated by discrepancies between nomenclature databases that interfere with search and complicate normalization. REN relations partially overlap several concepts: explicit renaming mentions, synonymy, and renaming deduced from biological proof. While the task is related to synonymy relation extraction (Yu and Agichtein, 2003), it has a novel definition of renaming, one name permanently replacing the other. Table 2 shows the task schedule, split into two phases to allow the use of supporting task results in addressing the main tasks. In recognition of their higher complexity, a longer development period was arranged for the main tasks (3 months vs 7 weeks). BioNLP-ST 2011 received 46 submissions from 24 teams (Table 3). While seven teams participated in multiple tasks, only one team, UTurku, submitted final results to all the tasks. The remaining 17 teams participated in only single tasks. Disappointingly, only two teams (UTurku, and ConcordU) performed both supporting and main tasks, and neither used supporting task analyses for the main tasks. Detailed evaluation results and analyses are presented in individual task papers, but interesting observations can be obtained also by comparisons over the tasks. Table 4 summarizes best results for various criteria (Note that the results shown for e.g. GEa, GEf and GEp may be from different teams). The community has made a significant improvement in the repeated GE task, with an over 10% reduction in error from ’09 to GEa. Three teams achieved better results than M10, the best previously reported individual result on the ’09 data. This indicates a beneficial role from focused efforts like BioNLP-ST. The GEf and ID results show that generalization to full papers is feasible, with very modest loss in performance compared to abstracts (GEa). The results for PHOSPHORYLATION events in GE and EPI are comparable (GEp vs EPIp), with the small drop for the EPI result, suggesting that the removal of the GE domain specificity does not compromise extraction performance. EPIc results indicate some challenges in generalization to similar event types, and EPIf suggest substantial further challenges in additional argument extraction. The complexity of ID is comparable to GE, also reflected to their final results, which further indicate successful generalization to a new subject domain as well as to new argument (entity) types. The BB task is in part comparable to GEl and involves a representation similar to REL, with lower results likely in part because BB requires entity recognition. The BI task is comparable to LLL Challenge, though BI involves more entity and event types. The BI result is 20 points above the LLL best result, indicating a substantial progress of the community in five years. Meeting with wide participation from the community, BioNLP-ST 2011 produced a wealth of valuable resources for the advancement of fine-grained IE in biology and biomedicine, and demonstrated that event extraction methods can successfully generalize to new text types, event types, and domains. However, the goal to observe the capacity of supporting tasks to assist the main tasks was not met. The entire shared task period was very long, more than 6 months, and the complexity of the task was high, which could be an excessive burden for participants, limiting the application of novel resources. There have been ongoing efforts since BioNLP-ST 2009 to develop IE systems based on the task resources, and we hope to see continued efforts also following BioNLP-ST 2011, especially exploring the use of supporting task resources for main tasks.","Overview of BioNLP Shared Task 2011
The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams.
Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully fully generalize in various aspects.
The BioNLP 2011 Shared Task series generalized this defining a series of tasks involving more text types, domains and target event types.
","BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams.The GE task (Kim et al., 2011) preserves the general design and goals of BioNLP-ST 2009, arranged based on the Genia corpus (Kim and al., 2008), restricting the community",0.8882601261138916,0.8831004500389099,0.8856727480888367
"Word-Sense Disambiguation Using Statistical Methods We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent. An alluring aspect of the statistical approach to machine translation rejuvenated by Brown et al. [Brown et at., 1988, Brown et al., 1990] is the systematic framework it provides for attacking the problem of lexical disambiguation. For example, the system they describe translates the French sentence Je vais prendre la decision as I will make the decision, correctly interpreting prendre as make. The statistical translation model, which supplies English translations of French words, prefers the more common translation take, but the trigram language model recognizes that the three-word sequence make the decision is much more probable than take the decision.. The system is not always so successful. It incorrectly renders Je vais prendre ma propre decision as / will take my own decision. The language model does not realize that take my own decision is improbable because take and decision no longer fall within a single trigram. Errors such as this are common because the statistical models only capture local phenomena; if the context necessary to determine a translation falls outside the scope of the models, the word is likely to be translated incorrectly. However, if the relevant context is encoded locally, the word should be translated correctly. We can achieve this within the traditional paradigm of analysis, transfer, and synthesis by incorporating into the analysis phase a sense-disambiguation component that assigns sense labels to French words. If prendre is labeled with one sense in the context of decision but with a different sense in other contexts, then the translation model will learn from training data that the first sense usually translates to make, whereas the other sense usually translates to take. Previous efforts at algorithmic disambiguation of word senses [Lesk, 1986, White, 1988, He and •Veronis, 1990] have concentrated on information that can be extracted from electronic dictionaries, and focus, therefore, on senses as determined by those dictionaries. Here, in contrast, we present a procedure for constructing a sense-disambiguation component that labels words so as to elucidate their translations in another language. We are conThe proposal will not now be implemented Les propositions ne seront pas mises en application maintenant cerned about senses as they occur in a dictionary only to the extent that those senses are translated differently. The French noun interet, for example, is translated into German as either Zins or Interesse according to its sense, but both of these senses are translated into English as interest, and so we make no attempt to distinguish them. Following Brown et al. [Brown et al., 1994 we choose as the translation of a French sentence F that sentence E for which Pr (EIF) is greatest. By Bayes' rule, Since the denominator does not depend on E, the sentence for which Pr (EIF) is greatest is also the sentence for which the product Pr (E) Pr (FIE) is greatest. The first factor in this product is a statistical characterization of the English language and the second factor is a statistical characterization of the process by which English sentences are translated into French. We can compute neither factors precisely. Rather, in statistical translation, we employ models from which we can obtain estimates of these values. We call the model from which we compute Pr (E) the language model and that from which we compute Pr (FIE) the translation model. The translation model used by Brown et al. [Brown et al., 1990] incorporates the concept of an alignment in which each word in E acts independently to produce some of the words in F. If we denote a typical alignment by A, then we can write the probability of F given E as a sum over all possible alignments: Although the number of possible alignments is a very rapidly growing function of the lengths of the French and English sentences, only a tiny fraction of the alignments contributes substantially to the sum, and of these few, one makes the greatest contribution. We call this most probable alignment the Viterbi alignment between E and F. The identity of the Viterbi alignment for a pair of sentences depends on the details of the translation model, but once the model is known, probable alignments can be discovered algorithmically [Brown et al., 1991]. Brown et al. [Brown et al., 1990], show an example of such au automatically derived alignment in their Figure 3. (For the reader's convenience, we have reproduced that figure here as Figure 1.) In a Viterbi alignment, a French word that is connected by a line to an English word is said to be aligned with that English word. Thus, in Figure 1, Les is aligned with The, propositions with proposal, and so on. We call a pair of aligned words obtained in this wa.y a connection. From the Viterbi alignments for 1,002,165 pairs of short French and English sentences from the Canadian Hansard data [Brown et al., 1990], we have extracted a set of 12,028,485 connections. Let p(e, f) be the probability that a connection chosen at random from this set will connect the English word e to the French word f. Because each French word gives rise to exactly one connection, the right marginal of this distribution is identical to the distribution of French words in these sentences. The left marginal, however, is not the same as the distribution of English words: English words that tend to produce several French words at a time are overrepresented while those that tend to produce no French words are underrepresented. Using p(e, f) we can compute the mutual information between a French word and its English mate in a connection. In this section, we discuss a method for labelling a word with a sense that depends on the context in which it appears in such a way as to increase the mutual information between the members of a connection. In the sentence Je vais prendre ma propre decision, the French verb prendre should be translated as make because the object of prendre is decision. If we replace decision by voiture, then prendre should be translated as take to yield I will take my own car. In these examples, one can imagine assigning a sense to prendre by asking whether the first noun to the right of prendre is decision OT voiture. We say that the noun to the right is the informant for prendre. In Il doute que les notres gagnent, which. means He doubts that we will win, the French word il should be translated as he. On the other hand, in Il faut que les notres gagnent, which means it is necessary that we win, il should be translated as it. Here, we can determine which sense to assign to il by asking about the identity of the first verb to its right. Even though we cannot hope to determine the translation of il from this informant unambiguously, we can hope to obtain a significant amount of information about the translation. As a -final example, consider the English word is. In the sentence I think it is a problem, it is best to translate is as est as in Je pense que c'est un ,probleme. However, this is certainly not true in the sentence I think there is a problem, which translates as Je pense qu'll y a un probleme. Here we can reduce the entropy of the distribution of the translation of is by asking if the word to the left is there. If so, then is is less likely to be translated as est than if not. Motivated by examples like these, we investigated a simple method of assigning two senses to a word w by asking a single binary question about one word of the context in which w appears. One does not know beforehand whether the informant will be the first noun to the right, the first verb to the right, or some other word. in the context of w. However, one can construct a question for each of a number of candidate informant sites, and then choose the most informative question. Given a potential informant such as the first 1101111 to the right, we can construct a question that has high mutual information with the translation of w by using the flip-flop algorithm devised by Nadas, Nahamoo, Picheny, and Powell [Nadas et al., 1991]. To understand their algorithm, first imagine that w is a French word and that English words which are possible translations of w have been divided into two classes. Consider the problem of constructing a binary question about the potential informant that provides maximal information about these two English word classes. If the French vocabulary is of size V, then there are 2&quot; possible questions. However, using the splitting theorem of Breiman, Friedman, 01shen, and Stone [Breiman et aL, 1984], it is possible to find the most informative of these 2v questions in time which is linear in V. The flip-flop algorithm begins by making an initial assignment of the English translations into two classes, and then uses the splitting theorem to find the best question about the potential informant. This question divides the French vocabulary into two sets. One can then use the splitting theorem to find a division of the English translations of w into two sets which has maximal mutual information with the French sets. In the flip-flop algorithm, one alternates between splitting the French vocabulary into two sets and the English translations of w into two sets. After each such split, the mutual information between the French and English sets is at least as great as before the split. Since the mutual information is bounded by one bit, the process converges to a partition of the French vocabulary that has high mutual information with the translation of w. We used the flip-flop algorithm in a pilot experiment in which we assigned two senses to each of the 500 most common English words and two senses to each of the 200 most common French words. For a French word, we considered questions about seven informants: the word to the left, the word to the right, the first noun to the left, the first noun to the right, the first verb to the left, the first verb to the right, and the tense of either the current word, if it is a verb, or of the first verb to the left of the current word. For an English word, we only considered questions about the the word to the left and the word two to the left. We restricted the English questions to the previous two words so that we could easily use them in our translation system which produces an English sentence from left to right. When a. potential informant did not exist, because, say there wa,s no noun to the left of some word in a particular sentence, we used the special word, TERM...WORD. To find the nouns and verbs in our French sentences, we used the tagging algorithm described by Merialdo [Merialdo, 1990]. Figure 2 shows the question that was constructed for the verb prendre. The noun to the right yielded the most information, .381 bits, about the English translation of prendre. The box in the top of the figure shows the words which most frequently occupy that site, that is, the nouns which appear to the right of prendre with a probability greater than one part in fifty. An instance of prendre is assigned the first or second sense depending on whether the first noun to the right appears in the lefthand or the right-hand column. So, for example, if the noun to the right of prendre is decision, parole, or connaissance, then prendre is assigned the second sense. The box at the bottom of the figure shows the most probable translations of each of the two senses. Notice that the English verb to_make is three times as likely when prendre has the second sense as when it has the first sense. People make decisions, speeches, and acquaintances, they do not take them. Figure 3 shows our results for the verb vouloir. Here, the best informant is the tense of vouloir. The first sense is three times more likely than the second sense to translate as to_want, but twelve times less likely to translate as to like. In polite English, one says I would like so and so more commonly than I would want so and so. The question in Figure 4 reduces the entropy of the translation of the French preposition depuis by .738 bits. When depuis is followed by an article, it translates with probability .772 to since, and otherwise only with probability .016. Finally, consider the English word cent. In our text, it is either a denomination of currency, in which case it is usually preceded by a number and translated as c., or it is the second half of per cent, in which case it is preceded by per and translated along with per as Z. The results in Figure 5 show that the algorithm has discovered this, and in so doing has reduced the entropy of the translation of cent by .378 bits. to a word, and thus can extract no more than one bit of information about the translation of that, word. Since the entropy of the translation of a. common word can be as high as five bits, there is reason to hope that using more senses will further improve the performance of our system. Our method asks a. single question about a single word of context. We can think of this as the first question in a decision tree which can be extended to additional levels [Lucassen, 1983, Lucassen and Mercer, 1984, Breiman et al., 1984, Bahl et a/., 1989]. We are working on these and other improvements and hope to report better results in the future. Pleased with these results, we incorporated sense-assignment questions for the 500 most common English words and 200 most common French words into our translation system. This system is an enhanced version of the one described by Brown et al. [Brown et al., 1990] in that it uses a trigram language model, and has a French vocabulary of 57,802 words, a.nd an English vocabulary of 40,809 words. We translated 100 randomly selected Hansard sentences each of which is 10 words or less in length. We judged 45 of the resultant translations as acceptable as compared with 37 acceptable translations produced by the same system running without sense-disambiguation questions.","Word-Sense Disambiguation Using Statistical Methods
We describe a statistical technique for assigning senses to words.
An instance of a word is assigned a sense by asking a question about the context in which the word appears.
The question is constructed to have high mutual information with the translation of that instance in another language.
When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.
We propose a word-sense disambiguation algorithm to disambiguate English translations of French target words based on the single most imformative context feature.
We perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.
","The translation model used by Brown et al. [Brown et al., 1994 we choose as the translation of a French sentence F that sentence E for which Pr (EIF) is greatest.The language model does not realize that take my own decision is improbable because take and decision no longer fall within a single trigram.If prendre is labeled with one sense in the context of decision but with a different sense in other contexts, then the translation model will learn from training data that the first sense usually translates to make, whereas the other sense often translates to take.The statistical translation model uses the language model and that from which we compute Pr (",0.82084721326828,0.8245769739151001,0.8227078914642334
"Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical matranslation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality. Statistical approaches to machine translation, pioneered by (Brown et al., 1993), achieved impressive performance by leveraging large amounts of parallel corpora. Such approaches, which are essentially stochastic string-to-string transducers, do not explicitly model natural language syntax or semantics. In reality, pure statistical systems sometimes suffer from ungrammatical outputs, which are understandable at the phrasal level but sometimes hard to comprehend as a coherent sentence. In recent years, syntax-based statistical machine translation, which aims at applying statistical models to structural data, has begun to emerge. With the research advances in natural language parsing, especially the broad-coverage parsers trained from treebanks, for example (Collins, 1999), the utilization of structural analysis of different languages has been made possible. Ideally, by combining the natural language syntax and machine learning methods, a broad-coverage and linguistically wellmotivated statistical MT system can be constructed. However, structural divergences between languages (Dorr, 1994),which are due to either systematic differences between languages or loose translations in real corpora,pose a major challenge to syntax-based statistical MT. As a result, the syntax based MT systems have to transduce between non-isomorphic tree structures. (Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) represents each production in parallel dependency trees as a finite-state transducer. Both approaches learn the tree representations directly from parallel sentences, and do not make allowances for nonisomorphic structures. (Yamada and Knight, 2001, 2002) modeled translation as a sequence of tree operations transforming a syntactic tree into a string of the target language. When researchers try to use syntax trees in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. Eisner (2003) proposed viewing the MT problem as a probabilistic synchronous tree substitution grammar parsing problem. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. All these approaches, though different in formalism, model the two languages using tree-based transduction rules or a synchronous grammar, possibly probabilistic, and using multi-lemma elementary structures as atomic units. The machine translation is done either as a stochastic tree-to-tree transduction or a synchronous parsing process. However, few of the above mentioned formalisms have large scale implementations. And to the best of our knowledge, the advantages of syntax based statistical MT systems over pure statistical MT systems have yet to be empirically verified. We believe difficulties in inducing a synchronous grammar or a set of tree transduction rules from large scale parallel corpora are caused by: Hajic et al. (2002) limited non-isomorphism by n-to-m matching of nodes in the two trees. However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. In a different approach, Hwa et al. (2002) aligned the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees. This motivated us to look for a more efficient and effective way to induce a synchronous grammar from parallel corpora and to build an MT system that performs competitively with the pure statistical MT systems. We chose to build the synchronous grammar on the parallel dependency structures of the sentences. The synchronous grammar is induced by hierarchical tree partitioning operations. The rest of this paper describes the system details as follows: Sections 2 and 3 describe the motivation behind the usage of dependency structures and how a version of synchronous dependency grammar is learned. This grammar is used as the primary translation knowledge source for our system. Section 4 defines the tree-to-tree transducer and the graphical model for the stochastic tree-to-tree transduction process and introduces a polynomial time decoding algorithm for the transducer. We evaluate our system in section 5 with the NIST/Bleu automatic MT evaluation software and the results are discussed in Section 6. According to Fox (2002), dependency representations have the best inter-lingual phrasal cohesion properties. The percentage for head crossings is 12.62% and that of modifier crossings is 9.22%. Furthermore, a grammar based on dependency structures has the advantage of being simple in formalism yet having CFG equivalent formal generative capacity (Ding and Palmer, 2004b). Dependency structures are inherently lexicalized as each node is one word. In comparison, phrasal structures (treebank style trees) have two node types: terminals store the lexical items and non-terminals store word order and phrasal scopes. Ding and Palmer (2004b) described one version of synchronous grammar: Synchronous Dependency Insertion Grammars. A Dependency Insertion Grammars (DIG) is a generative grammar formalism that captures word order phenomena within the dependency representation. In the scenario of two languages, the two sentences in the source and target languages can be modeled as being generated from a synchronous derivation process. A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g. Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990)). Apart from other details, a DIG can be viewed as a tree substitution grammar defined on dependency trees (as opposed to phrasal structure trees). The basic units of the grammar are elementary trees (ET), which are sub-sentential dependency structures containing one or more lexical items. The synchronous version, SDIG, assumes that the isomorphism of the two syntactic structures is at the ET level, rather than at the word level, hence allowing non-isomorphic tree to tree mapping. We illustrate how the SDIG works using the following pseudo-translation example: Almost any tree-transduction operations defined on a single node will fail to generate the target sentence from the source sentence without using insertion/deletion operations. However, if we view each dependency tree as an assembly of indivisible sub-sentential elementary trees (ETs), we can find a proper way to transduce the input tree to the output tree. An ET is a single “symbol” in a transducer’s language. As shown in Figure 2, each circle stands for an ET and thick arrows denote the transduction of each ET as a single symbol. As the start to our syntax-based SMT system, the SDIG must be learned from the parallel corpora. One straightforward way to induce a generative grammar is using EM style estimation on the generative process. Different versions of such training algorithms can be found in (Hajic et al., 2002; Eisner 2003; Gildea 2003; Graehl and Knight 2004). However, a synchronous derivation process cannot handle two types of cross-language mappings: crossing-dependencies (parent-descendent switch) and broken dependencies (descendent appears elsewhere), which are illustrated below: In the above graph, the two sides are English and the foreign dependency trees. Each node in a tree stands for a lemma in a dependency tree. The arrows denote aligned nodes and those resulting inconsistent dependencies are marked with a “*”. Fox (2002) collected the statistics mainly on French and English data: in dependency representations, the percentage of head crossings per chance (case [b] in the graph) is 12.62%. Using the statistics on cross-lingual dependency consistencies from a small word to word aligned Chinese-English parallel corpus1, we found that the percentage of crossing-dependencies (case [b]) between Chinese and English is 4.7% while that of broken dependencies (case [c]) is 59.3%. The large number of broken dependencies presents a major challenge for grammar induction based on a top-down style EM learning process. Such broken and crossing dependencies can be modeled by SDIG if they appear inside a pair of elementary trees. However, if they appear between the elementary trees, they are not compatible with the isomorphism assumption on which SDIG is based. Nevertheless, the hope is that the fact that the training corpus contains a significant percentage of dependency inconsistencies does not mean that during decoding the target language sentence cannot be written in a dependency consistent way. (Ding and Palmer, 2004a) gave a polynomial time solution for learning parallel sub-sentential dependency structures from non-isomorphic dependency trees. Our approach, while similar to (Ding and Palmer, 2004a) in that we also iteratively partition the parallel dependency trees based on a heuristic function, departs (Ding and Palmer, 2004a) in three ways: (1) we base the hierarchical tree partitioning operations on the categories of the dependency trees; (2) the statistics of the resultant tree pairs from the partitioning operation are collected at each iteration rather than at the end of the algorithm; (3) we do not re-train the word to word probabilities at each iteration. Our grammar induction algorithm is sketched below: C = CategorySequence[i] be the current syntactic category set. For each tree pair in the corpus, do { a) For the tentative synchronous partitioning operation, use a heuristic function to select the BEST word pair (ei*, f j*) , where both ei*, f j* are NOT “chosen”, Category(ei*) E C and Category(f j*) E C. b) If (ei*, fj*) is found in (a), mark ei*, fj* as “chosen” and go back to (a), else go to (c). At each iteration, one specific set of categories of nodes is handled. The category sequence we used in the grammar induction is: We first process top NP chunks because they are the most stable between languages. Interestingly, NPs are also used as anchor points to learn monolingual paraphrases (Ibrahim et al., 2003). The phrasal structure categories can be extracted from automatic parsers using methods in (Xia, 2001). An illustration is given below (Chinese in pinyin form). The placement of the dependency arcs reflects the relative word order between a parent node and all its immediate children. The collected ETs are put into square boxes and the partitioning operations taken are marked with dotted arrows. Similar to (Ding and Palmer, 2004a), we also use a heuristic function in Step 1(a) of the algorithm to rank all the word pairs for the tentative tree partitioning operation. The heuristic function is based on a set of heuristics, most of which are similar to those in (Ding and Palmer, 2004a). For a word pair (ei, fj)for the tentative partitioning operation, we briefly describe the heuristics: the foreign words in the current tree. The above heuristics are a set of real valued numbers. We use a Maximum Entropy model to interpolate the heuristics in a log-linear fashion, which is different from the error minimization training in (Ding and Palmer, 2004a). where y = (0,1) as labeled in the training data whether the two words are mapped with each other. The MaxEnt model is trained using the same word level aligned parallel corpus as the one in Section 3.1. Although the training corpus isn’t large, the fact that we only have a handful of parameters to fit eased the problem. It is worth noting that the set of derived parallel dependency Elementary Trees is not a full-fledged SDIG yet. Many features in the SDIG formalism such as arguments, head percolation, etc. are not yet filled. We nevertheless use this derived grammar as a Mini-SDIG, assuming the unfilled features as empty by default. A full-fledged SDIG remains a goal for future research. As discussed before (see Figure 1 and 2), the architecture of our syntax based statistical MT system is illustrated in Figure 5. Note that this is a nondeterministic process. The input sentence is first parsed using an automatic parser and a dependency tree is derived. The rest of the pipeline can be viewed as a stochastic tree transducer. The MT decoding starts first by decomposing the input dependency tree in to elementary trees. Several different results of the decomposition are possible. Each decomposition is indeed a derivation process on the foreign side of SDIG. Then the elementary trees go through a transfer phase and target ETs are combined together into the output. The stochastic tree-to-tree transducer we propose models MT as a probabilistic optimization process. Let f be the input sentence (foreign language), and e be the output sentence (English). We have lation model” (TM) and the “language model” (LM). Assuming the decomposition of the foreign tree is given, our approach, which is based on ETs, uses the graphical model shown in Figure 6. In the model, the left side is the input dependency tree (foreign language) and the right side is the output dependency tree (English). Each circle stands for an ET. The solid lines denote the syntactical dependencies while the dashed arrows denote the statistical dependencies. Let T( x) be the dependency tree constructed from sentence x . A tree-decomposition function D( t) is defined on a dependency tree t , and outputs a certain ET derivation tree of t , which is generated by decomposing t into ETs. Given t , there could be multiple decompositions. Conditioned on decomposition D , we can rewrite (2) as: By definition, the ET derivation trees of the input and output trees should be isomorphic: D(T(f )) ≅ D(T(e)) . Let Tran(u) be a set of possible translations for the ET u . We have: For any ET v in a given ET derivation tree d , let Root(d) be the root ET of d , and let Parent(v) denote the parent ET of v . We have: an HMM. While HMM is defined on a sequence our model is defined on the derivation tree of ETs. In reality, the learned parallel ETs are unlikely to cover all the structures that we may encounter in decoding. As a unified approach, we augment the SDIG by adding all the possible word pairs (fj, ei) as a parallel ET pair and using the IBM Model 1 (Brown et al., 1993) word to word translation probability as the ET translation probability. For efficiency reasons, we use maximum approximation for (3). Instead of summing over all the possible decompositions, we only search for the best decomposition as follows: where, letting root(v) denote the root word of v , P(v  |Parent(v)) = P(root(v)  |root (Parent(v))) (6) The prior probability of tree decomposition is An analogy between our model and a Hidden Markov Model (Figure 7) may be helpful. In Eq. (4), P(u  |v) is analogous to the emission probably P(oi  |si) in an HMM. In Eq. (5), P(v  |Parent(v)) is analogous to the transition probability P(si  |si−1) in So bringing equations (4) to (9) together, the best translation would maximize: Observing the similarity between our model and a HMM, our dynamic programming decoding algorithm is in spirit similar to the Viterbi algorithm except that instead of being sequential the decoding is done on trees in a top down fashion. As to the relative orders of the ETs, we currently choose not to reorder the children ETs given the parent ET because: (1) the permutation of the ETs is computationally expensive (2) it is possible that we can resort to simple linguistic treatments on the output dependency tree to order the ETs. Currently, all the ETs are attached to each other at their root nodes. In our implementation, the different decompositions of the input dependency tree are stored in a shared forest structure, utilizing the dynamic programming property of the tree structures explicitly. Suppose the input sentence has n words and the shared forest representation has m nodes. Suppose for each word, there are maximally k different ETs containing it, we have m <_ kn . Let b be the max breadth factor in the packed forest, it can be shown that the decoder visits at most mb nodes during execution. Hence, we have: which is linear to the input size. Combined with a polynomial time parsing algorithm, the whole decoding process is polynomial time. We implemented the above approach for a Chinese-English machine translation system. We used an automatic syntactic parser (Bikel, 2002) to produce the parallel parse trees. The parser was trained using the Penn English/Chinese Treebanks. We then used the algorithm in (Xia 2001) to convert the phrasal structure trees to dependency trees to acquire the parallel dependency trees. The statistics of the datasets we used are shown as follows: The training set consists of Xinhua newswire data from LDC and the FBIS data (mostly news), both filtered to ensure parallel sentence pair quality. We used the development test data from the 2001 NIST MT evaluation workshop as our test data for the MT system performance. In the testing data, each input Chinese sentence has 4 English translations as references. Our MT system was evaluated using the n-gram based Bleu (Papineni et al., 2002) and NIST machine translation evaluation software. We used the NIST software package “mteval” version 11a, configured as case-insensitive. In comparison, we deployed the GIZA++ MT modeling tool kit, which is an implementation of the IBM Models 1 to 4 (Brown et al., 1993; AlOnaizan et al., 1999; Och and Ney, 2003). The IBM models were trained on the same training data as our system. We used the ISI Rewrite decoder (Germann et al. 2001) to decode the IBM models. The results are shown in Figure 9. The score types “I” and “C” stand for individual and cumulative n-gram scores. The final NIST and Bleu scores are marked with bold fonts. The evaluation results show that the NIST score achieved a 97.3% increase, while the Bleu score increased by 21.1%. In terms of decoding speed, the Rewrite decoder took 8102 seconds to decode the test sentences on a Xeon 1.2GHz 2GB memory machine. On the same machine, the SDIG decoder took 3 seconds to decode, excluding the parsing time. The recent advances in parsing have achieved parsers with 3 O(n ) time complexity without the grammar constant (McDonald et al., 2005). It can be expected that the total decoding time for SDIG can be as short as 0.1 second per sentence. Neither of the two systems has any specific translation components, which are usually present in real world systems (E.g. components that translate numbers, dates, names, etc.) It is reasonable to expect that the performance of SDIG can be further improved with such specific optimizations. We noticed that the SDIG system outputs tend to be longer than those of the IBM Model 4 system, and are closer to human translations in length. This partly explains why the IBM Model 4 system has slightly higher individual n-gram precision scores (while the SDIG system outputs are still better in terms of absolute matches). The relative orders between the parent and child ETs in the output tree is currently kept the same as the orders in the input tree. Admittedly, we benefited from the fact that both Chinese and English are SVO languages, and that many of orderings between the arguments and adjuncts can be kept the same. However, we did notice that this simple “ostrich” treatment caused outputs such as “foreign financial institutions the president of”. While statistical modeling of children reordering is one possible remedy for this problem, we believe simple linguistic treatment is another, as the output of the SDIG system is an English dependency tree rather than a string of words. In this paper we presented a syntax-based statistical MT system based on a Synchronous Dependency Insertion Grammar and a non-isomorphic stochastic tree-to-tree transducer. A graphical model for the transducer is defined and a polynomial time decoding algorithm is introduced. The results of our current implementation were evaluated using the NIST and Bleu automatic MT evaluation software. The evaluation shows that the SDIG system outperforms an IBM Model 4 based system in both speed and quality. Future work includes a full-fledged version of SDIG and a more sophisticated MT pipeline with possibly a tri-gram language model for decoding.","Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.
In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar.
Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.
We first introduce our approach to inducing such a grammar from parallel corpora.
Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.
We introduce a polynomial time decoding algorithm for the model.
We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software.
The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality.
Our approach requires some assumptions on the level of isomorphism (lexical and/or structural) between two languages.
We present a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures.
","In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT.Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation.We first introduce our approach to inducing such a grammar",0.8521901965141296,0.8258283138275146,0.8388020992279053
"Joshua: An Open Source Toolkit for Parsing-Based Machine Translation describe an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context grammars (SCFGs): chart-parsing, gram language model integration, beamcube-pruning, and extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 2008b). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a). We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.' When designing our toolkit, we applied general principles of software engineering to achieve three major goals: Extensibility, end-to-end coherence, and scalability. Extensibility: The Joshua code is organized into separate packages for each major aspect of functionality. In this way it is clear which files contribute to a given functionality and researchers can focus on a single package without worrying about the rest of the system. Moreover, to minimize the problems of unintended interactions and unseen dependencies, which is common hinderance to extensibility in large projects, all extensible components are defined by Java interfaces. Where there is a clear point of departure for research, a basic implementation of each interface is provided as an abstract class to minimize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hindering repeatability of experiments. To combat these issues, the Joshua toolkit integrates most critical components of the machine translation pipeline. Moreover, each component can be treated as a stand-alone tool and does not rely on the rest of the toolkit we provide. Scalability: Our third design goal was to ensure that the decoder is scalable to large models and data sets. The parsing and pruning algorithms are carefully implemented with dynamic programming strategies, and efficient data structures are used to minimize overhead. Other techniques contributing to scalability includes suffix-array grammar extraction, parallel and distributed decoding, and bloom filter language models. Below we give a short description about the main functions implemented in our Joshua toolkit. Rather than inducing a grammar from the full parallel training data, we made use of a method proposed by Kishore Papineni (personal communication) to select the subset of the training data consisting of sentences useful for inducing a grammar to translate a particular test set. This method works as follows: for the development and test sets that will be translated, every n-gram (up to length 10) is gathered into a map W and associated with an initial count of zero. Proceeding in order through the training data, for each sentence pair whose source-to-target length ratio is within one standard deviation of the average, if any n-gram found in the source sentence is also found in W with a count of less than k, the sentence is selected. When a sentence is selected, the count of every n-gram in W that is found in the source sentence is incremented by the number of its occurrences in the source sentence. For our submission, we used k = 20, which resulted in 1.5 million (out of 23 million) sentence pairs being selected for use as training data. There were 30,037,600 English words and 30,083,927 French words in the subsampled training corpus. Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values. In real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory. In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f|e) and reverse translation probability p(e|f) (Koehn et al., 2003). Since the extraction steps must be re-run if any change is made to the input training data, the time required can be a major hindrance to researchers, especially those investigating the effects of tokenization or word segmentation. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences. This results in a vastly smaller rule set than techniques which extract all rules from the training set. The current code requires suffix array rule extraction to be run as a pre-processing step to extract the rules needed to translate a particular test set. However, we are currently extending the decoder to directly access the suffix array. This will allow the decoder at runtime to efficiently extract exactly those rules needed to translate a particular sentence, without the need for a rule extraction pre-processing step. Grammar formalism: Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Currently, it only handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs (e.g., (Galley et al., 2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). Chart parsing: Given a source sentence to decode, the decoder generates a one-best or k-best translations using a CKY algorithm. Specifically, the decoding algorithm maintains a chart, which contains an array of cells. Each cell in turn maintains a list of proven items. The parsing process starts with the axioms, and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backpointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivations from the hypergraph. Parallel and distributed decoding: We also implement parallel decoding and a distributed language model by exploiting multi-core and multi-processor architectures and distributed computing techniques. More details on these two features are provided by Li and Khudanpur (2008b). In addition to the distributed LM mentioned above, we implement three local n-gram language models. Specifically, we first provide a straightforward implementation of the n-gram scoring function in Java. This Java implementation is able to read the standard ARPA backoff n-gram models, and thus the decoder can be used independently from the SRILM toolkit.3 We also provide a native code bridge that allows the decoder to use the SRILM toolkit to read and score n-grams. This native implementation is more scalable than the basic Java LM implementation. We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). Johsua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu. The optimization consists of a series of line-optimizations along the dimensions corresponding to the parameters. The search across a dimension uses the efficient method of Och (2003). Each iteration of our MERT implementation consists of multiple weight updates, each reflecting a greedy selection of the dimension giving the most gain. Each iteration also optimizes several random “intermediate initial” points in addition to the one surviving from the previous iteration, as an approximation to performing multiple random restarts. More details on the MERT method and the implementation can be found in Zaidan (2009).4 We assembled a very large French-English training corpus (Callison-Burch, 2009) by conducting a web crawl that targted bilingual web sites from the Canadian government, the European Union, and various international organizations like the Amnesty International and the Olympic Committee. The crawl gathered approximately 40 million files, consisting of over 1TB of data. We converted pdf, doc, html, asp, php, etc. files into text, and preserved the directory structure of the web crawl. We wrote set of simple heuristics to transform French URLs onto English URLs, and considered matching documents to be translations of each other. This yielded 2 million French documents paired with their English equivalents. We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002). We filtered and de-duplcated the resulting parallel corpus. After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words. We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words. Our translation model was trained on these corpora using the subsampling descried in Section 2.1. For language model training, we used the monolingual news and blog data that was assembled by the University of Edinburgh and distributed as part of WMT09. This data consisted of 21.2 million English sentences with half a billion words. We used SRILM to train a 5-gram language model using a vocabulary containing the 500,000 most frequent words in this corpus. Note that we did not use the English side of the parallel corpus as language model training data. To tune the system parameters we used News Test Set from WMT08 (Callison-Burch et al., 2008), which consists of 2,051 sentence pairs with 43 thousand English words and 46 thousand French words. This is in-domain data that was gathered from the same news sources as the WMT09 test set. The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Variational Decoding: Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations). In principle, the goodness of a string is measured by the total probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. More details will be provided in Li et al. (2009b). In this system, we have used both deterministic annealing (for training) and variational decoding (for decoding). We have described a scalable toolkit for parsingbased machine translation. It is written in Java and implements all the essential algorithms described in Chiang (2007) and Li and Khudanpur (2008b): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffixarray grammar extraction (Callison-Burch et al., 2005; Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable. The decoder achieves state of the art translation performance. This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No. HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone.","Joshua: An Open Source Toolkit for Parsing-Based Machine Translation
We describe Joshua, an open source toolkit for statistical machine translation.
Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array grammar extraction and minimum error rate training.
It uses parallel and distributed computing techniques for scalability.
We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task.
We develop the syntax-based MT system Joshua, which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimization.
","Joshua: An Open Source Toolkit for Parsing-Based Machine Translation describe an open source toolkit for statistical machine translation.Joshua implements all of the algorithms required for synchronous context grammars (SCFGs): chart-parsing, gram language model integration, beamcube-pruning, and extraction.The toolkit also implements suffix-array grammar extraction and minimum error rate training (Och, 2003).We have also made great effort to ensure that our toolkit is easy to use and to extend.We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task",0.9480239152908325,0.9175183773040771,0.9325217008590698
"Incrementality In Deterministic Dependency Parsing Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%. Incrementality in parsing has been advocated for at least two different reasons. The first is mainly practical and has to do with real-time applications such as speech recognition, which require a continually updated analysis of the input received so far. The second reason is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguistic evidence suggesting that human parsing is largely incremental (Marslen-Wilson, 1973; Frazier, 1987). However, most state-of-the-art parsing methods today do not adhere to the principle of incrementality, for different reasons. Parsers that attempt to disambiguate the input completely — full parsing — typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input — partial parsing — are usually deterministic and construct the final analysis in one pass over the input (Abney, 1991; Daelemans et al., 1999). But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason. Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic parsing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003). In some ways, this approach can be seen as a compromise between traditional full and partial parsing. Essentially, it is a kind of full parsing in that the goal is to build a complete syntactic analysis for the input string, not just identify major constituents. But it resembles partial parsing in being robust, efficient and deterministic. Taken together, these properties seem to make dependency parsing suitable for incremental processing, although existing implementations normally do not satisfy this constraint. For example, Yamada and Matsumoto (2003) use a multipass bottom-up algorithm, combined with support vector machines, in a way that does not result in incremental processing. In this paper, we analyze the constraints on incrementality in deterministic dependency parsing and argue that strict incrementality is not achievable. We then analyze the algorithm proposed in Nivre (2003) and show that, given the previous result, this algorithm is optimal from the point of view of incrementality. Finally, we evaluate experimentally the degree of incrementality achieved with the algorithm in practical parsing. In a dependency structure, every word token is dependent on at most one other word token, usually called its head or regent, which means that the structure can be represented as a directed graph, with nodes representing word tokens and arcs representing dependency relations. In addition, arcs may be labeled with specific dependency types. Figure 1 shows a labeled dependency graph for a simple Swedish sentence, where each word of the sentence is labeled with its part of speech and each arc labeled with a grammatical function. In the following, we will restrict our attention to unlabeled dependency graphs, i.e. graphs without labeled arcs, but the results will apply to labeled dependency graphs as well. We will also restrict ourselves to projective dependency graphs (Mel’cuk, 1988). Formally, we define these structures in the following way: We write wz < wj to express that wz precedes wj in the string W (i.e., i < j); we write wz —* wj to say that there is an arc from wz to wj; we use —** to denote the reflexive and transitive closure of the arc relation; and we use H and H* for the corresponding undirected relations, i.e. wz H wj iff wz —* wj or wj —* wz. 2. A dependency graph D = (W, A) is wellformed iff the five conditions given in Figure 2 are satisfied. The task of mapping a string W = w1· · ·wn to a dependency graph satisfying these conditions is what we call dependency parsing. For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). Having defined dependency graphs, we may now consider to what extent it is possible to construct these graphs incrementally. In the strictest sense, we take incrementality to mean that, at any point during the parsing process, there is a single connected structure representing the analysis of the input consumed so far. In terms of our dependency graphs, this would mean that the graph being built during parsing is connected at all times. We will try to make this more precise in a minute, but first we want to discuss the relation between incrementality and determinism. It seems that incrementality does not by itself imply determinism, at least not in the sense of never undoing previously made decisions. Thus, a parsing method that involves backtracking can be incremental, provided that the backtracking is implemented in such a way that we can always maintain a single structure representing the input processed up to the point of backtracking. In the context of dependency parsing, a case in point is the parsing method proposed by Kromann (Kromann, 2002), which combines heuristic search with different repair mechanisms. In this paper, we will nevertheless restrict our attention to deterministic methods for dependency parsing, because we think it is easier to pinpoint the essential constraints within a more restrictive framework. We will formalize deterministic dependency parsing in a way which is inspired by traditional shift-reduce parsing for context-free grammars, using a buffer of input tokens and a stack for storing previously processed input. However, since there are no nonterminal symbols involved in dependency parsing, we also need to maintain a representation of the dependency graph being constructed during processing. We will represent parser configurations by triples (5, I, A), where 5 is the stack (represented as a list), I is the list of (remaining) input tokens, and A is the (current) arc relation for the dependency graph. (Since the nodes of the dependency graph are given by the input string, only the arc relation needs to be represented explicitly.) Given an input string W, the parser is initialized to (nil, W, 0) and terminates when it reaches a configuration (5, nil, A) (for any list 5 and set of arcs A). The input string W is accepted if the dependency graph D = (W, A) given at termination is well-formed; otherwise W is rejected. In order to understand the constraints on incrementality in dependency parsing, we will begin by considering the most straightforward parsing strategy, i.e. left-to-right bottom-up parsing, which in this case is essentially equivalent to shift-reduce parsing with a context-free grammar in Chomsky normal form. The parser is defined in the form of a transition system, represented in Figure 3 (where wi and wj are arbitrary word tokens): the two topmost tokens on the stack, wi and wj, by a right-directed arc wi —* wj and reduces them to the head wi. 3. The transition Shift pushes the next input token wi onto the stack. The transitions Left-Reduce and RightReduce are subject to conditions that ensure that the Single head condition is satisfied. For Shift, the only condition is that the input list is non-empty. As it stands, this transition system is nondeterministic, since several transitions can often be applied to the same configuration. Thus, in order to get a deterministic parser, we need to introduce a mechanism for resolving transition conflicts. Regardless of which mechanism is used, the parser is guaranteed to terminate after at most 2n transitions, given an input string of length n. Moreover, the parser is guaranteed to produce a dependency graph that is acyclic and projective (and satisfies the singlehead constraint). This means that the dependency graph given at termination is well-formed if and only if it is connected. We can now define what it means for the parsing to be incremental in this framework. Ideally, we would like to require that the graph (W —I, A) is connected at all times. However, given the definition of Left-Reduce and Right-Reduce, it is impossible to connect a new word without shifting it to the stack first, so it seems that a more reasonable condition is that the size of the stack should never exceed 2. In this way, we require every word to be attached somewhere in the dependency graph as soon as it has been shifted onto the stack. We may now ask whether it is possible to achieve incrementality with a left-to-right bottom-up dependency parser, and the answer turns out to be no in the general case. This can be demonstrated by considering all the possible projective dependency graphs containing only three nodes and checking which of these can be parsed incrementally. Figure 4 shows the relevant structures, of which there are seven altogether. We begin by noting that trees (2–5) can all be constructed incrementally by shifting the first two tokens onto the stack, then reducing – with Right-Reduce in (2–3) and Left-Reduce in (4–5) – and then shifting and reducing again – with Right-Reduce in (2) and (4) and LeftReduce in (3) and (5). By contrast, the three remaining trees all require that three tokens are Initialization hnil, W, ∅i Termination hS, nil, Ai Left-Reduce hwjwi|S, I, Ai → hwj|S, I, A ∪ {(wj, wi)}i ¬∃wk(wk, wi) ∈ A Right-Reduce hwjwi|S, I, Ai → hwi|S, I, A ∪ {(wi, wj)}i ¬∃wk(wk, wj) ∈ A Shift hS, wi|I, Ai → hwi|S, I, Ai shifted onto the stack before the first reduction. However, the reason why we cannot parse the structure incrementally is different in (1) compared to (6–7). In (6–7) the problem is that the first two tokens are not connected by a single arc in the final dependency graph. In (6) they are sisters, both being dependents on the third token; in (7) the first is the grandparent of the second. And in pure dependency parsing without nonterminal symbols, every reduction requires that one of the tokens reduced is the head of the other(s). This holds necessarily, regardless of the algorithm used, and is the reason why it is impossible to achieve strict incrementality in dependency parsing as defined here. However, it is worth noting that (2–3), which are the mirror images of (6–7) can be parsed incrementally, even though they contain adjacent tokens that are not linked by a single arc. The reason is that in (2–3) the reduction of the first two tokens makes the third token adjacent to the first. Thus, the defining characteristic of the problematic structures is that precisely the leftmost tokens are not linked directly. The case of (1) is different in that here the problem is caused by the strict bottom-up strategy, which requires each token to have found all its dependents before it is combined with its head. For left-dependents this is not a problem, as can be seen in (5), which can be processed by alternating Shift and Left-Reduce. But in (1) the sequence of reductions has to be performed from right to left as it were, which rules out strict incrementality. However, whereas the structures exemplified in (6–7) can never be processed incrementally within the present framework, the structure in (1) can be handled by modifying the parsing strategy, as we shall see in the next section. It is instructive at this point to make a comparison with incremental parsing based on extended categorial grammar, where the structures in (6–7) would normally be handled by some kind of concatenation (or product), which does not correspond to any real semantic combination of the constituents (Steedman, 2000; Morrill, 2000). By contrast, the structure in (1) would typically be handled by function composition, which corresponds to a well-defined compositional semantic operation. Hence, it might be argued that the treatment of (6–7) is only pseudo-incremental even in other frameworks. Before we leave the strict bottom-up approach, it can be noted that the algorithm described in this section is essentially the algorithm used by Yamada and Matsumoto (2003) in combination with support vector machines, except that they allow parsing to be performed in multiple passes, where the graph produced in one pass is given as input to the next pass.' The main motivation they give for parsing in multiple passes is precisely the fact that the bottomup strategy requires each token to have found all its dependents before it is combined with its head, which is also what prevents the incremental parsing of structures like (1). In order to increase the incrementality of deterministic dependency parsing, we need to combine bottom-up and top-down processing. More precisely, we need to process left-dependents bottom-up and right-dependents top-down. In this way, arcs will be added to the dependency graph as soon as the respective head and dependent are available, even if the dependent is not complete with respect to its own dependents. Following Abney and Johnson (1991), we will call this arc-eager parsing, to distinguish it from the standard bottom-up strategy discussed in the previous section. Using the same representation of parser configurations as before, the arc-eager algorithm can be defined by the transitions given in Figure 5, where wi and wj are arbitrary word tokens (Nivre, 2003): the stack to the next input token wj, and pushes wj onto the stack. 4. The transition Shift (SH) pushes the next input token wi onto the stack. The transitions Left-Arc and Right-Arc, like their counterparts Left-Reduce and RightReduce, are subject to conditions that ensure lA purely terminological, but potentially confusing, difference is that Yamada and Matsumoto (2003) use the term Right for what we call Left-Reduce and the term Left for Right-Reduce (thus focusing on the position of the head instead of the position of the dependent). that the Single head constraint is satisfied, while the Reduce transition can only be applied if the token on top of the stack already has a head. The Shift transition is the same as before and can be applied as long as the input list is non-empty. Comparing the two algorithms, we see that the Left-Arc transition of the arc-eager algorithm corresponds directly to the Left-Reduce transition of the standard bottom-up algorithm. The only difference is that, for reasons of symmetry, the former applies to the token on top of the stack and the next input token instead of the two topmost tokens on the stack. If we compare Right-Arc to Right-Reduce, however, we see that the former performs no reduction but simply shifts the newly attached right-dependent onto the stack, thus making it possible for this dependent to have rightdependents of its own. But in order to allow multiple right-dependents, we must also have a mechanism for popping right-dependents off the stack, and this is the function of the Reduce transition. Thus, we can say that the action performed by the Right-Reduce transition in the standard bottom-up algorithm is performed by a Right-Arc transition in combination with a subsequent Reduce transition in the arc-eager algorithm. And since the RightArc and the Reduce can be separated by an arbitrary number of transitions, this permits the incremental parsing of arbitrary long rightdependent chains. Defining incrementality is less straightforward for the arc-eager algorithm than for the standard bottom-up algorithm. Simply considering the size of the stack will not do anymore, since the stack may now contain sequences of tokens that form connected components of the dependency graph. On the other hand, since it is no longer necessary to shift both tokens to be combined onto the stack, and since any tokens that are popped off the stack are connected to some token on the stack, we can require that the graph (5, AS) should be connected at all times, where AS is the restriction of A to 5, i.e. AS = {(wi, wj) E A|wi, wj E 51. Given this definition of incrementality, it is easy to show that structures (2–5) in Figure 4 can be parsed incrementally with the arc-eager algorithm as well as with the standard bottomup algorithm. However, with the new algorithm we can also parse structure (1) incrementally, as wj Initialization hnil, W, ∅i Termination hS, nil, Ai Left-Arc hwi|S, wj|I, Ai → hS, wj|I, A ∪ {(wj, wi)}i ¬∃wk(wk, wi) ∈ A Right-Arc hwi|S, wj|I, Ai → hwj|wi|S, I, A ∪ {(wi, wj)}i ¬∃wk(wk, wj) ∈ A Reduce hwi|S, I, Ai → hS, I, Ai ∃wj(wj, wi) ∈ A Shift hS, wi|I, Ai → hwi|S, I, Ai is shown by the following transition sequence: hnil, abc, ∅i ↓ (Shift) ha, bc, ∅i ↓ (Right-Arc) hba, c, {(a, b)}i ↓ (Right-Arc) hcba, nil, {(a, b), (b, c)}i We conclude that the arc-eager algorithm is optimal with respect to incrementality in dependency parsing, even though it still holds true that the structures (6–7) in Figure 4 cannot be parsed incrementally. This raises the question how frequently these structures are found in practical parsing, which is equivalent to asking how often the arc-eager algorithm deviates from strictly incremental processing. Although the answer obviously depends on which language and which theoretical framework we consider, we will attempt to give at least a partial answer to this question in the next section. Before that, however, we want to relate our results to some previous work on context-free parsing. First of all, it should be observed that the terms top-down and bottom-up take on a slightly different meaning in the context of dependency parsing, as compared to their standard use in context-free parsing. Since there are no nonterminal nodes in a dependency graph, top-down construction means that a head is attached to a dependent before the dependent is attached to (some of) its dependents, whereas bottomup construction means that a dependent is attached to its head before the head is attached to its head. However, top-down construction of dependency graphs does not involve the prediction of lower nodes from higher nodes, since all nodes are given by the input string. Hence, in terms of what drives the parsing process, all algorithms discussed here correspond to bottom-up algorithms in context-free parsing. It is interesting to note that if we recast the problem of dependency parsing as context-free parsing with a CNF grammar, then the problematic structures (1), (6–7) in Figure 4 all correspond to rightbranching structures, and it is well-known that bottom-up parsers may require an unbounded amount of memory in order to process rightbranching structure (Miller and Chomsky, 1963; Abney and Johnson, 1991). Moreover, if we analyze the two algorithms discussed here in the framework of Abney and Johnson (1991), they do not differ at all as to the order in which nodes are enumerated, but only with respect to the order in which arcs are enumerated; the first algorithm is arc-standard while the second is arc-eager. One of the observations made by Abney and Johnson (1991), is that arc-eager strategies for context-free parsing may sometimes require less space than arcstandard strategies, although they may lead to an increase in local ambiguities. It seems that the advantage of the arc-eager strategy for dependency parsing with respect to structure (1) in Figure 4 can be explained along the same lines, although the lack of nonterminal nodes in dependency graphs means that there is no corresponding increase in local ambiguities. Although a detailed discussion of the relation between context-free parsing and dependency parsing is beyond the scope of this paper, we conjecture that this may be a genuine advantage of dependency representations in parsing. In order to measure the degree of incrementality achieved in practical parsing, we have evaluated a parser that uses the arc-eager parsing algorithm in combination with a memory-based classifier for predicting the next transition. In experiments reported in Nivre et al. (2004), a parsing accuracy of 85.7% (unlabeled attachment score) was achieved, using data from a small treebank of Swedish (Einarsson, 1976), divided into a training set of 5054 sentences and a test set of 631 sentences. However, in the present context, we are primarily interested in the incrementality of the parser, which we measure by considering the number of connected components in (5, As) at different stages during the parsing of the test data. The results can be found in Table 1, where we see that out of 16545 configurations used in parsing 613 sentences (with a mean length of 14.0 words), 68.9% have zero or one connected component on the stack, which is what we require of a strictly incremental parser. We also see that most violations of incrementality are fairly mild, since more than 90% of all configurations have no more than three connected components on the stack. Many violations of incrementality are caused by sentences that cannot be parsed into a wellformed dependency graph, i.e. a single projective dependency tree, but where the output of the parser is a set of internally connected components. In order to test the influence of incomplete parses on the statistics of incrementality, we have performed a second experiment, where we restrict the test data to those 444 sentences (out of 613), for which the parser produces a well-formed dependency graph. The results can be seen in Table 2. In this case, 87.1% of all configurations in fact satisfy the constraints of incrementality, and the proportion of configurations that have no more than three connected components on the stack is as high as 99.5%. It seems fair to conclude that, although strict word-by-word incrementality is not possible in deterministic dependency parsing, the arc-eager algorithm can in practice be seen as a close approximation of incremental parsing. In this paper, we have analyzed the potential for incremental processing in deterministic dependency parsing. Our first result is negative, since we have shown that strict incrementality is not achievable within the restrictive parsing framework considered here. However, we have also shown that the arc-eager parsing algorithm is optimal for incremental dependency parsing, given the constraints imposed by the overall framework. Moreover, we have shown that in practical parsing, the algorithm performs incremental processing for the majority of input structures. If we consider all sentences in the test data, the share is roughly two thirds, but if we limit our attention to well-formed output, it is almost 90%. Since deterministic dependency parsing has previously been shown to be competitive in terms of parsing accuracy (Yamada and Matsumoto, 2003; Nivre et al., 2004), we believe that this is a promising approach for situations that require parsing to be robust, efficient and (almost) incremental. The work presented in this paper was supported by a grant from the Swedish Research Council (621-2002-4207). The memorybased classifiers used in the experiments were constructed using the Tilburg Memory-Based Learner (TiMBL) (Daelemans et al., 2003). Thanks to three anonymous reviewers for constructive comments on the submitted paper.","Incrementality In Deterministic Dependency Parsing
Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text.
In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework.
However, we also show that it is possible to minimize the number of structures that require non-incremental processing by choosing an optimal parsing algorithm.
This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text.
","Incrementality In Deterministic Dependency Parsing Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text.The first reason is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguistic evidence suggesting that human parsing is largely incremental (Marslen-Wilson, 1973; Frazier, 1987).In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework.In this way, we evaluate experimentally the degree of incrementality achieved with the algorithm in practical parsing.In the following, we will restrict our attention to unlabeled dependency graphs, i.e. graphs without labeled arcs, but the results",0.8873542547225952,0.914341151714325,0.9006456136703491
"SemEval-2007 Task 15: TempEval Temporal Relation Identification The TempEval task proposes a simple way to evaluate automatic extraction of temporalrelations. It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise eval uation of temporal relations. The task not only allows straightforward evaluation, italso avoids the complexities of full tempo ral parsing. Newspaper texts, narratives and other texts describe events that occur in time and specify the temporallocation and order of these events. Text comprehen sion, amongst other capabilities, clearly requires the capability to identify the events described in a text and locate these in time. This capability is crucial to a wide range of NLP applications, from document summarization and question answering to machine translation.Recent work on the annotation of events and temporal relations has resulted in both a de-facto stan dard for expressing these relations and a hand-builtgold standard of annotated texts. TimeML (Puste jovsky et al, 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al, 2003b; Boguraev et al., forthcoming) was originally conceived of as aproof of concept that illustrates the TimeML lan guage, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al, 2006; Boguraev et al, forthcoming).An open evaluation challenge in the area of temporal annotation should serve to drive research forward, as it has in other areas of NLP. The automatic identification of all temporal referring expres sions, events and temporal relations within a text is the ultimate aim of research in this area. However, addressing this aim in a first evaluation challenge was judged to be too difficult, both for organizers and participants, and a staged approach was deemedmore effective. Thus we here present an initial eval uation exercise based on three limited tasks that webelieve are realistic both from the perspective of as sembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks. They are also tasks, whichshould they be performable automatically, have ap plication potential. The tasks as originally proposed were modified slightly during the course of resource development for the evaluation exercise due to constraints on dataand annotator availability. In the following we de scribe the tasks as they were ultimately realized in the evaluation. There were three tasks ? A, B and C. For allthree tasks the data provided for testing and train ing includes annotations identifying: (1) sentence boundaries; (2) all temporal referring expression as 75 specified by TIMEX3; (3) all events as specifiedin TimeML; (4) selected instances of temporal re lations, as relevant to the given task. For tasks A and B a restricted set of event terms were identified ? those whose stems occurred twenty times or more in TimeBank. This set is referred to as the Event Target List or ETL.TASK A This task addresses only the temporal re lations holding between time and event expressions that occur within the same sentence. Furthermore only event expressions that occur within the ETL areconsidered. In the training and test data, TLINK an notations for these temporal relations are provided, the difference being that in the test data the relation type is withheld. The task is to supply this label. TASK B This task addresses only the temporal relations holding between the Document Creation Time (DCT) and event expressions. Again onlyevent expressions that occur within the ETL are con sidered. As in Task A, TLINK annotations for these temporal relations are provided in both training and test data, and again the relation type is withheld in the test data and the task is to supply this label. TASK C Task C relies upon the idea of their beinga main event within a sentence, typically the syn tactically dominant verb. The aim is to assign thetemporal relation between the main events of adja cent sentences. In both training and test data the main events are identified (via an attribute in the event annotation) and TLINKs between these main events are supplied. As for Tasks A and B, the task here is to supply the correct relation label for these TLINKs. The TempEval annotation language is a simplifiedversion of TimeML 1. For TempEval, we use the fol lowing five tags: TempEval, s, TIMEX3, EVENT, and TLINK. TempEval is the document root and s marks sentence boundaries. All sentence tags in the TempEval data are automatically created using the Alembic Natural Language processing tools. The other three tags are discussed here in more detail:1See http://www.timeml.org for language specifica tions and annotation guidelines ? TIMEX3. Tags the time expressions in the text. It is identical to the TIMEX3 tag in TimeML. See the TimeML specifications and guidelines for further details on this tag and its attributes. Each document has one special TIMEX3 tag,the Document Creation Time, which is inter preted as an interval that spans a whole day. EVENT. Tags the event expressions in the text. The interpretation of what an event is is taken from TimeML where an event is a cover term for predicates describing situations that happen or occur as well as some, but not all, stative predicates. Events can be denoted by verbs,nouns or adjectives. The TempEval event an notation scheme is somewhat simpler than thatused in TimeML, whose complexity was designed to handle event expressions that intro duced multiple event instances (consider, e.g. He taught on Wednesday and Friday). Thiscomplication was not necessary for the Tem pEval data. The most salient attributes encodetense, aspect, modality and polarity informa tion. For TempEval task C, one extra attribute is added: mainevent, with possible values YES and NO. ? TLINK. This is a simplified version of the TimeML TLINK tag. The relation types for the TimeML version form a fine-grained set based on James Allen?s interval logic (Allen, 1983). For TempEval, we use only six relation typesincluding the three core relations BEFORE, AFTER, and OVERLAP, the two less specific relations BEFORE-OR-OVERLAP and OVERLAP OR-AFTER for ambiguous cases, and finally therelation VAGUE for those cases where no partic ular relation can be established. As stated above the TLINKs of concern for each task are explicitly included in the training and in thetest data. However, in the latter the relType at tribute of each TLINK is set to UNKNOWN. For each task the system must replace the UNKNOWN values with one of the six allowed values listed above. The EVENT and TIMEX3 annotations were takenverbatim from TimeBank version 1.2.2 The annota 2TimeBank 1.2 is available for free through the Linguistic Data Consortium, see http://www.timeml.org for more 76tion procedure for TLINK tags involved dual annotation by seven annotators using a web-based anno tation interface. After this phase, three experiencedannotators looked at all occurrences where two an notators differed as to what relation type to select and decided on the best option. For task C, there was an extra annotation phase where the main events were marked up. Main events are those events that are syntactically dominant in the sentences.It should be noted that annotation of temporal relations is not an easy task for humans due to ram pant temporal vagueness in natural language. As aresult, inter-annotator agreement scores are well be low the often kicked-around threshold of 90%, both for the TimeML relation set as well as the TempEvalrelation set. For TimeML temporal links, an inter annotator agreement of 0.77 was reported, whereagreement was measured by the average of preci sion and recall. The numbers for TempEval are even lower, with an agreement of 0.72 for anchorings of events to times (tasks A and B) and an agreement of0.65 for event orderings (task C). Obviously, num bers like this temper the expectations for automatic temporal linking. The lower number for TempEval came a bit asa surprise because, after all, there were fewer relations to choose form. However, the TempEval an notation task is different in the sense that it did not give the annotator the option to ignore certain pairs of events and made it therefore impossible to skip hard-to-classify temporal relations. In full temporal annotation, evaluation of temporal annotation runs into the same issues as evaluation of anaphora chains: simple pairwise comparisons maynot be the best way to evaluate. In temporal annota tion, for example, one may wonder how the response in (1) should be evaluated given the key in (2). (1) {A before B, A before C, B equals C} (2) {A after B, A after C, B equals C}Scoring (1) at 0.33 precision misses the interde pendence between the temporal relations. What we need to compare is not individual judgements but two partial orders. details. For TempEval however, the tasks are defined in a such a way that a simple pairwise comparison is possible since we do not aim to create a full temporal graph and judgements are made in isolation. Recall that there are three basic temporal relations (BEFORE, OVERLAP, and AFTER) as well as three disjunctions over this set (BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and VAGUE). The addition of these disjunctions raises the question of how to score a response of, for example, BEFORE given akey of BEFORE-OR-OVERLAP. We use two scor ing schemes: strict and relaxed. The strict scoring scheme only counts exact matches as success. For example, if the key is OVERLAP and the responseBEFORE-OR-OVERLAP than this is counted as fail ure. We can use standard definitions of precision and recall Precision = Rc/R Recall = Rc/Kwhere Rc is number of correct answers in the response, R the total number of answers in the re sponse, and K the total number of answers in the key. For the relaxed scoring scheme, precision and recall are defined as Precision = Rcw/R Recall = Rcw/K where Rcw reflects the weighted number of correctanswers. A response is not simply counted as 1 (correct) or 0 (incorrect), but is assigned one of the val ues in table 1. B O A B-O O-A V B 1 0 0 0.5 0 0.33 O 0 1 0 0.5 0.5 0.33 A 0 0 1 0 0.5 0.33 B-O 0.5 0.5 0 1 0.5 0.67 O-A 0 0.5 0.5 0.5 1 0.67 V 0.33 0.33 0.33 0.67 0.67 1 Table 1: Evaluation weights This scheme gives partial credit for disjunctions,but not so much that non-commitment edges out pre cise assignments. For example, assigning VAGUE as the relation type for every temporal relation results in a precision of 0.33. 77 Six teams participated in the TempEval tasks. Three of the teams used statistics exclusively, one used arule-based system and the other two employed a hy brid approach. This section gives a short description of the participating systems. CU-TMP trained three support vector machine (SVM) models, one for each task. All models used the gold-standard TimeBank features for events and times as well as syntactic features derived from the text. Additionally, the relation types obtained by running the task B system on the training data for Task A and Task C, were added as a feature to the two latter systems. A subset of features was selectedusing cross-validations on the training data, discarding features whose removal improved the cross validation F-score. When applied to the test data, the Task B system was run first in order to supplythe necessary features to the Task A and Task C sys tems.LCC-TE automatically identifies temporal refer ring expressions, events and temporal relations in text using a hybrid approach, leveraging variousNLP tools and linguistic resources at LCC. For tem poral expression labeling and normalization, they used a syntactic pattern matching tool that deploys a large set of hand-crafted finite state rules. For event detection, they used a small set of heuristics as well as a lexicon to determine whether or not a token is an event, based on the lemma, part of speech and WordNet senses. For temporal relation discovery, LCC-TE used a large set of syntactic and semantic features as input to a machine learning components.NAIST-japan defined the temporal relation iden tification task as a sequence labeling model, in which the target pairs ? a TIMEX3 and an EVENT? are linearly ordered in the document. For analyz ing the relative positions, they used features fromdependency trees which are obtained from a dependency parser. The relative position between the tar get EVENT and a word in the target TIMEX3 is used as a feature for a machine learning based relation identifier. The relative positions between a word inthe target entities and another word are also intro duced. The USFD system uses an off-the-shelf Machine Learning suite(WEKA), treating the assignment of temporal relations as a simple classification task. The features used were the ones provided in theTempEval data annotation together with a few features straightforwardly computed from the docu ment without any deeper NLP analysis.WVALI?s approach for discovering intra sentence temporal relations relies on sentence-levelsyntactic tree generation, bottom-up propaga tion of the temporal relations between syntactic constituents, a temporal reasoning mechanism that relates the two targeted temporal entities to their closest ancestor and then to each other, and on conflict resolution heuristics. In establishing the temporal relation between an event and theDocument Creation Time (DCT), the temporal ex pressions directly or indirectly linked to that event are first analyzed and, if no relation is detected, the temporal relation with the DCT is propagatedtop-down in the syntactic tree. Inter-sentence tem poral relations are discovered by applying several heuristics and by using statistical data extracted from the training corpus. XRCE-T used a rule-based system that relies on a deep syntactic analyzer that was extended to treattemporal expressions. Temporal processing is inte grated into a more generic tool, a general purpose linguistic analyzer, and is thus a complement for a better general purpose text understanding system.Temporal analysis is intertwined with syntacticosemantic text processing like deep syntactic analysis and determination of thematic roles. TempEval specific treatment is performed in a post-processing stage. The results for the six teams are presented in tables 2, 3, and 4. team strict relaxed P R F P R F CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63 LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60 NAIST 0.61 0.61 0.61 0.63 0.63 0.63 USFD* 0.59 0.59 0.59 0.60 0.60 0.60 WVALI 0.62 0.62 0.62 0.64 0.64 0.64 XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41 average 0.59 0.54 0.56 0.62 0.57 0.59 stddev 0.03 0.13 0.10 0.01 0.12 0.08 Table 2: Results for Task A 78 team strict relaxed P R F P R F CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76 LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74 NAIST 0.75 0.75 0.75 0.76 0.76 0.76 USFD* 0.73 0.73 0.73 0.74 0.74 0.74 WVALI 0.80 0.80 0.80 0.81 0.81 0.81 XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71 average 0.76 0.72 0.74 0.78 0.74 0.75 stddev 0.03 0.08 0.05 0.03 0.06 0.03 Table 3: Results for Task B team strict relaxed P R F P R F CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58 LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58 NAIST 0.49 0.49 0.49 0.53 0.53 0.53 USFD* 0.54 0.54 0.54 0.57 0.57 0.57 WVALI 0.54 0.54 0.54 0.64 0.64 0.64 XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58 average 0.51 0.51 0.51 0.58 0.58 0.58 stddev 0.05 0.05 0.05 0.04 0.04 0.04 Table 4: Results for Task C All tables give precision, recall and f-measure for both the strict and the relaxed scoring scheme, aswell as averages and standard deviation on the pre cision, recall and f-measure numbers. The entry for USFD is starred because the system developers are co-organizers of the TempEval task.3 For task A, the f-measure scores range from 0.34 to 0.62 for the strict scheme and from 0.41 to 0.63 for the relaxed scheme. For task B, the scores range from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed). Finally, task C scores range from 0.42 to 0.55 (strict) and from 0.56 to 0.66 (relaxed).The differences between the systems is not spec tacular. WVALI?s hybrid approach outperforms the other systems in task B and, using relaxed scoring, in task C as well. But for task A, the winners barely edge out the rest of the field. Similarly, for task C using strict scoring, there is no system that clearly separates itself from the field.It should be noted that for task A, and in lesser ex tent for task B, the XRCE-T system has recall scores that are far below all other systems. This seemsmostly due to a choice by the developers to not as sign a temporal relation if the syntactic analyzer did not find a clear syntactic relation between the two 3There was a strict separation between people assisting in the annotation of the evaluation corpus and people involved in system development. elements that needed to be linked for the TempEval task. EvaluationThe evaluation approach of TempEval avoids the in terdependencies that are inherent to a network of temporal relations, where relations in one part of the network may constrain relations in any other part ofthe network. To accomplish that, TempEval delib erately focused on subtasks of the larger problem of automatic temporal annotation. One thing we may want to change to the present TempEval is the definition of task A. Currently, it instructs to temporally link all events in a sentence to all time expressions in the same sentence. In the future we may consider splitting this into two tasks, where one subtask focuses on those anchorings thatare very local, like ?...White House spokesman Marlin Fitzwater [said] [late yesterday] that...?. We expect both inter-annotator agreement and system per formance to be higher on this subtask. There are two research avenues that loom beyondthe current TempEval: (1) definition of other subtasks with the ultimate goal of establishing a hierar chy of subtasks ranked on performance of automatictaggers, and (2) an approach to evaluate entire time lines. Some other temporal linking tasks that can be considered are ordering of consecutive events in a sentence, ordering of events that occur in syntacticsubordination relations, ordering events in coordi nations, and temporal linking of reporting events to the document creation time. Once enough temporallinks from all these subtasks are added to the entire temporal graph, it becomes possible to let confidence scores from the separate subtasks drive a con straint propagation algorithm as proposed in (Allen, 1983), in effect using high-precision relations to constrain lower-precision relations elsewhere in the graph. With this more complete temporal annotation it is no longer possible to simply evaluate the entire graph by scoring pairwise comparisons. Instead the entire timeline must be evaluated. Initial ideas regarding this focus on transforming the temporal graph of a document into a set of partial orders built 79 around precedence and inclusion relations and then evaluating each of these partial orders using some kind of edit distance measure.4 We hope to have taken the first baby steps with the three TempEval tasks. We would like to thank all the people who helped prepare the data for TempEval, listed here in no particular order: Amber Stubbs, Jessica Littman, Hongyuan Qiu, Emin Mimaroglu, Emma Barker, Catherine Havasi, Yonit Boussany, Roser Saur??, and Anna Rumshisky. Thanks also to all participants to this new task: Steven Bethard and James Martin (University ofColorado at Boulder), Congmin Min, Munirathnam Srikanth and Abraham Fowler (Language Computer Corporation), Yuchang Cheng, Masayuki Asa hara and Yuji Matsumoto (Nara Institute of Science and Technology), Mark Hepple, Andrea Setzer and Rob Gaizauskas (University of Sheffield), CarolineHageg`e and Xavier Tannier (XEROX Research Cen tre Europe), and Georgiana Pus?cas?u (University of Wolverhampton and University of Alicante). Part of the work in this paper was funded bythe DTO/AQUAINT program under grant num ber N61339-06-C-0140 and part funded by the EU VIKEF project (IST- 2002-507173).","SemEval-2007 Task 15: TempEval Temporal Relation Identification
The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations.
It avoids the pitfalls of evaluating a graph of inter-related labels by defining three subtasks that allow pairwise evaluation of temporal relations.
The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing.
Temporal information processing is a topic of natural language processing boosted by our evaluation campaign TempEval.
TempEval07 (Verhagen et al, 2007) integrated 14 TLINK relations into three: before, after, and.
","In the training and test data, TLINK an notations for these temporal relations are provided, the difference being that in the test data the relation type is withheld.The task is to supply this label.TASK B This task addresses only the temporal relations holding between the Document Creation Time (DCT) and event expressions.The automatic identification of all temporal referring expres sions, events and temporal relations within a text is the ultimate aim of research in this area.TimBank (Puste jovsky et al, 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them.",0.8343483209609985,0.8404821157455444,0.8374040126800537
"A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. I(lavans, M. Liberman, M. Jl4arcus, S. Roukos, B. Santorini, T. Strzalkowski IBM Research Division, Thomas J. Watson Research Center Yorktown Heights, NY 10598 The problem of quantitatively comparing tile perfor- mance of different broad-coverage rammars of En- glish has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even tile simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (IIewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boe- ing), Don tfindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in com- mon only the following constituents, when each gram- marian provides the single parse which he/she would ideally want his/her grammar to specify for three sam- ple Brown Corpus sentences: The famed Yankee Clipper, now retired, has been as- sisting (as (a batting coach)). One of those cai)ital-gains ventures, ill fact, has sad- dled him (with Gore Court). lie said this constituted a (very serious) misuse (of the (Criminal court) processes). Specific differences among grammars which con- tribute to this apparent disparateness of analysis in- clude the treatmeat of punctuation as independent to- kens or, on the other hand, as parasites on the words to which they attach in writing; the recursive attach- ment of auxiliary elements to the right of Verb Phrase nodes, versus their incorporation there en bloc; the grouping of pre-infinitiva,1 ""to"" either with the main verb alone or with the entire Verb Phrase that it in- tro(luces; and the employment or non-employment of ""null nodes"" as a device in the grammar; as well as 306 other differences. Despite the seeming intractability of this problem, it appears to us that a solution to it is now at hand. We propose an evaluation pro- cedure with these characteristics: it judges a parse based only on the constituent boundaries it stipulates (and not the names it assigns to these constituents); it compares the parse to a ""hand-parse"" of the same sentence from the University of Pennsylvania Tree- bank; and it yields two principal measures for each parse submitted. The procedure has three steps. For each parse to be evaluated: (1) erase from the fully-parsed sentence all instances of: auxiliaries, ""not"", pre-infinitival ""to"", null categories, possessive ndings (% and ), and all word-external punctuation (e.g. "" , ; - ) ;  (2) recur- sively erase all parenthesis pairs enclosing either a sin- gle constituent or word, or nothing at all; (3) compute goodness cores (Crossing Parentheses, and Recall) for the input parse, by comparing it to a similarly- reduced version of the Penn Treebank parse of the same sentence. For example, for the Brown Corpus sentence: Miss Xydis was best when she did not need to be too probing, consider the candidate parse: (S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPAST was) (ADJP(ADJ best))) (S(COMP(WIIADVP(WI[ADV when))) ( ie -s  (PRO she)) (VP ((VPAST did) (NEG ,tot) (V need)) (VP((X to) (V be)) (ADJP(ADV too) (ADJ probing))))))(? After step-one rasures, this becomes: (S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPAST was) (ADJP(ADJ best))) (S(COMP(WIIADVP(WIIADV wheu))) (NP-s (PRO she)) (VP((VPAST) (NEG) (V need)) (VP((X)  (V be)) (ADJP(ADV too) (ADJ probing)))))) (? (FIN)) And after step-two erasures: (S(NP-s Miss Xydis) (VP was best) (S when she (VP need (V be (ADJP too probing))))) The Uuiversity of Pennsylvania Treebank output for this sentence, after steps one and two have been ap- plied to it, is: (S(NP Miss Xydis) (VP was best (SBAR when (S she (VP need (VP be (ADJP too probing))))))) Step three consists of comparing the candidate parse to the treebank parse and deriving two scores: (1) The Crossing Parentheses score is the number of times the treebank has a parenthesization such as, say, (A (B C)) and the parse being evaluated has a paren- thesization for the same input of ((A B) C)), i.e. there are parentheses which ""cross"". (2) The Recall score is the number of parenthesis pairs in the intersection of tlle candidate and treebank parses (T intersection C) divided by the number of parenthesis pairs in the treebank parse T, viz. (T intersection C) / T. This score provides an additional measure of the degree of fit between the standard and tile candidate parses; in theory a RecMl of 1 certifies a candidate parse as in- cluding all constituent boundaries that are essential to the analysis of the input sentence. We applie d this metric to 14 sentences selected from the Brown Cor- pus and analyzed by each of the grammarians named above in the manner that each wished his/her gram- mar to do. Instead of using the UPenn Treebank as a standard, we used the automaticMly computed ""ma- jority parse"" of each sentence obtained from the set of candidate parses themselves. The average Crossing Parentheses rate over all our grammars was .4%, with a corresponding Recall score of 94%. We have agreed on three additionM categories of systematic alteration to our input parses which we believe will significantly improve the correlation between our ""ideal parses"", i.e. our individuM goals, and our standard. Even at the current level of fit, we feel comfortable Mlow- ing one of our number, the UPenn parse, to serve as the standard parse, since, crucially, it. is produced by hand. Our intention is to apply the current metric to more Brown Corpus data ""ideally parsed"" by us, and then to employ it to measure the performance of our grammars, run automatically, on a 1)enchma.rk set of sentences. APPENDIX: EVALUATION PROCEDURE FOR COMPUTER ENGLISH GRAMMARS O. Input format A parse for evaluation should consist initially of: (a) the input word string, tokenized as follows: (I) Any tokens containing punctuation marks are enclosed by vertical bars, e.g. ~DAlbert~ I~,oooI (2) Contracted forms in which the abbreviated verb is used in the sentence under analysis as a main verb, as opposed to an auxiliary, are to be split: you ve  -> you lvel (In ""Youve a good reason for that."" but not in ""Youve been here often."") Johns -> John lsl (In ""Johns (i.e. is) a good friend"" or ""Johns (i.e. has) a good friend"" but not ""Johns (i.e. is) leaving"" and not ""Johns (i.e. has) been here"" (3) Hyphenated words, numbers and miscellaneous digital expressions are left as is (i.e. not split), i.e. ~co-signersl (and not ""co I-I signers"")| 12,0001 (and not ""2 I , I  0 o 0"")~ lall-womanl~ Ififty-threel: Ifree-for-alll| 56th~ 13/.1~ 1212-~88-9o271~ (b) the parse of the input word string with respect to the grammar under evaluation (I) Each grammatical constituent of the input is grouped using a pair of parentheses, e.g. 307 ""(((I)) ((see) ((Ed))))"" (2) Constituent labels may, optionally, immediately fol low left parentheses and~or immediately precede right parentheses, e.g. (S (N (N Sue)) (V (V sees) (N (N Tom))))  = : ( ( (Sue)  ) ( ( sees) ( (Tom)  ) ) )e tc . I. Erasures of Input Elements The f irst of the two steps necessary to prepare init ial parsed input for evaluat ion consists  of erasing the fo l lowing types of word (token) str ings from the parse: (a) Auxi l iar ies Examples are : ""would go there"" -? ""go there"", ""has been laughing"" - ? ""laughing"", ""does sing it correct ly"" - ? ""sing it correctly"", but not: ""is a cup"", ""is blue"", ""has a dollar"", ""does the laundry"" (b) ""Not"" E.g. ""is not in here"" -> ""is in here"", ""Not precisely asleep, John sort of dozed"" - ? ""precisely asleep, John sort of dozed"" (c ) Pre- inf init ival ""to"" E.g. ""she opted to retire"" - ? ""she opted retire"", ""how to construe it"" - ? ""how construe it"" (d) Null categories Example 1 : (""getting more pro letters than con""): (NXc (Qr more ) (NX (A pro ) (Npl letters)) (Than than ) (NX (A con) (Npl ) ) ) NOTA BENE - ? (NXc (Qr more ) (NX (A pro ) (Npl letters)) (Than than ) (NX (A con) ( ))); NOTA BENE Example 2 : (""The lawyer with whom I studied law""): (NP (DET The ) (N lawyer) (S-REL (PP (P with) (NP whom ) ) (NP I) (VP (V studied) (NP (N law)) (PP 0)))) NOTA BENE - ? (NP (DET The) (N lawyer ) (S- REL (PP (P with) (NP whom) ) (NP I) (VP (V studied) (NP (N law)) (PP ) ) )) NOTA BENE (e) Possess ive endings ( s,  ) E.g. "" ILorisl mother"" (i.e. the mother  of Lori) - ? ""Lori mother"" (f) Word-external  punctuat ion (quotes, commas, periods, dashes, etc. ) The ""blue book"" was there - ? The blue book was there Your f i rst  , second and third ideas -? Your f i rst second and third ideas This is it. -> This is it A l l - -or  almost al l - -of  them -> All or almost all of  them But leave as is: 13,~56 18.2gl 13/17/g01 111:301 Ip.m.I I1)1 Ieh.D.I IU.N. I Ineer-do-welll 308 2. Erasures of Constituent Delimiters, i.e. Parentheses The second of the two steps necessary to prepare initial parsed input for evaluation consists of erasing parenthesis pairs, proceeding recursively, from the most to the least deeply embedded portion of the parenthesization, whenever they enclose either a single constituent or word, or nothing at all. Example: ""Miss Xydis was best when she did not need to be too probing."" Original parse (S (NP-s (PNP (PNP Miss ) (PNP Xydis ))) (VP (VPAST was ) (ADJP (ADJ best ))) (S (COMP (WHADVP (WHADV when ))) (NP-s (PRO she )) (VP ((VPAST did ) (MEG not ) (V need )) (vP ((x to ) (V be )) (ADJP (ADV too ) (ADJ probing ) ))))) (? Parse with all erasures performed except those of const i tuentdel imiters (parentheses): (S (NP-s (PNP (PNP Miss ) (PNP Xydis ))) (VP (VPAST was ) (ADJP (ADJ best ))) (S (COMP (WHADVP (WHADV when ))) (NP-s (PRO she )) (VP ((VPAST ) (MEG ) (V need )) (vP ((x ) (V be )) (ADJP (ADV too ) (ADJ probing ) ))))) (? Parse with all constituent delimiters erased which are superfluous by the above definition: (S (NP-s Miss Xydis ) (VP was best ) (S when she (vP need (vP be (ADJP too probing))))) NOTE: Any single-word adverbs which are left behind, as it were, by the erasure of auxil iary elements, are attached to the highest node of the immediately fol lowing verb constituent. Example: (will probably have) (seen Milton) -> ( probably ) (seen Milton) -> (probably seen Milton) 3. Redefinit ion of Selected Constituents The third step in the process of preparing initial parsed input for evaluation is necessary only if the parse submitted treats any of three particular constructions in a manner different from the canonical analysis currently accepted by the group. This step consists of redrawing constituent boundaries in conformity with the adopted standard. The three constructions involved are extraposition, modification of noun phrases, and sequences of prepositions which occur constituent-init ial ly and~or 309 particles which occur constituent-finally. (a) Extraposition The treatment accepted at present attaches the extraposed clause to the topmost node of the host (sentential) clause. Example: If initial analysis is: (It (is (necessary (for us to leave)))) Then change to standard as follows: (It (is necessary) (for us to leave)) NOTE: The fol lowing is not an example of extraposition, and therefore not to be modified, although it seems to differ only minimally from a genuine extraposition sentence such as: ""It seemed like a good idea to begin early"": (It (seemed (like ((a good meeting) (to begin early))))) (b) Modification of Noun Phrases The treatment accepted at present attaches the modified ""core"" noun phrase and all of its modifiers from a single (noun phrase) node: Example: If initial analysis is: ((((the tree (that (we saw))) (with (orange leaves))) (that (was (very old)))) Then change to standard as follows: ((the tree) (that (we saw)) (with (orange leaves)) (that (was (very old)))) (c) Sequences of Constituent-Initial Prepositions and~or Constituent-Final Particles For sequences of prepositions occurring at the start of a prepositional phrase, the currently accepted practice is to attach each individually to the preposit ional-phrase node. For sequences of particles which come at the end of a verb phrase or other constituent with a verbal head, the adopted standard is, likewise, to attach each individually to the top node of the constituent: Example: If initial analysis is: (We (were (out (of (oatmeal cookies))))) Then change to standard as follows: (We (were (out of (oatmeal cookies)))) ~. Computation of Evaluation Statistics (a) Number of Constituents Incompatible With Standard Parse For the sentence under analysis, compare the constituents as del imited by the standard parse with those del imited by the parse for evaluation. The first statistic computed for each sentence is the number of constituents in the parse being evaluated which ""cross"", i.e. are neither subsstrings nor superstrings of, the constituents of the standard parse. Example: Standard parse: ((The prospect) (of (cutting back spending))) Parse for evaluation: (The (prospect (of ((cutting back) spending)))) The (non-unary) constituents of the parse for evaluation are: 310 1. The prospect of cutting backspend ing 2. prospect of cutting back spending 3. of cutting back spending 4. cutting back spending 5. cutting back While both constituents 2 and 5 differ from the standard, only 2 qualif ies as a ""crossing"" violation, as 5 is merely a substring of a constituent of the standard parse. So the ""Constituents Incompatible With Standard"" score for this sentence is I. (b) ""Recall"" and ""Precision"" of Parse Being Evaluated As a preliminary to computing Recall: Number of Standard-Parse Constituents in Candidate Total Number of Standard-Parse Constituents and Precision: Number of Candidate-Parse Constituents in Standard Total Number of Candidate-Parse Constituents the total number of constituents in the standard parse, and in the candidate parse, are simply counted. Notice that ""Number of Standard-Parse Constituents in Candidate"" and ""Number of Candidate-Parse Constituents in Standard"" are merely different names for the same object--the intersection of the set of standard-parse constituents with the set of candidate-parse constituents. So the final count prel iminary to the computation of Recall and Precision is the number of elements in that intersection. To return to the first example of the last subsection: Standard parse: ((The prospect) (of (cutting back spending))) Parse for evaluation: (The (prospect (of ((cutting back) spending)))) there are  4 standard-parse constituents, if the convention is adopted of excluding unary constituents~ and 5 candidate-parse constituents, under the same convention. Three of these are common to both sets, i.e. the intersection here is 3. Computing Recall and Precision is accomplished for this parse as follows: Recall = 3 / Precision = 3 / 5 . (C) Combining Statistics Gathered In order to evaluate a set of parses, first simply compute a distribution over ""Incompatible Constituents"" scores for the parses in the set, e.g. Incompatible Constituents: 0 I 2 Frequency: 3 I I (Total = 5) Next, average the Recall and Precision scores for the various parses in the set, e.g. Average Recall = (3 /4  + 7 /8 + 2/4  + 518 + 314)  / 5 = .700 Average Precision = (3 /5 + 7 /10  + 2 /5  + 5 /10 + 315)  / 5 = .560 311","A Procedure For Quantitatively Comparing The Syntactic Coverage Of English Grammars
We define PARSEVAL measures for parsing: labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively), which are based on the number of non-terminal items in the parser's output that match those in the gold-standard parse.
","For each parse to be evaluated: (1) erase from the fully-parsed sentence all instances of: auxiliaries, ""not"", pre-infinitival ""to"", null categories, possessive ndings (% and ), and all word",0.8071154356002808,0.8125667572021484,0.8098319172859192
