{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e831bf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kabi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bert_score import score\n",
    "\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a56bf2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scientific test dataset (must have 'article' and 'summary' columns)\n",
    "df = pd.read_csv(\"sci_test.csv\")\n",
    "df = df.dropna(subset=[\"text\", \"summary\"]).reset_index(drop=True)\n",
    "df_sample = df.sample(n=20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f0e85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained Random Forest model and TF-IDF vectorizer\n",
    "model = joblib.load(\"random_forest_summary_model.joblib\")\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba3e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define extractive summarization function using predicted probabilities\n",
    "def summarize_with_rf(text, model, vectorizer, top_k=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "    \n",
    "    X = vectorizer.transform(sentences)\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    top_indices = probs.argsort()[::-1][:top_k]\n",
    "    selected = [sentences[i] for i in sorted(top_indices)]\n",
    "    return \" \".join(selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ded9013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and prepare for evaluation\n",
    "generated = []\n",
    "references = []\n",
    "\n",
    "for _, row in df_sample.iterrows():\n",
    "    ref = row['summary']\n",
    "    article = row['text']\n",
    "    summary = summarize_with_rf(article, model, vectorizer, top_k=3)\n",
    "    \n",
    "    if summary.strip():\n",
    "        generated.append(summary)\n",
    "        references.append(ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1c64f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:17<00:00, 17.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 56.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 17.89 seconds, 1.12 sentences/sec\n",
      "\n",
      "ðŸ§ª BERTScore Results for Extractive Summarizer:\n",
      "Precision: 0.8388\n",
      "Recall:    0.8313\n",
      "F1 Score:  0.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using BERTScore\n",
    "P, R, F1 = score(generated, references, lang=\"en\", verbose=True)\n",
    "\n",
    "print(\"\\nðŸ§ª BERTScore Results for Extractive Summarizer:\")\n",
    "print(f\"Precision: {P.mean():.4f}\")\n",
    "print(f\"Recall:    {R.mean():.4f}\")\n",
    "print(f\"F1 Score:  {F1.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summaries saved to rf_extractive_summary_results_sci.csv\n"
     ]
    }
   ],
   "source": [
    "# Save generated and reference summaries to a CSV file\n",
    "summary_df = pd.DataFrame({\n",
    "    \"article\": df_sample[\"text\"][:len(generated)],\n",
    "    \"reference_summary\": references,\n",
    "    \"generated_summary\": generated\n",
    "})\n",
    "\n",
    "summary_df.to_csv(\"rf_extractive_summary_results_sci.csv\", index=True)\n",
    "print(\"âœ… Summaries saved to rf_extractive_summary_results_sci.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the summarizer with news article dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"news_test.csv\")\n",
    "df1= df1.dropna(subset=[\"text\", \"summary\"]).reset_index(drop=True)\n",
    "df_sample1 = df1.sample(n=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and prepare for evaluation\n",
    "generated1 = []\n",
    "references1 = []\n",
    "\n",
    "for _, row in df_sample1.iterrows():\n",
    "    ref = row['summary']\n",
    "    article = row['text']\n",
    "    summary = summarize_with_rf(article, model, vectorizer, top_k=3)\n",
    "    \n",
    "    if summary.strip():\n",
    "        generated1.append(summary)\n",
    "        references1.append(ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 46.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 21.00 seconds, 0.95 sentences/sec\n",
      "\n",
      "ðŸ§ª BERTScore Results for Extractive Summarizer:\n",
      "Precision: 0.9012\n",
      "Recall:    0.8671\n",
      "F1 Score:  0.8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using BERTScore\n",
    "P_1, R_1, F1_1 = score(generated1, references1, lang=\"en\", verbose=True)\n",
    "\n",
    "print(\"\\nðŸ§ª BERTScore Results for Extractive Summarizer:\")\n",
    "print(f\"Precision: {P_1.mean():.4f}\")\n",
    "print(f\"Recall:    {R_1.mean():.4f}\")\n",
    "print(f\"F1 Score:  {F1_1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summaries saved to rf_extractive_summary_results_news.csv\n"
     ]
    }
   ],
   "source": [
    "# Save generated and reference summaries to a CSV file\n",
    "summary_df1 = pd.DataFrame({\n",
    "    \"article\": df_sample1[\"text\"][:len(generated1)],\n",
    "    \"reference_summary\": references1,\n",
    "    \"generated_summary\": generated1\n",
    "})\n",
    "\n",
    "summary_df1.to_csv(\"rf_extractive_summary_results_news.csv\", index=True)\n",
    "print(\"âœ… Summaries saved to rf_extractive_summary_results_news.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
